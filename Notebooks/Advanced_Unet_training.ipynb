{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ],
   "id": "4e455a46b1b50452",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_stack(arr, nrows, ncols, flatten_panels=True):\n",
    "    \"\"\"\n",
    "    Split a stack of 2D panels into (nrows x ncols) tiles, without Python loops.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : ndarray, shape (P, H, W)\n",
    "        P = number of panels, each of size HxW.\n",
    "    nrows : int\n",
    "        number of rows per tile\n",
    "    ncols : int\n",
    "        number of cols per tile\n",
    "    flatten_panels : bool, optional (default=True)\n",
    "        If True, returns shape (P * Nblocks,  nrows, ncols),\n",
    "        otherwise returns (P, Nblocks, nrows, ncols).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tiles : ndarray\n",
    "        If flatten_panels:\n",
    "            shape = (P * (H//nrows)*(W//ncols),  nrows, ncols)\n",
    "        else:\n",
    "            shape = (P, (H//nrows)*(W//ncols),  nrows, ncols)\n",
    "    \"\"\"\n",
    "    P, H, W = arr.shape\n",
    "\n",
    "    #–– pad to exact multiples of nrows, ncols ––\n",
    "    pad_h = (-H) % nrows\n",
    "    pad_w = (-W) % ncols\n",
    "    if pad_h or pad_w:\n",
    "        arr = np.pad(arr,\n",
    "                     ((0,0),     # no pad on panel axis\n",
    "                      (0, pad_h), # pad rows\n",
    "                      (0, pad_w)),# pad cols\n",
    "                     mode='constant',\n",
    "                     constant_values=0)\n",
    "\n",
    "    H2, W2 = arr.shape[1], arr.shape[2]\n",
    "\n",
    "    #–– reshape into blocks and swap axes into a block-dimension ––\n",
    "    blocks = (arr\n",
    "        .reshape(P,\n",
    "                 H2 // nrows, nrows,\n",
    "                 W2 // ncols, ncols)\n",
    "        .swapaxes(2, 3)\n",
    "        # now blocks.shape == (P, Hb, Wb, nrows, ncols)\n",
    "    )\n",
    "\n",
    "    #–– optionally flatten panel × block into one axis ––\n",
    "    P, Hb, Wb, nr, nc = blocks.shape\n",
    "    out = blocks.reshape(P * Hb * Wb, 1, nr, nc) if flatten_panels \\\n",
    "          else      blocks.reshape(P, Hb * Wb, nr, nc)\n",
    "\n",
    "    return out\n",
    "\n",
    "def prepare_train_test (data_file, train_size=0.8, seed=42):\n",
    "    \"\"\"    Splits the data into training and validation sets.\n",
    "    Args:\n",
    "        data_file (str): Path to the .npz file containing the data.\n",
    "        train_size (float): Proportion of the data to use for training.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        tuple: Training and validation datasets as numpy arrays.\n",
    "    \"\"\"\n",
    "    x,y = np.load('../DATA/train.npz').values()\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    split_idx = int(len(x) * train_size)\n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    x_train, y_train = x[train_indices], y[train_indices]\n",
    "    x_val, y_val = x[val_indices], y[val_indices]\n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "def reshape_outputs(targets, img_shape=(32, 32)):\n",
    "    if not isinstance(targets, torch.Tensor):\n",
    "        torch_type = False\n",
    "        targets = torch.tensor(targets).float()\n",
    "    else:\n",
    "        torch_type = True\n",
    "    \"\"\"\n",
    "    # if shape is (..., H, W), unsqueeze to (..., 1, H, W)\n",
    "    added_channel = False\n",
    "    if targets.ndim >= 2 and targets.ndim < 4:\n",
    "        targets = targets.unsqueeze(1)\n",
    "        added_channel = True\"\"\"\n",
    "\n",
    "    # interpolate expects shape (N, C, H, W) or (..., C, H, W)\n",
    "    resized = F.interpolate(\n",
    "        targets,\n",
    "        size=img_shape,\n",
    "        mode='bilinear',      # or 'nearest' if you prefer\n",
    "        align_corners=False   # doesn't matter for nearest\n",
    "    )\n",
    "\n",
    "    # put it back to integer mask if needed\n",
    "    #resized = torch.ceil(resized)\n",
    "    resized = torch.round(resized)\n",
    "\n",
    "    \"\"\"if added_channel:\n",
    "        resized = resized.squeeze(1)\"\"\"\n",
    "    if not torch_type:\n",
    "        resized = resized.numpy()\n",
    "    return resized"
   ],
   "id": "2c2b658adec4a7e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# CBAM Block\n",
    "# -------------------------\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        # We implement the shared MLP with two 1×1 convolutions.\n",
    "        # First reduces channel dimension by 'ratio', then restores it.\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)   # output size = (B, C, 1, 1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)   # output size = (B, C, 1, 1)\n",
    "\n",
    "        # Shared MLP: conv ↓ then conv ↑\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // ratio, kernel_size=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_channels // ratio, in_channels, kernel_size=1, bias=True)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        b, c, _, _ = x.size()\n",
    "\n",
    "        # 1. Average‐pooling branch\n",
    "        avg_out = self.avg_pool(x)               # (B, C, 1, 1)\n",
    "        avg_out = self.fc1(avg_out)              # (B, C//ratio, 1, 1)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.fc2(avg_out)              # (B, C, 1, 1)\n",
    "\n",
    "        # 2. Max‐pooling branch\n",
    "        max_out = self.max_pool(x)               # (B, C, 1, 1)\n",
    "        max_out = self.fc1(max_out)              # (B, C//ratio, 1, 1)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.fc2(max_out)              # (B, C, 1, 1)\n",
    "\n",
    "        # 3. Combine, apply sigmoid, and scale the input\n",
    "        out = avg_out + max_out                  # (B, C, 1, 1)\n",
    "        scale = self.sigmoid(out)                # (B, C, 1, 1)\n",
    "        return x * scale                         # broadcast along H, W\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        # kernel_size must be odd; we set padding = (kernel_size-1)//2 to preserve spatial dims\n",
    "        assert kernel_size in (3, 7), \"kernel size must be 3 or 7\"\n",
    "        padding = (kernel_size - 1) // 2\n",
    "\n",
    "        # Convolution to produce single‐channel attention map from 2‐channel concatenation\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # 1. Channel‐wise average: produce (B, 1, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        # 2. Channel‐wise max: also (B, 1, H, W)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "\n",
    "        # 3. Concatenate along channel dim → (B, 2, H, W)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)\n",
    "\n",
    "        # 4. Convolution + sigmoid → spatial attention map (B, 1, H, W)\n",
    "        attn = self.conv(concat)\n",
    "        attn = self.sigmoid(attn)\n",
    "\n",
    "        # 5. Scale the input feature map\n",
    "        return x * attn                            # broadcast along channel dimension\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=1, kernel_size=7):\n",
    "        \"\"\"\n",
    "        CBAM module wrapper.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input feature channels.\n",
    "            ratio (int): Reduction ratio in channel attention MLP (default=1).\n",
    "            kernel_size (int): Kernel size for spatial attention (either 3 or 7; default=7).\n",
    "        \"\"\"\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Apply channel attention\n",
    "        x_out = self.channel_attention(x)\n",
    "        # 2. Apply spatial attention\n",
    "        x_out = self.spatial_attention(x_out)\n",
    "        return x_out\n",
    "\n",
    "# -------------------------\n",
    "# Attention Gate for Skip Connections\n",
    "# -------------------------\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Conv2d(F_g, F_int, kernel_size=1)\n",
    "        self.W_x = nn.Conv2d(F_l, F_int, kernel_size=1)\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, g, x):\n",
    "        # g: gating signal, x: skip connection\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# -------------------------\n",
    "# Depthwise Separable Conv (for ASPP)\n",
    "# -------------------------\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, kernel_size, padding, dilation=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size, padding=padding,\n",
    "                                   dilation=dilation, groups=in_ch)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act = activation\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.norm(x))\n",
    "# -------------------------\n",
    "# ASPP Module with Mixed Kernels\n",
    "# -------------------------\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        dilations = [1,1,1,1]\n",
    "        kernels   = [1,3,5,7]\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d,k in zip(dilations,kernels):\n",
    "            pad = (k//2)*d\n",
    "            self.branches.append(SepConv(in_ch, out_ch, activation, kernel_size=k, padding=pad, dilation=d))\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(dilations)*out_ch, out_ch, 1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation,\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.merge(torch.cat([b(x) for b in self.branches], dim=1))\n",
    "\n",
    "class BottleneckTransformer(nn.Module):\n",
    "    def __init__(self, dim, heads=8, depth=6, mlp_dim=None):\n",
    "        super().__init__()\n",
    "        mlp_dim = mlp_dim or dim*4\n",
    "        layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads,\n",
    "                                           dim_feedforward=mlp_dim,\n",
    "                                           activation='relu', norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layer, depth)\n",
    "        self.norm    = nn.LayerNorm(dim)\n",
    "    def forward(self,x):\n",
    "        bs,c,h,w = x.shape\n",
    "        x = x.flatten(2).permute(2,0,1)\n",
    "        x = self.encoder(x)\n",
    "        x = x.permute(1,2,0).view(bs,c,h,w)\n",
    "        return self.norm(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence of two 3x3 convolutions each followed by ReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, activation):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation,\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# -------------------------\n",
    "# Encoder/Decoder Blocks\n",
    "# -------------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch, activation, pool=True):\n",
    "        super().__init__()\n",
    "        #self.conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.conv = ASPP(in_ch, out_ch, activation)\n",
    "        self.pool = pool\n",
    "        self.cbam = CBAM(out_ch, ratio=8, kernel_size=7)\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        skip = x\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1) if self.pool else x\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch, activation, attn=True):\n",
    "        super().__init__()\n",
    "        self.up   = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.attn = attn\n",
    "        self.cbam = CBAM(out_ch, ratio=8, kernel_size=7)\n",
    "        if attn:\n",
    "            self.attn = AttentionGate(F_g=out_ch, F_l=out_ch, F_int=out_ch//2)\n",
    "            self.conv = ASPP(out_ch*2, out_ch, activation)\n",
    "        else:\n",
    "            self.conv = ASPP(out_ch, out_ch, activation)\n",
    "    def forward(self,x,skip):\n",
    "        x = self.up(x)\n",
    "        if self.attn:\n",
    "            skip = self.attn(g=x, x=skip)\n",
    "            x = self.conv(torch.cat([x,skip],dim=1))\n",
    "        else:\n",
    "            x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        return x\n",
    "\n",
    "class UNetEnhanced(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic U-Net architecture without attention, SE blocks, or deep supervision.\n",
    "    \"\"\"\n",
    "    def __init__(self, downfeatures=None, downactivation=None, bottleneckfeatures=None, upfeatures=None, upactivation=None):\n",
    "        super(UNetEnhanced, self).__init__()\n",
    "        in_channels = 1\n",
    "        out_channels = 1\n",
    "        if downfeatures is None:\n",
    "            downfeatures = [128, 256, 512, 1024]\n",
    "        if bottleneckfeatures is None:\n",
    "            bottleneckfeatures = downfeatures[-1] * 2\n",
    "        if upfeatures is None:\n",
    "            upfeatures = downfeatures[::-1]\n",
    "        if downactivation is None:\n",
    "            downactivation = [nn.ReLU(inplace=True) for _ in range(len(downfeatures))]\n",
    "        else:\n",
    "            downactivation = [get_activation(act) for act in downactivation]\n",
    "        if upactivation is None:\n",
    "            upactivation = downactivation[::-1]\n",
    "        else:\n",
    "            upactivation = [get_activation(act) for act in upactivation]\n",
    "\n",
    "        # Encoder path\n",
    "        self.downs = nn.ModuleList()\n",
    "        for l, feature in enumerate(downfeatures):\n",
    "            self.downs.append(EncoderBlock(in_channels, feature, downactivation[l]))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(downfeatures[-1], bottleneckfeatures, nn.ReLU(inplace=True))\n",
    "\n",
    "        # Decoder path\n",
    "        self.ups = nn.ModuleList()\n",
    "        in_channels = bottleneckfeatures\n",
    "        for l, feature in enumerate(upfeatures):\n",
    "            self.ups.append(DecoderBlock(in_channels, feature, upactivation[l], attn=False if l==0 else True))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Final 1x1 conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(upfeatures[-1]),\n",
    "            nn.Conv2d(upfeatures[-1], out_channels, kernel_size=3),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = nn.LayerNorm(normalized_shape=x.shape[1:], elementwise_affine=False)(x)\n",
    "        skip_connections = []\n",
    "\n",
    "        # Downsampling\n",
    "        for down in self.downs:\n",
    "            x, skip = down(x)\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Reverse skip connections for decoding\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for i, up in enumerate(self.ups):\n",
    "            if i == 0:\n",
    "                x = up(x, None)\n",
    "            else:\n",
    "                # Use attention gate for all but the first decoder block\n",
    "                #print (\"X:\", x.mean(), x.std(), \"SKIP:\", skip_connections[i].mean(), skip_connections[i].std())\n",
    "                x = up(x, skip_connections[i])\n",
    "        x = self.final_conv(x)\n",
    "        return x\n"
   ],
   "id": "464f392bb8f79ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class H5Dataset(Dataset):\n",
    "    def __init__(self,path): self.path=path; self.f=None\n",
    "    def __len__(self): return h5py.File(self.path,'r')['x'].shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        if self.f is None:\n",
    "            self.f = h5py.File(self.path,'r', swmr=True)\n",
    "        x = torch.from_numpy(self.f['x'][idx]).float()\n",
    "        y = torch.from_numpy(self.f['y'][idx]).float()\n",
    "        return x, y\n",
    "\n",
    "def get_activation(name):\n",
    "    \"\"\"\n",
    "    Get activation function by name.\n",
    "    \"\"\"\n",
    "    activations = {\n",
    "        'relu': nn.ReLU(inplace=True),\n",
    "        'leaky_relu': nn.LeakyReLU(inplace=True),\n",
    "        'elu': nn.ELU(inplace=True),\n",
    "        'prelu': nn.PReLU(),\n",
    "        'sigmoid': nn.Sigmoid(),\n",
    "        'tanh': nn.Tanh(),\n",
    "        'softmax': nn.Softmax(dim=1),\n",
    "    }\n",
    "    return activations.get(name, None)\n",
    "\n",
    "def train(model, train_ds, val_ds, loss=None, epochs=100, help_factor=1):\n",
    "    bs, max_batches = 64, 500\n",
    "     # ─── train loader ───\n",
    "    # pick exactly bs*max_batches random samples from train set\n",
    "    tr_idx = torch.randperm(len(train_ds))[: bs * max_batches]\n",
    "    output_size = get_model_output_size(model)\n",
    "\n",
    "\n",
    "    tr_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=bs,\n",
    "        sampler=SubsetRandomSampler(tr_idx),\n",
    "        num_workers=10,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=8,\n",
    "    )\n",
    "\n",
    "    # ─── val loader ───\n",
    "    # just iterate through val.h5 in order (or set shuffle=True if you like)\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        num_workers=10,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=8,\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    if loss is None:\n",
    "        crit  = ComboLoss(pos_weight=500, bce_weight=0.5, dice_weight=0.5)\n",
    "    else:\n",
    "        crit = loss\n",
    "\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=1e-3, steps_per_epoch=len(tr_loader), epochs=epochs)\n",
    "\n",
    "    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "    val_precisions = []\n",
    "    val_recalls = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # ——— train ———\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        tp=fp=fn=0\n",
    "        for batch_num, (imgs, msk) in enumerate(pbar:=tqdm(tr_loader, desc=f\"Epoch {epoch}\")):\n",
    "            imgs[msk==1] *= help_factor\n",
    "            imgs = imgs.to(device)\n",
    "            #msk = downsize_y(msk, output_size)\n",
    "            msk = reshape_outputs(msk, img_shape=output_size)\n",
    "            msk  = msk.to(device)\n",
    "            opt.zero_grad()\n",
    "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                seg_logits = model(imgs)\n",
    "                loss = crit(seg_logits, msk)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (seg_logits>0.5).float().view(-1)\n",
    "            t     = msk.view(-1)\n",
    "\n",
    "            tp += (preds * t).sum().item()\n",
    "            fp += (preds * (1-t)).sum().item()\n",
    "            fn += ((1-preds)*t).sum().item()\n",
    "            prec = tp/(tp+fp+1e-8)\n",
    "            rec  = tp/(tp+fn+1e-8)\n",
    "            f1   = 2*prec*rec/(prec+rec+1e-8)\n",
    "\n",
    "            pbar.set_postfix(avg_train_loss=total_loss/(bs*(batch_num+1)), batch_loss=loss.item()/bs, F1=f1, prec=prec, rec=rec, lr=sched.get_last_lr()[0])\n",
    "\n",
    "            sched.step()\n",
    "        train_losses.append(total_loss/len(train_ds))\n",
    "\n",
    "        # ——— validate ———\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tp=fp=fn=0\n",
    "            loss = 0\n",
    "            for imgs, msk in val_loader:\n",
    "                imgs[msk==1] *= help_factor\n",
    "                imgs = imgs.to(device)\n",
    "                #msk = downsize_y(msk, output_size)\n",
    "                msk = reshape_outputs(msk, img_shape=output_size)\n",
    "                msk  = msk.to(device)\n",
    "                with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                    seg_logits = model(imgs)\n",
    "                    loss += crit(seg_logits, msk)\n",
    "                preds = (seg_logits>0.5).float().view(-1)\n",
    "                t     = msk.view(-1)\n",
    "\n",
    "                tp += (preds * t).sum().item()\n",
    "                fp += (preds * (1-t)).sum().item()\n",
    "                fn += ((1-preds)*t).sum().item()\n",
    "\n",
    "            prec = tp/(tp+fp+1e-8)\n",
    "            rec  = tp/(tp+fn+1e-8)\n",
    "            f1   = 2*prec*rec/(prec+rec+1e-8)\n",
    "            val_losses.append(loss/len(val_ds))\n",
    "            val_f1s.append(f1)\n",
    "            val_precisions.append(prec)\n",
    "            val_recalls.append(rec)\n",
    "            print(f\"Val loss: {loss/len(val_ds):.4f}  Val F1: {f1:.4f}  (P={prec:.4f}, R={rec:.4f})\")\n",
    "\n",
    "def downsize_y(y, size):\n",
    "    \"\"\"\n",
    "    Downsize the y tensor to the given size.\n",
    "    \"\"\"\n",
    "    if y.shape[-2:] == size:\n",
    "        return y\n",
    "    else:\n",
    "        downsized = F.interpolate(y, size=size, mode='bilinear', align_corners=True)\n",
    "        #downsized = F.interpolate(y, size=size, mode=\"area\")\n",
    "        # make downsized tensor only 0 or 1 by doing ceil\n",
    "        #downsized = torch.ceil(downsized)\n",
    "        downsized = torch.round(downsized)\n",
    "        return downsized\n",
    "\n",
    "def get_model_output_size(model):\n",
    "    \"\"\"\n",
    "    Get the output size of the model.\n",
    "    \"\"\"\n",
    "    x = torch.randn(1, 1, 128, 128)\n",
    "    x = x.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    return y.shape[2:]"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma, self.eps = alpha, gamma, eps\n",
    "        self.beta = 1 - alpha  # Ensure alpha + beta = 1\n",
    "    def forward(self, preds, targets):\n",
    "        #preds = preds.view(-1)\n",
    "        #targets = targets.view(-1)\n",
    "        TP = (preds * targets).sum()\n",
    "        FP = (preds * (1 - targets)).sum()\n",
    "        FN = ((1 - preds) * targets).sum()\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return torch.pow((1 - tversky), self.gamma)\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.2, dice_weight=0.4):\n",
    "        super().__init__()\n",
    "        self.bce  = nn.BCELoss()\n",
    "        self.ft   = FocalTverskyLoss(alpha=0.995, gamma=2)\n",
    "        self.bw, self.dw= bce_weight, dice_weight\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss  = self.bce(logits, targets)\n",
    "        dice_loss = self.ft(logits, targets)\n",
    "        return self.bw*bce_loss + self.dw*dice_loss"
   ],
   "id": "b8f5c2a7c64b991a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigzi(x, axis=None):\n",
    "    return 0.741 * (np.percentile(x, 75, axis=axis) - np.percentile(x, 25, axis=axis))"
   ],
   "id": "f2b296f5a1d3b95a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_train, y_train, x_val, y_val = prepare_train_test('../DATA/train.npz', train_size=0.8, seed=42)\n",
    "x_train = split_stack(x_train, 128, 128)\n",
    "sig = sigzi(x_train)\n",
    "med = np.median(x_train)\n",
    "x_train = ((x_train - med) / sig).clip(-7,7)\n",
    "y_train = split_stack(y_train, 128, 128)\n",
    "x_val = split_stack(x_val, 128, 128)\n",
    "x_val = ((x_val - med) / sig).clip(-7,7)\n",
    "y_val = split_stack(y_val, 128, 128)\n",
    "\n",
    "\n",
    "down_filters = [32,  64, 64, 128, 256, 512]\n",
    "up_filters   = [512, 256, 128, 64]\n",
    "down_activations = ['relu', 'relu', 'relu', 'relu', 'relu', 'sigmoid', 'relu']\n",
    "up_activations = ['relu', 'sigmoid', 'relu', 'relu', 'relu']\n",
    "model = UNetEnhanced(downfeatures= down_filters, upfeatures=up_filters, downactivation=down_activations, upactivation=up_activations)\n",
    "get_model_output_size(model)\n",
    "#loss = ComboLoss(bce_weight=0, dice_weight=1)\n",
    "loss = FocalTverskyLoss(alpha=0.995, gamma=2)\n",
    "#train_ds = H5Dataset('train.h5')\n",
    "#val_ds   = H5Dataset('val.h5')\n",
    "train_ds = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "val_ds   = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).float())\n",
    "#train(model, train_ds, val_ds, loss=loss, epochs=8, help_factor=100)\n",
    "#train(model, train_ds, val_ds, loss=loss, epochs=8, help_factor=10)\n",
    "#train(model, train_ds, val_ds, loss=loss, epochs=8, help_factor=2)\n",
    "#train(model, train_ds, val_ds, loss=loss, epochs=32, help_factor=1.5)\n",
    "train(model, train_ds, val_ds, loss=loss, epochs=350, help_factor=1)"
   ],
   "id": "6995c6582f36f651",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make predictions with the model in batch size of 128\n",
    "def predict(model, val_ds, batch_size=128):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(val_ds), batch_size)):\n",
    "            imgs = []\n",
    "            for j in range(i, min(i+batch_size, len(val_ds))):\n",
    "                img, _ = val_ds[j]\n",
    "                imgs.append(img)\n",
    "            imgs = torch.stack(imgs).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            p = model(imgs)\n",
    "            #p = (torch.sigmoid(p)).float()\n",
    "            preds.append(p)\n",
    "    return torch.cat(preds).cpu()\n",
    "\n",
    "p = predict(model, val_ds)"
   ],
   "id": "bb175161290b40d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "p.view(-1).shape, p.shape",
   "id": "3d451d782b67ff8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "4096*32*32",
   "id": "7625ef05caddb656",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_val_reduced = downsize_y(torch.from_numpy(y_val).float(), (32,32))\n",
    "i = np.unique(np.argwhere(y_val==1)[:,0])[7]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(21, 7))\n",
    "ax[0].imshow(x_val[i, 0], cmap=\"Grays\")\n",
    "ax[1].imshow((p.numpy()>=0.5)[i, 0], cmap=\"Grays\")\n",
    "ax[2].imshow(y_val_reduced[i, 0], cmap=\"Grays\")\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ],
   "id": "499ace2b71afcc42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "p.numpy()[i, 0].max(), p.numpy()[i, 0].min()",
   "id": "102e683fa34f80d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.imshow(x_val[i, 0], cmap=\"Grays\")\n",
    "plt.show()\n",
    "plt.imshow((p.numpy()>=0.5)[i, 0], cmap=\"Grays\")\n",
    "plt.show()\n",
    "plt.imshow(y_val_reduced[i, 0], cmap=\"Grays\")\n",
    "plt.show()"
   ],
   "id": "b8494bcc7f24eb94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_val_reduced = downsize_y(torch.from_numpy(y_val).float(), (32,32))\n",
    "index = np.unique(np.argwhere(y_val==1)[:,0])\n",
    "for i in index:\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(21, 7))\n",
    "    ax[0].imshow(x_val[i, 0], cmap=\"Grays\")\n",
    "    ax[1].imshow((p.numpy()>=0.5)[i, 0], cmap=\"Grays\")\n",
    "    ax[2].imshow(y_val_reduced[i, 0], cmap=\"Grays\")\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    plt.show()"
   ],
   "id": "609ed38bd0ca16d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5d987bb38302740e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
