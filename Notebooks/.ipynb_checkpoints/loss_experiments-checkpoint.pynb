{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a63d56f-1e6f-44f2-9780-6e4976b82c30",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bff8b64d-329b-4e32-a9a6-d47ca85cb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, os, math, time, copy, random, numpy as np\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Quick knobs\n",
    "TILE         = 128\n",
    "BATCH        = 64\n",
    "NUM_WORKERS  = 2          # HDF5 is happier with 0–2\n",
    "SEED         = 1337\n",
    "MAX_WARMUP_BATCHES  = 120    # quick BCE warmup\n",
    "MAX_HEADONLY_BATCHES= 150\n",
    "MAX_TRAIN_BATCHES   = 200\n",
    "PRINT_EVERY         = 50     # log frequency\n",
    "DATA = {\n",
    "    \"train_h5\": \"/home/karlo/train_chunked.h5\",\n",
    "    \"test_h5\":  \"../DATA/test.h5\",\n",
    "    \"train_csv\": \"../DATA/train.csv\",  \n",
    "    \"test_csv\":  \"../DATA/test.csv\",\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(s=1337):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d012b524-0055-4638-8744-dce2d987f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stats_mad(arr):\n",
    "    med = np.median(arr); mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)\n",
    "    return np.float32(med), np.float32(1.0 if not np.isfinite(sigma) or sigma<=0 else sigma)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"Stream tiles from big (H,W) images, robust-normalize per-image, k-sigma clip, pad edges.\"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, crop_for_stats=512):\n",
    "        self.h5_path, self.tile, self.k_sigma, self.crop_for_stats = h5_path, int(tile), float(k_sigma), int(crop_for_stats)\n",
    "        self._h5 = self._x = self._y = None\n",
    "        self._stats_cache = {}\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "        Hb = math.ceil(self.H/self.tile); Wb = math.ceil(self.W/self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "    def _ensure(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self._x, self._y = self._h5[\"images\"], self._h5[\"masks\"]\n",
    "    def _image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        s = min(self.crop_for_stats, self.H, self.W)\n",
    "        h0, w0 = (self.H-s)//2, (self.W-s)//2\n",
    "        crop = self._x[i, h0:h0+s, w0:w0+s].astype(\"float32\")\n",
    "        med, sig = robust_stats_mad(crop); self._stats_cache[i] = (med, sig); return med, sig\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure()\n",
    "        i, r, c = self.indices[idx]; t = self.tile\n",
    "        r0, c0 = r*t, c*t; r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "        x = self._x[i, r0:r1, c0:c1].astype(\"float32\"); y = self._y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        if x.shape != (t,t):\n",
    "            xp = np.zeros((t,t), np.float32); yp = np.zeros((t,t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x; yp[:y.shape[0], :y.shape[1]] = y; x, y = xp, yp\n",
    "        med, sig = self._image_stats(i); x = np.clip((x-med)/sig, -5, 5)\n",
    "        return torch.from_numpy(x[None,...]), torch.from_numpy(y[None,...])\n",
    "\n",
    "class SubsetDS(Dataset):\n",
    "    \"\"\"Select full panels by id while reusing tiling of base dataset.\"\"\"\n",
    "    def __init__(self, base, panel_ids):\n",
    "        self.base, self.panel_ids = base, np.asarray(panel_ids)\n",
    "        t = base.tile; Hb, Wb = math.ceil(base.H/t), math.ceil(base.W/t)\n",
    "        base_map = {(i,r,c):k for k,(i,r,c) in enumerate(base.indices)}\n",
    "        self.map = [base_map[(i,r,c)] for i in self.panel_ids for r in range(Hb) for c in range(Wb)]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, k): return self.base[self.map[k]]\n",
    "\n",
    "def tile_pos_weights(h5_path, tile=128):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        Y = f[\"masks\"]; N,H,W = Y.shape\n",
    "    Hb, Wb = math.ceil(H/tile), math.ceil(W/tile)\n",
    "    w = []\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        Y = f[\"masks\"]\n",
    "        for i in range(N):\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0,c0=r*tile,c*tile; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    w.append(1.0 + 9.0*(Y[i,r0:r1,c0:c1].any()))\n",
    "    return np.asarray(w, np.float64)\n",
    "\n",
    "\n",
    "def panels_with_positives(h5_path, tile=128, max_panels=None):\n",
    "    ids=[]\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        Y=f['masks']; N,H,W=Y.shape\n",
    "        rng = np.random.default_rng(0)\n",
    "        order = rng.permutation(N) if max_panels else np.arange(N)\n",
    "        for i in order:\n",
    "            yi = Y[i]\n",
    "            if yi.any(): ids.append(i)\n",
    "            if max_panels and len(ids)>=max_panels: break\n",
    "    return np.array(sorted(ids))\n",
    "\n",
    "# per-panel median/MAD normalize, clip like stream_panels_direct\n",
    "def norm_medmad_clip(x, clip=5.0, eps=1e-6):\n",
    "    # x: torch.Tensor [B,1,H,W] or [1,H,W]\n",
    "    if x.ndim == 4:\n",
    "        med = x.median(dim=-1, keepdim=True).values.median(dim=-2, keepdim=True).values\n",
    "    else:  # [1,H,W]\n",
    "        med = x.median()\n",
    "        med = med.view(1,1,1)\n",
    "    mad = (x - med).abs().median()\n",
    "    sigma = 1.4826 * mad + eps\n",
    "    z = (x - med) / sigma\n",
    "    return z.clamp_(-clip, clip)\n",
    "\n",
    "class WithTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.base[i]\n",
    "        x = norm_medmad_clip(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a63300a-91fb-4a87-a67e-a3e9f1f4350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self,c,r=8): super().__init__(); self.fc1=nn.Conv2d(c,c//r,1); self.fc2=nn.Conv2d(c//r,c,1)\n",
    "    def forward(self,x): s=F.adaptive_avg_pool2d(x,1); s=F.silu(self.fc1(s),inplace=True); s=torch.sigmoid(self.fc2(s)); return x*s\n",
    "def _norm(c, groups=8): g=min(groups,c) if c%groups==0 else 1; return nn.GroupNorm(g,c)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); p=1\n",
    "    def __init__(self,c_in,c_out,k=3,act=nn.SiLU,se=True):\n",
    "        super().__init__(); p=k//2\n",
    "        self.proj = nn.Identity() if c_in==c_out else nn.Conv2d(c_in,c_out,1)\n",
    "        self.bn1=_norm(c_in); self.c1=nn.Conv2d(c_in,c_out,k,padding=p,bias=False)\n",
    "        self.bn2=_norm(c_out); self.c2=nn.Conv2d(c_out,c_out,k,padding=p,bias=False)\n",
    "        self.act=act(); self.se=SEBlock(c_out) if se else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        h=self.act(self.bn1(x)); h=self.c1(h)\n",
    "        h=self.act(self.bn2(h)); h=self.c2(h)\n",
    "        h=self.se(h); return h + self.proj(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); self.pool=nn.MaxPool2d(2); self.rb=ResBlock(c_in,c_out)\n",
    "    def forward(self,x): return self.rb(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self,c_in,c_skip,c_out): super().__init__(); self.up=nn.ConvTranspose2d(c_in,c_in,2,stride=2); self.rb1=ResBlock(c_in+c_skip,c_out); self.rb2=ResBlock(c_out,c_out)\n",
    "    def forward(self,x,skip):\n",
    "        x=self.up(x)\n",
    "        dh=skip.size(-2)-x.size(-2); dw=skip.size(-1)-x.size(-1)\n",
    "        if dh or dw: x=F.pad(x,(0,max(0,dw),0,max(0,dh)))\n",
    "        x=torch.cat([x,skip],1); x=self.rb1(x); x=self.rb2(x); return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self,c,r=[1,6,12,18]):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList([nn.Sequential(nn.Conv2d(c,c//4,3,padding=d,dilation=d,bias=False), nn.BatchNorm2d(c//4), nn.SiLU(True)) for d in r])\n",
    "        self.project=nn.Conv2d(c,c,1)\n",
    "    def forward(self,x): return self.project(torch.cat([b(x) for b in self.blocks],1))\n",
    "\n",
    "class UNetResSE(nn.Module):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(); w=widths\n",
    "        self.stem=nn.Sequential(nn.Conv2d(in_ch,w[0],3,padding=1,bias=False), nn.BatchNorm2d(w[0]), nn.SiLU(True), ResBlock(w[0],w[0]))\n",
    "        self.d1=Down(w[0],w[1]); self.d2=Down(w[1],w[2]); self.d3=Down(w[2],w[3]); self.d4=Down(w[3],w[4])\n",
    "        self.u1=Up(w[4],w[3],w[3]); self.u2=Up(w[3],w[2],w[2]); self.u3=Up(w[2],w[1],w[1]); self.u4=Up(w[1],w[0],w[0])\n",
    "        self.head=nn.Conv2d(w[0],out_ch,1)\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x) # logits\n",
    "\n",
    "class UNetResSEASPP(UNetResSE):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(in_ch,out_ch,widths); self.aspp=ASPP(widths[-1]); self.d4=Down(widths[3],widths[4])\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3); b=self.aspp(b)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x)\n",
    "\n",
    "# --- SHAPE UTILS ---\n",
    "@torch.no_grad()\n",
    "def resize_masks_to(logits, masks):\n",
    "    \"\"\"\n",
    "    Safe resize: (B,1,H,W or B,H,W) -> match logits spatial size using nearest.\n",
    "    \"\"\"\n",
    "    if masks.dim() == 3:\n",
    "        masks = masks.unsqueeze(1)\n",
    "    if masks.size(-2) == logits.size(-2) and masks.size(-1) == logits.size(-1):\n",
    "        return masks\n",
    "    return F.interpolate(masks.float(), size=logits.shape[-2:], mode='nearest')\n",
    "\n",
    "def init_head_bias_to_prior(model, p0=0.70):\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "            model.head.bias.data.fill_(b)\n",
    "\n",
    "def _set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "def freeze_all(model): _set_requires_grad(model, False)\n",
    "\n",
    "def unfreeze_head_only(model):\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        _set_requires_grad(model.head, True)\n",
    "    else:\n",
    "        raise AttributeError(\"Model has no attribute 'head'.\")\n",
    "\n",
    "def unfreeze_head_and_last_conv(model):\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"): _set_requires_grad(model.head, True)\n",
    "    # try to unfreeze a tiny tail conv (model-dependent)\n",
    "    try:\n",
    "        _set_requires_grad(model.u4.rb2.c2, True)\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d7c89ec-8b5e-45d8-8d36-85e9fb4f8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def quick_prob_stats(model, loader, n_batches=3, thr=0.5):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    pos_means, neg_means = [], []\n",
    "    tp = fp = fn = 0.0\n",
    "    t0=time.time()\n",
    "    for b,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yb_r   = resize_masks_to(logits, yb)\n",
    "        p      = torch.sigmoid(logits)\n",
    "\n",
    "        if (yb_r>0.5).any():\n",
    "            pos_means.append(float(p[yb_r>0.5].mean().item()))\n",
    "        neg_means.append(float(p[yb_r<=0.5].mean().item()))\n",
    "\n",
    "        pv, tv = p.view(-1), yb_r.view(-1)\n",
    "        pred   = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum())\n",
    "        fp += float((pred*(1-tv)).sum())\n",
    "        fn += float(((1-pred)*tv).sum())\n",
    "        if b>=n_batches: break\n",
    "\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "    import numpy as np\n",
    "    print(f\"[quick_prob_stats] batches={min(n_batches,b)} | pos_mean≈{(np.mean(pos_means) if pos_means else float('nan')):.4f} | \"\n",
    "          f\"neg_mean≈{np.mean(neg_means):.4f} | P {P:.3f} R {R:.3f} F1 {F1:.3f} @ thr={thr:.3f} | {time.time()-t0:.1f}s\")\n",
    "    return P,R,F1\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_under_min(model, loader, max_batches=40, n_bins=256, beta=2.0):\n",
    "    \"\"\"\n",
    "    Histogram-based pixel threshold sweep for recall-leaning F_beta.\n",
    "    Very fast: builds (p,t) histograms without scanning many thr in Python.\n",
    "    \"\"\"\n",
    "    model.eval(); dev=next(model.parameters()).device\n",
    "    pos_hist = torch.zeros(n_bins, dtype=torch.float64)\n",
    "    neg_hist = torch.zeros(n_bins, dtype=torch.float64)\n",
    "    nb = 0\n",
    "    for b,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        p = torch.sigmoid(model(xb)).detach()\n",
    "        y = resize_masks_to(p, yb)\n",
    "        p = p.view(-1).clamp(0,1).cpu()\n",
    "        y = y.view(-1).cpu()\n",
    "        idx = torch.clamp((p*(n_bins-1)).long(),0,n_bins-1)\n",
    "        pos_hist += torch.bincount(idx[y>0.5], minlength=n_bins).double()\n",
    "        neg_hist += torch.bincount(idx[y<=0.5], minlength=n_bins).double()\n",
    "        nb+=1\n",
    "        if nb>=max_batches: break\n",
    "\n",
    "    tp = torch.cumsum(pos_hist.flip(0), dim=0)\n",
    "    fp = torch.cumsum(neg_hist.flip(0), dim=0)\n",
    "    fn = pos_hist.sum() - tp\n",
    "    beta2 = beta*beta\n",
    "    denom = (beta2*(tp+fn) + (tp+fp)).clamp_min(1.0)\n",
    "    fbeta = (1+beta2)*tp / denom\n",
    "    best = torch.argmax(fbeta)\n",
    "    thr  = float((n_bins-1 - best) / (n_bins-1))\n",
    "    P = float(tp[best] / max(tp[best]+fp[best], 1.0))\n",
    "    R = float(tp[best] / max(tp[-1], 1.0))\n",
    "    F = float(fbeta[best])\n",
    "    return thr, (P,R,F), dict()\n",
    "\n",
    "def pick_thr_with_floor(model, loader, max_batches=20, n_bins=256, beta=2.0,\n",
    "                        min_pos_rate=0.01, max_pos_rate=0.10):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "\n",
    "    # histograms over probs for pos/neg pixels + overall count for pos-rate\n",
    "    pos_hist = np.zeros(n_bins, np.int64)\n",
    "    neg_hist = np.zeros(n_bins, np.int64)\n",
    "    all_hist = np.zeros(n_bins, np.int64)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for b, (xb, yb) in enumerate(loader, 1):\n",
    "            xb = xb.to(dev, non_blocking=True)\n",
    "            yb = yb.to(dev, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            yb_r   = resize_masks_to(logits, yb)\n",
    "            p      = torch.sigmoid(logits).clamp(1e-6, 1-1e-6).float().cpu()\n",
    "\n",
    "            t  = (yb_r>0.5).cpu()\n",
    "            idx = torch.clamp((p*(n_bins-1)).long(), 0, n_bins-1)\n",
    "\n",
    "            # update hist\n",
    "            for k in range(n_bins):\n",
    "                pass\n",
    "            flat_idx = idx.view(-1)\n",
    "            flat_t   = t.view(-1)\n",
    "            pos_hist += np.bincount(flat_idx[flat_t].numpy(), minlength=n_bins)\n",
    "            neg_hist += np.bincount(flat_idx[~flat_t].numpy(), minlength=n_bins)\n",
    "            all_hist += np.bincount(flat_idx.numpy(), minlength=n_bins)\n",
    "\n",
    "            if b >= max_batches: break\n",
    "\n",
    "    # sweep thresholds\n",
    "    pos_cum = pos_hist[::-1].cumsum()     # TP as threshold lowers\n",
    "    neg_cum = neg_hist[::-1].cumsum()     # FP as threshold lowers\n",
    "    all_cum = all_hist[::-1].cumsum()     # predicted-positive count\n",
    "\n",
    "    TP = pos_cum\n",
    "    FP = neg_cum\n",
    "    FN = pos_hist.sum() - TP\n",
    "    P = TP / np.maximum(TP+FP, 1)\n",
    "    R = TP / np.maximum(TP+FN, 1)\n",
    "\n",
    "    # F-beta (beta=2 -> recall-friendly)\n",
    "    beta2 = beta*beta\n",
    "    F1 = (1+beta2) * P * R / np.maximum(beta2*P + R, 1e-12)\n",
    "\n",
    "    # enforce predicted-positive rate floor/ceiling\n",
    "    total_px = all_hist.sum()\n",
    "    pos_rate = all_cum / max(total_px, 1)\n",
    "    mask = (pos_rate >= min_pos_rate) & (pos_rate <= max_pos_rate)\n",
    "\n",
    "    if not mask.any():\n",
    "        # fallback: choose the closest rate above min_pos_rate\n",
    "        k = np.argmin(np.abs(pos_rate - min_pos_rate))\n",
    "    else:\n",
    "        k = np.argmax(F1 * mask)\n",
    "\n",
    "    thr = (n_bins-1 - k) / (n_bins-1)\n",
    "    return float(thr), (float(P[k]), float(R[k]), float(F1[k])), dict(pos_rate=float(pos_rate[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8065934e-4ca8-49c8-b59b-189bc58f29cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== BASIC LOSSES ======\n",
    "class BCEWeighted(nn.Module):\n",
    "    def __init__(self, pos_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.pos_weight = torch.tensor(float(pos_weight))\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            logits, t, pos_weight=self.pos_weight.to(logits.device)\n",
    "        )\n",
    "\n",
    "class SoftDice(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__(); self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits); t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0),-1); t = t.view(t.size(0),-1)\n",
    "        inter = (p*t).sum(1); denom = p.sum(1)+t.sum(1)\n",
    "        dice = (2*inter+self.eps)/(denom+self.eps)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "class FocalDice(nn.Module):\n",
    "    def __init__(self, gamma=1.0, eps=1e-6):\n",
    "        super().__init__(); self.g=gamma; self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits); t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0),-1); t = t.view(t.size(0),-1)\n",
    "        inter = (p*t).sum(1); denom = p.sum(1)+t.sum(1)\n",
    "        dice = (2*inter+self.eps)/(denom+self.eps)\n",
    "        return torch.pow(1.0 - dice, self.g).mean()\n",
    "\n",
    "# ====== TVERSKY FAMILY ======\n",
    "class AsymFocalTversky(nn.Module):\n",
    "    def __init__(self, alpha=0.4, beta=0.6, gamma=1.2, eps=1e-6):\n",
    "        super().__init__(); self.a=alpha; self.b=beta; self.g=gamma; self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits).clamp(self.eps,1-self.eps)\n",
    "        t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0),-1); t = t.view(t.size(0),-1)\n",
    "        TP=(p*t).sum(1); FP=((1-t)*p).sum(1); FN=(t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps)/(TP+self.a*FP+self.b*FN+self.eps)\n",
    "        return torch.pow(1.0 - tv, self.g).mean()\n",
    "\n",
    "class AdaptiveAFTL(nn.Module):\n",
    "    \"\"\"AFTL with batch-adaptive alpha/beta (balances FP/FN as distribution drifts).\"\"\"\n",
    "    def __init__(self, base_alpha=0.35, base_beta=0.65, gamma=1.2, eps=1e-6, strength=0.3):\n",
    "        super().__init__(); self.a0=base_alpha; self.b0=base_beta; self.g=gamma; self.eps=eps; self.k=strength\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits).clamp(self.eps,1-self.eps)\n",
    "        t = targets.clamp(0,1)\n",
    "        pos_ratio = float(t.mean().item())\n",
    "        alpha = float(np.clip(self.a0 + self.k*(1-pos_ratio-0.5), 0.05, 0.95))\n",
    "        beta  = float(np.clip(self.b0 - self.k*(1-pos_ratio-0.5), 0.05, 0.95))\n",
    "        p = p.view(p.size(0),-1); t = t.view(t.size(0),-1)\n",
    "        TP=(p*t).sum(1); FP=((1-t)*p).sum(1); FN=(t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps)/(TP+alpha*FP+beta*FN+self.eps)\n",
    "        return torch.pow(1.0 - tv, self.g).mean()\n",
    "\n",
    "# ====== EDGE-ASSISTED (no SciPy; simple Sobel via conv) ======\n",
    "class SobelConv2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
    "        ky = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32)\n",
    "        self.register_buffer('kx', kx.view(1,1,3,3))\n",
    "        self.register_buffer('ky', ky.view(1,1,3,3))\n",
    "    def forward(self, x):\n",
    "        gx = F.conv2d(x, self.kx, padding=1)\n",
    "        gy = F.conv2d(x, self.ky, padding=1)\n",
    "        return torch.sqrt(gx*gx + gy*gy + 1e-12)\n",
    "\n",
    "class BCEplusSobelEdge(nn.Module):\n",
    "    def __init__(self, pos_weight=2.0, lam=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_weight = torch.tensor(float(pos_weight))\n",
    "        self.edge = SobelConv2D()\n",
    "        self.lam = lam\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            logits, t, pos_weight=self.pos_weight.to(logits.device)\n",
    "        )\n",
    "        p = torch.sigmoid(logits)\n",
    "        e_p = self.edge(p); e_t = self.edge(t)\n",
    "        edge_bce = F.binary_cross_entropy(\n",
    "            (e_p/(e_p.max().clamp_min(1e-6))).clamp(0,1),\n",
    "            (e_t/(e_t.max().clamp_min(1e-6))).clamp(0,1)\n",
    "        )\n",
    "        return bce + self.lam*edge_bce\n",
    "\n",
    "# ====== SOFT-IOU ======\n",
    "class SoftIoU(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__(); self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits); t=targets.clamp(0,1)\n",
    "        p = p.view(p.size(0),-1); t = t.view(t.size(0),-1)\n",
    "        inter = (p*t).sum(1); union = p.sum(1)+t.sum(1)-inter\n",
    "        ji = (inter+self.eps)/(union+self.eps)\n",
    "        return 1.0 - ji.mean()\n",
    "\n",
    "class SoftIoULoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        t = targets.clamp(0,1)\n",
    "        inter = (p*t).sum(dim=(1,2,3))\n",
    "        union = (p + t - p*t).sum(dim=(1,2,3)) + self.eps\n",
    "        iou = inter / union\n",
    "        return (1.0 - iou).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba97e98-1301-429b-b734-8a8a1a0baf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LossSpec:\n",
    "    name: str\n",
    "    fn:   callable\n",
    "    args: dict\n",
    "\n",
    "def make_loss(spec: LossSpec):\n",
    "    return spec.fn(**spec.args).to(device)\n",
    "\n",
    "CANDIDATES = [\n",
    "    # Stable baselines\n",
    "    LossSpec(\"BCE(posw=20)\", BCEWeighted, dict(pos_weight=20.0)),\n",
    "    LossSpec(\"BCE+SoftDice(0.2)\", nn.Module, {})  # placeholder; we’ll wrap below\n",
    "]\n",
    "\n",
    "# we’ll add wrapped combos as inline classes for clarity\n",
    "class BCEplusSoftDice(nn.Module):\n",
    "    def __init__(self, pos_weight=10.0, lam=0.2):\n",
    "        super().__init__()\n",
    "        self.bce = BCEWeighted(pos_weight=pos_weight)\n",
    "        self.dice= SoftDice()\n",
    "        self.lam  = lam\n",
    "    def forward(self, logits, targets):\n",
    "        return self.bce(logits, targets) + self.lam*self.dice(logits, targets)\n",
    "\n",
    "class BCEplusFocalDice(nn.Module):\n",
    "    def __init__(self, pos_weight=10.0, gamma=1.0, lam=0.2):\n",
    "        super().__init__()\n",
    "        self.bce = BCEWeighted(pos_weight=pos_weight)\n",
    "        self.fd  = FocalDice(gamma=gamma)\n",
    "        self.lam = lam\n",
    "    def forward(self, logits, targets):\n",
    "        return self.bce(logits, targets) + self.lam*self.fd(logits, targets)\n",
    "\n",
    "class BCEPlusAFTL(nn.Module):\n",
    "    \"\"\"Small BCE term + Asymmetric Focal Tversky (device-safe).\"\"\"\n",
    "    def __init__(self, bce_posw=6.0, aftl_alpha=0.45, aftl_beta=0.55, aftl_gamma=1.3, w_bce=0.20, w_aftl=0.80):\n",
    "        super().__init__()\n",
    "        self.aftl = AsymFocalTversky(alpha=aftl_alpha, beta=aftl_beta, gamma=aftl_gamma)\n",
    "        self.w_bce, self.w_aftl = float(w_bce), float(w_aftl)\n",
    "        self.register_buffer(\"posw\", torch.tensor(float(bce_posw), dtype=torch.float32))\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        # pos_weight must be on same device/dtype as logits\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, t, pos_weight=self.posw.to(logits.device, logits.dtype))\n",
    "        return self.w_bce*bce + self.w_aftl*self.aftl(logits, t)\n",
    "\n",
    "class BCEPlusSoftIoU(nn.Module):\n",
    "    \"\"\"SoftIoU for recall/shape + small BCE for margins (device-safe).\"\"\"\n",
    "    def __init__(self, bce_posw=10.0, w_bce=0.20, w_iou=0.80):\n",
    "        super().__init__()\n",
    "        self.iou = SoftIoULoss()\n",
    "        self.w_bce, self.w_iou = float(w_bce), float(w_iou)\n",
    "        self.register_buffer(\"posw\", torch.tensor(float(bce_posw), dtype=torch.float32))\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        bce  = F.binary_cross_entropy_with_logits(logits, t, pos_weight=self.posw.to(logits.device, logits.dtype))\n",
    "        siou = self.iou(logits, t)\n",
    "        return self.w_bce*bce + self.w_iou*siou\n",
    "\n",
    "class BCEIoUPlusSobel(nn.Module):\n",
    "    \"\"\"SoftIoU + BCE + tiny Sobel edge alignment term (device-safe).\"\"\"\n",
    "    def __init__(self, bce_posw=10.0, w_bce=0.20, w_iou=0.75, w_sobel=0.05):\n",
    "        super().__init__()\n",
    "        self.core = BCEPlusSoftIoU(bce_posw=bce_posw, w_bce=w_bce, w_iou=w_iou)\n",
    "        self.w_sobel = float(w_sobel)\n",
    "        # Register on-CPU buffers (will be moved per-call to logits.device)\n",
    "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "        ky = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "        self.register_buffer(\"kx\", kx)\n",
    "        self.register_buffer(\"ky\", ky)\n",
    "    def sobel(self, x):\n",
    "        # ensure kernels live on the same device/dtype as x\n",
    "        kx = self.kx.to(device=x.device, dtype=x.dtype)\n",
    "        ky = self.ky.to(device=x.device, dtype=x.dtype)\n",
    "        gx = F.conv2d(x, kx, padding=1)\n",
    "        gy = F.conv2d(x, ky, padding=1)\n",
    "        return torch.sqrt(gx*gx + gy*gy + 1e-12)\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        p = torch.sigmoid(logits)\n",
    "        core_loss = self.core(logits, t)\n",
    "        edge_loss = F.l1_loss(self.sobel(p), self.sobel(t))\n",
    "        return core_loss + self.w_sobel*edge_loss\n",
    "\n",
    "# Replace placeholder & add a proper slate\n",
    "CANDIDATES = [\n",
    "    LossSpec(\"BCE(posw=20)\",       BCEWeighted,          dict(pos_weight=20.0)),\n",
    "    LossSpec(\"BCE+SoftDice(0.2)\",  BCEplusSoftDice,      dict(pos_weight=15.0, lam=0.2)),\n",
    "    LossSpec(\"BCE+FocalDice(g1.0)\",BCEplusFocalDice,     dict(pos_weight=12.0, gamma=1.0, lam=0.2)),\n",
    "    LossSpec(\"BCE+Sobel(0.1)\",     BCEplusSobelEdge,     dict(pos_weight=8.0, lam=0.1)),\n",
    "    LossSpec(\"SoftIoU only\",       SoftIoU,              dict()),\n",
    "    LossSpec(\"AFTL(adapt)\",        AdaptiveAFTL,         dict(base_alpha=0.35, base_beta=0.65, gamma=1.2, strength=0.3)),\n",
    "    LossSpec(\"BCE+AFTL(0.15)\",     BCEPlusAFTL,          dict(bce_posw=6.0, aftl_alpha=0.35, aftl_beta=0.65, aftl_gamma=1.1, w_aftl=0.15)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8c6626-9782-4ad7-ba7e-a7a03edb5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quick_warmup(model, loader, epochs=1, max_batches=120, lr=2e-4, pos_weight=30.0, metric_thr=0.20):\n",
    "    dev = next(model.parameters()).device\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train(); seen=tp=fp=fn=0; loss_sum=0.0; t0=time.time()\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits).view(-1); t = yb_r.view(-1)\n",
    "                pred=(p>=metric_thr).float()\n",
    "                tp+=float((pred*t).sum()); fp+=float((pred*(1-t)).sum()); fn+=float(((1-pred)*t).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen+=xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P=tp/max(tp+fp,1); R=tp/max(tp+fn,1); F1=2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[WARMUP] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f}\")\n",
    "\n",
    "def fit_quick_head_only(model, loader, epochs=2, max_batches=150, lr=5e-5, pos_weight=5.0, metric_thr=0.15):\n",
    "    unfreeze_head_only(model)\n",
    "    head_params = [p for p in model.head.parameters() if p.requires_grad]\n",
    "    dev = next(model.parameters()).device\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    opt = torch.optim.Adam(head_params, lr=lr)\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train(); seen=tp=fp=fn=0; loss_sum=0.0\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p=torch.sigmoid(logits).view(-1); t=yb_r.view(-1)\n",
    "                pred=(p>=metric_thr).float()\n",
    "                tp+=float((pred*t).sum()); fp+=float((pred*(1-t)).sum()); fn+=float(((1-pred)*t).sum())\n",
    "                loss_sum+=float(loss.item())*xb.size(0); seen+=xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P=tp/max(tp+fp,1); R=tp/max(tp+fn,1); F1=2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[HEAD] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f}\")\n",
    "\n",
    "def fit_quick(model, criterion, loader, epochs=1, max_batches=200, lr=1.5e-4, metric_thr=0.15, weight_decay=1e-4, unfreeze='head_last'):\n",
    "    if unfreeze=='head': unfreeze_head_only(model)\n",
    "    elif unfreeze=='head_last': unfreeze_head_and_last_conv(model)\n",
    "    else: freeze_all(model)  # (shouldn't happen)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "    dev = next(model.parameters()).device\n",
    "\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train(); seen=tp=fp=fn=0; loss_sum=0.0; t0=time.time()\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = criterion(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p=torch.sigmoid(logits).view(-1); t=yb_r.view(-1)\n",
    "                pred=(p>=metric_thr).float()\n",
    "                tp+=float((pred*t).sum()); fp+=float((pred*(1-t)).sum()); fn+=float(((1-pred)*t).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen+=xb.size(0)\n",
    "            if b%PRINT_EVERY==0 or b==max_batches:\n",
    "                print(f\"\\r[QP] batch {b}/{len(loader)} | loss={loss_sum/seen:.4f} | \"\n",
    "                      f\"over>=thr {float((p>=metric_thr).float().mean()):.3f} | {time.time()-t0:.1f}s\", end='')\n",
    "            if b>=max_batches: break\n",
    "        print()\n",
    "        P=tp/max(tp+fp,1); R=tp/max(tp+fn,1); F1=2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[QP] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2f685c-0d12-4362-b25f-649940427c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_probe_bakeoff(model_cls, p0=0.70):\n",
    "    # 0) fresh probe & warmup\n",
    "    probe = model_cls(in_ch=1, out_ch=1).to(device)\n",
    "    init_head_bias_to_prior(probe, p0=p0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(\"Warmup...\")\n",
    "    fit_quick_warmup(probe, train_loader_small,\n",
    "                     epochs=1, max_batches=MAX_WARMUP_BATCHES,\n",
    "                     lr=2e-4, pos_weight=30.0, metric_thr=0.20)\n",
    "\n",
    "    warm_state = copy.deepcopy(probe.state_dict())\n",
    "\n",
    "    # 1) recall-friendly (pixel) threshold from val\n",
    "    thr0, (P0,R0,F0), _ = pick_thr_under_min(\n",
    "        probe, val_loader_small, max_batches=40, n_bins=256, beta=2.0\n",
    "    )\n",
    "    thr0 = float(np.clip(thr0, 0.08, 0.20))\n",
    "    print(f\"[thr0] ≈ {thr0:.3f} | P≈{P0:.3f} R≈{R0:.3f} Fβ≈{F0:.3f}\")\n",
    "\n",
    "    # 2) head-only calibration (tiny LR)\n",
    "    probe.load_state_dict(warm_state, strict=True)\n",
    "    fit_quick_head_only(probe, train_loader_small,\n",
    "                        epochs=2, max_batches=MAX_HEADONLY_BATCHES,\n",
    "                        lr=3e-5, pos_weight=5.0, metric_thr=thr0)\n",
    "    quick_prob_stats(probe, train_loader_small, n_batches=3, thr=thr0)\n",
    "\n",
    "    # snapshot\n",
    "    headcal_state = copy.deepcopy(probe.state_dict())\n",
    "\n",
    "    # 3) try each candidate with same starting weights & fair budget\n",
    "    results = []\n",
    "    for spec in CANDIDATES:\n",
    "        print(f\"\\n=== TRY: {spec.name} ===\")\n",
    "        probe.load_state_dict(headcal_state, strict=True)\n",
    "\n",
    "        crit = make_loss(spec)\n",
    "        # unfreeze head + last tiny conv for all candidates for fairness\n",
    "        fit_quick(probe, crit, train_loader_small,\n",
    "                  epochs=1, max_batches=MAX_TRAIN_BATCHES,\n",
    "                  lr=1.5e-4, metric_thr=thr0, weight_decay=1e-4,\n",
    "                  unfreeze='head_last')\n",
    "\n",
    "        # quick recall-leaning thr from val for THIS candidate (fast)\n",
    "        thr_c, (Pc,Rc,Fc), _ = pick_thr_under_min(\n",
    "            probe, val_loader_small, max_batches=20, n_bins=256, beta=2.0\n",
    "        )\n",
    "        thr_c = float(np.clip(thr_c, max(0.05, thr0*0.6), min(0.35, thr0*1.8)))\n",
    "\n",
    "        # compact train-side snapshot at that thr\n",
    "        Pq,Rq,Fq = quick_prob_stats(probe, train_loader_small, n_batches=3, thr=thr_c)\n",
    "        results.append(dict(\n",
    "            name=spec.name, thr=thr_c, val_P=Pc, val_R=Rc, val_F=Fc, trn_P=Pq, trn_R=Rq, trn_F=Fq\n",
    "        ))\n",
    "\n",
    "    # 4) show leaderboard (sort by val_F then trn_F)\n",
    "    results = sorted(results, key=lambda d: (d['val_F'], d['trn_F']), reverse=True)\n",
    "    print(\"\\n=== Leaderboard (val_F then train_F) ===\")\n",
    "    for r in results:\n",
    "        print(f\"{r['name']:<20} | thr={r['thr']:.3f} | \"\n",
    "              f\"val: P {r['val_P']:.3f} R {r['val_R']:.3f} F {r['val_F']:.3f} | \"\n",
    "              f\"train: P {r['trn_P']:.3f} R {r['trn_R']:.3f} F {r['trn_F']:.3f}\")\n",
    "\n",
    "    # return best (first) to keep evaluating\n",
    "    best = results[0] if len(results) else None\n",
    "    return probe, best, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15657987-7b4f-46c6-ba0a-6707e69ae83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, torch\n",
    "\n",
    "def evaluate_PRF_on_loader(model, loader, thr=0.5, max_batches=40):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    tp=fp=fn=0.0\n",
    "    with torch.inference_mode():\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb)\n",
    "            yb_r = resize_masks_to(logits, yb)\n",
    "            p = torch.sigmoid(logits)\n",
    "            pred = (p >= thr).float()\n",
    "            tp += float((pred*yb_r).sum())\n",
    "            fp += float((pred*(1-yb_r)).sum())\n",
    "            fn += float(((1-pred)*yb_r).sum())\n",
    "            if b>=max_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "    return P,R,f1\n",
    "\n",
    "def brief_train(model, criterion, loader, epochs=1, max_batches=400, lr=1.5e-4, wd=0.0):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    dev = next(model.parameters()).device\n",
    "    model.train()\n",
    "    t0=time.time()\n",
    "    for ep in range(1,epochs+1):\n",
    "        seen=loss_sum=0.0\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb)\n",
    "            yb_r   = resize_masks_to(logits, yb)\n",
    "            loss   = criterion(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "    return (loss_sum/ max(seen,1)), time.time()-t0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97da40-2204-4e4e-a262-8e55381295a8",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae089666-3c33-4b2b-a0f2-c2bd90b94596",
   "metadata": {},
   "source": [
    "### Broad probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab819fde-b131-4374-8171-05cc7b847544",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA[\"train_h5\"], \"r\") as f:\n",
    "    N = f[\"images\"].shape[0]\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "split = int(0.9*N); idx_tr, idx_va = np.sort(idx[:split]), np.sort(idx[split:])\n",
    "\n",
    "ds_full  = H5TiledDataset(DATA[\"train_h5\"], tile=TILE, k_sigma=5.0)\n",
    "\n",
    "pos_panels = panels_with_positives(DATA[\"train_h5\"], max_panels=2000)\n",
    "# Sample small train/val from the intersection with idx_tr/idx_va\n",
    "sub_tr = np.random.default_rng(SEED).choice(np.intersect1d(idx_tr, pos_panels), size=min(200, len(pos_panels)), replace=False)\n",
    "sub_va = np.random.default_rng(SEED+1).choice(np.intersect1d(idx_va, pos_panels), size=min(80,  len(pos_panels)), replace=False)\n",
    "\n",
    "train_ds_small = SubsetDS(ds_full, np.sort(sub_tr))\n",
    "val_ds_small   = SubsetDS(ds_full, np.sort(sub_va))\n",
    "train_loader_small = DataLoader(train_ds_small, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader_small   = DataLoader(val_ds_small,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "train_ds_small = WithTransform(train_ds_small)\n",
    "val_ds_small   = WithTransform(val_ds_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cafd1d0-3e90-4b20-847b-a66907b6d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup...\n",
      "[WARMUP] ep1 loss 0.3891 | F1 0.012 P 0.006 R 0.355\n",
      "[thr0] ≈ 0.200 | P≈0.012 R≈0.171 Fβ≈0.048\n",
      "[HEAD] ep1 loss 0.1583 | F1 0.016 P 0.008 R 0.480\n",
      "[HEAD] ep2 loss 0.1495 | F1 0.018 P 0.009 R 0.487\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.2818 | neg_mean≈0.0988 | P 0.006 R 0.600 F1 0.012 @ thr=0.200 | 0.5s\n",
      "\n",
      "=== TRY: BCE(posw=20) ===\n",
      "[QP] batch 200/3200 | loss=0.2238 | over>=thr 0.035 | 11.5s\n",
      "[QP] ep1 loss 0.2238 | F1 0.021 P 0.011 R 0.218\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.1198 | neg_mean≈0.0609 | P 0.011 R 0.249 F1 0.020 @ thr=0.169 | 0.5s\n",
      "\n",
      "=== TRY: BCE+SoftDice(0.2) ===\n",
      "[QP] batch 200/3200 | loss=0.3964 | over>=thr 0.020 | 11.5s\n",
      "[QP] ep1 loss 0.3964 | F1 0.024 P 0.013 R 0.172\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.0814 | neg_mean≈0.0525 | P 0.016 R 0.271 F1 0.030 @ thr=0.145 | 0.5s\n",
      "\n",
      "=== TRY: BCE+FocalDice(g1.0) ===\n",
      "[QP] batch 200/3200 | loss=0.3607 | over>=thr 0.013 | 11.5s\n",
      "[QP] ep1 loss 0.3607 | F1 0.022 P 0.012 R 0.149\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.0703 | neg_mean≈0.0422 | P 0.011 R 0.191 F1 0.021 @ thr=0.129 | 0.5s\n",
      "\n",
      "=== TRY: BCE+Sobel(0.1) ===\n",
      "[QP] batch 200/3200 | loss=0.1622 | over>=thr 0.005 | 11.6s\n",
      "[QP] ep1 loss 0.1622 | F1 0.025 P 0.014 R 0.132\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.0411 | neg_mean≈0.0294 | P 0.009 R 0.111 F1 0.017 @ thr=0.120 | 0.5s\n",
      "\n",
      "=== TRY: SoftIoU only ===\n",
      "[QP] batch 200/3200 | loss=0.9958 | over>=thr 0.285 | 11.4s\n",
      "[QP] ep1 loss 0.9958 | F1 0.014 P 0.007 R 0.559\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.4976 | neg_mean≈0.1834 | P 0.011 R 0.592 F1 0.022 @ thr=0.350 | 0.5s\n",
      "\n",
      "=== TRY: AFTL(adapt) ===\n",
      "[QP] batch 200/3200 | loss=0.9909 | over>=thr 0.288 | 11.5s\n",
      "[QP] ep1 loss 0.9909 | F1 0.015 P 0.008 R 0.603\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.5475 | neg_mean≈0.1804 | P 0.013 R 0.613 F1 0.025 @ thr=0.350 | 0.5s\n",
      "\n",
      "=== TRY: BCE+AFTL(0.15) ===\n",
      "[QP] batch 200/3200 | loss=0.1698 | over>=thr 0.002 | 11.5s\n",
      "[QP] ep1 loss 0.1698 | F1 0.023 P 0.013 R 0.112\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.0202 | neg_mean≈0.0214 | P 0.003 R 0.011 F1 0.005 @ thr=0.120 | 0.5s\n",
      "\n",
      "=== Leaderboard (val_F then train_F) ===\n",
      "AFTL(adapt)          | thr=0.350 | val: P 0.011 R 0.162 F 0.045 | train: P 0.013 R 0.613 F 0.025\n",
      "SoftIoU only         | thr=0.350 | val: P 0.011 R 0.163 F 0.045 | train: P 0.011 R 0.592 F 0.022\n",
      "BCE(posw=20)         | thr=0.169 | val: P 0.011 R 0.144 F 0.042 | train: P 0.011 R 0.249 F 0.020\n",
      "BCE+FocalDice(g1.0)  | thr=0.129 | val: P 0.011 R 0.133 F 0.041 | train: P 0.011 R 0.191 F 0.021\n",
      "BCE+SoftDice(0.2)    | thr=0.145 | val: P 0.011 R 0.141 F 0.041 | train: P 0.016 R 0.271 F 0.030\n",
      "BCE+Sobel(0.1)       | thr=0.120 | val: P 0.010 R 0.152 F 0.040 | train: P 0.009 R 0.111 F 0.017\n",
      "BCE+AFTL(0.15)       | thr=0.120 | val: P 0.010 R 0.134 F 0.040 | train: P 0.003 R 0.011 F 0.005\n",
      "\n",
      "Best (by val_F):\n",
      "{'name': 'AFTL(adapt)', 'thr': 0.35, 'val_P': 0.011477585380936286, 'val_R': 0.1615734799137952, 'val_F': 0.04468959069090208, 'trn_P': 0.01255885418982545, 'trn_R': 0.6134663341645885, 'trn_F': 0.024613815496439935}\n"
     ]
    }
   ],
   "source": [
    "# If your model class is already in scope (UNetResSEASPP), run:\n",
    "probe, best, all_results = run_probe_bakeoff(UNetResSEASPP, p0=0.70)\n",
    "\n",
    "print(\"\\nBest (by val_F):\")\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec08cb5-d8d8-4b57-b32b-fbc17cd46a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[AFTL] AFTL+BCE (α0.45 β0.55 γ1.3, posw6) | thr=0.212 | val: P 0.024 R 0.214 F 0.044 | train: P 0.019 R 0.247 F 0.035\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.2416 | neg_mean≈0.0273 | P 0.053 R 0.567 F1 0.097 @ thr=0.212 | 0.5s\n",
      "\n",
      "[AFTL] AFTL+BCE (α0.55 β0.45 γ1.5, posw6) | thr=0.188 | val: P 0.019 R 0.169 F 0.034 | train: P 0.023 R 0.218 F 0.042\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.0509 | neg_mean≈0.0299 | P 0.014 R 0.087 F1 0.024 @ thr=0.188 | 0.5s\n",
      "\n",
      "[SIOU] SoftIoU+BCE (posw10, λ_bce=0.2) | thr=0.188 | val: P 0.023 R 0.200 F 0.041 | train: P 0.019 R 0.221 F 0.035\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.1049 | neg_mean≈0.0379 | P 0.019 R 0.157 F1 0.034 @ thr=0.188 | 0.5s\n",
      "\n",
      "[SIOU] SoftIoU+BCE+Sobel (posw10, λ_edge=0.05) | thr=0.075 | val: P 0.019 R 0.178 F 0.034 | train: P 0.024 R 0.250 F 0.043\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.0316 | neg_mean≈0.0198 | P 0.019 R 0.183 F1 0.035 @ thr=0.075 | 0.5s\n",
      "\n",
      "=== Refinement Leaderboard (val_F then val_P) ===\n",
      "AFTL+BCE (α0.45 β0.55 γ1.3, posw6) | thr=0.212 | val: P 0.024 R 0.214 F 0.044 | train: P 0.019 R 0.247 F 0.035\n",
      "SoftIoU+BCE (posw10, λ_bce=0.2) | thr=0.188 | val: P 0.023 R 0.200 F 0.041 | train: P 0.019 R 0.221 F 0.035\n",
      "AFTL+BCE (α0.55 β0.45 γ1.5, posw6) | thr=0.188 | val: P 0.019 R 0.169 F 0.034 | train: P 0.023 R 0.218 F 0.042\n",
      "SoftIoU+BCE+Sobel (posw10, λ_edge=0.05) | thr=0.075 | val: P 0.019 R 0.178 F 0.034 | train: P 0.024 R 0.250 F 0.043\n",
      "\n",
      "Best (by val_F):\n",
      "{'name': 'AFTL+BCE (α0.45 β0.55 γ1.3, posw6)', 'thr': 0.21176470588235294, 'val_P': 0.02437893970745481, 'val_R': 0.2136382741543466, 'val_F': 0.04376384817139795, 'trn_P': 0.01891345004310463, 'trn_R': 0.24701352407811963, 'trn_F': 0.035136548017073575}\n"
     ]
    }
   ],
   "source": [
    "# ===== AFTL(adapt) refinements =====\n",
    "aftl_cfgs = [\n",
    "    # small BCE + recall-leaning AFTL\n",
    "    dict(name=\"AFTL+BCE (α0.45 β0.55 γ1.3, posw6)\",\n",
    "         loss=BCEPlusAFTL(bce_posw=6.0, aftl_alpha=0.45, aftl_beta=0.55, aftl_gamma=1.3,\n",
    "                          w_bce=0.20, w_aftl=0.80),\n",
    "         lr=1.5e-4),\n",
    "    # more FP pressure\n",
    "    dict(name=\"AFTL+BCE (α0.55 β0.45 γ1.5, posw6)\",\n",
    "         loss=BCEPlusAFTL(bce_posw=6.0, aftl_alpha=0.55, aftl_beta=0.45, aftl_gamma=1.5,\n",
    "                          w_bce=0.20, w_aftl=0.80),\n",
    "         lr=1.5e-4),\n",
    "]\n",
    "\n",
    "aftl_results = []\n",
    "for cfg in aftl_cfgs:\n",
    "    # clone probe weights to avoid interference\n",
    "    tmp = UNetResSEASPP(in_ch=1, out_ch=1).to(next(probe.parameters()).device)\n",
    "    tmp.load_state_dict(probe.state_dict(), strict=True)\n",
    "\n",
    "    trL,_ = brief_train(tmp, cfg[\"loss\"], train_loader_small, epochs=1, max_batches=400, lr=cfg[\"lr\"], wd=0.0)\n",
    "\n",
    "    # precision-friendlier band: 3–8%, β=1.0\n",
    "    thr, (P,R,F1), aux = pick_thr_with_floor(tmp, val_loader_small,\n",
    "                                            max_batches=60, n_bins=256, beta=1.0,\n",
    "                                            min_pos_rate=0.03, max_pos_rate=0.08)\n",
    "    vP,vR,vF = P,R,F1\n",
    "    tP,tR,tF = evaluate_PRF_on_loader(tmp, train_loader_small, thr, max_batches=40)\n",
    "\n",
    "    print(f\"\\n[AFTL] {cfg['name']} | thr={thr:.3f} | val: P {vP:.3f} R {vR:.3f} F {vF:.3f} | train: P {tP:.3f} R {tR:.3f} F {tF:.3f}\")\n",
    "    quick_prob_stats(tmp, train_loader_small, n_batches=3, thr=thr)\n",
    "\n",
    "    aftl_results.append(dict(name=cfg[\"name\"], thr=float(thr),\n",
    "                             val_P=float(vP), val_R=float(vR), val_F=float(vF),\n",
    "                             trn_P=float(tP), trn_R=float(tR), trn_F=float(tF)))\n",
    "\n",
    "# ===== SoftIoU refinements =====\n",
    "siou_cfgs = [\n",
    "    dict(name=\"SoftIoU+BCE (posw10, λ_bce=0.2)\", loss=BCEPlusSoftIoU(bce_posw=10.0, w_bce=0.20, w_iou=0.80), lr=1.5e-4),\n",
    "    dict(name=\"SoftIoU+BCE+Sobel (posw10, λ_edge=0.05)\", loss=BCEIoUPlusSobel(bce_posw=10.0, w_bce=0.20, w_iou=0.75, w_sobel=0.05), lr=1.5e-4),\n",
    "]\n",
    "\n",
    "siou_results = []\n",
    "for cfg in siou_cfgs:\n",
    "    tmp = UNetResSEASPP(in_ch=1, out_ch=1).to(next(probe.parameters()).device)\n",
    "    tmp.load_state_dict(probe.state_dict(), strict=True)\n",
    "\n",
    "    trL,_ = brief_train(tmp, cfg[\"loss\"], train_loader_small, epochs=1, max_batches=400, lr=cfg[\"lr\"], wd=0.0)\n",
    "\n",
    "    thr, (P,R,F1), aux = pick_thr_with_floor(tmp, val_loader_small,\n",
    "                                            max_batches=60, n_bins=256, beta=1.0,\n",
    "                                            min_pos_rate=0.03, max_pos_rate=0.08)\n",
    "    vP,vR,vF = P,R,F1\n",
    "    tP,tR,tF = evaluate_PRF_on_loader(tmp, train_loader_small, thr, max_batches=40)\n",
    "\n",
    "    print(f\"\\n[SIOU] {cfg['name']} | thr={thr:.3f} | val: P {vP:.3f} R {vR:.3f} F {vF:.3f} | train: P {tP:.3f} R {tR:.3f} F {tF:.3f}\")\n",
    "    quick_prob_stats(tmp, train_loader_small, n_batches=3, thr=thr)\n",
    "\n",
    "    siou_results.append(dict(name=cfg[\"name\"], thr=float(thr),\n",
    "                             val_P=float(vP), val_R=float(vR), val_F=float(vF),\n",
    "                             trn_P=float(tP), trn_R=float(tR), trn_F=float(tF)))\n",
    "\n",
    "\n",
    "all_results = (aftl_results if 'aftl_results' in globals() else []) + (siou_results if 'siou_results' in globals() else [])\n",
    "\n",
    "# sort by val_F desc, then by val_P desc (tie-breaker toward precision)\n",
    "all_results = sorted(all_results, key=lambda d: (d['val_F'], d['val_P']), reverse=True)\n",
    "\n",
    "print(\"\\n=== Refinement Leaderboard (val_F then val_P) ===\")\n",
    "for r in all_results:\n",
    "    print(f\"{r['name']:<30} | thr={r['thr']:.3f} | \"\n",
    "          f\"val: P {r['val_P']:.3f} R {r['val_R']:.3f} F {r['val_F']:.3f} | \"\n",
    "          f\"train: P {r['trn_P']:.3f} R {r['trn_R']:.3f} F {r['trn_F']:.3f}\")\n",
    "\n",
    "best = all_results[0] if all_results else None\n",
    "print(\"\\nBest (by val_F):\")\n",
    "print(best if best else \"No results produced.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b14196-4bb7-4d88-b453-60e97905b2e5",
   "metadata": {},
   "source": [
    "### Probe stabilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6cb726-65da-424d-8140-1ecc6c514c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — setup\n",
    "import time, copy, json, math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def init_head_bias_to_prior(model, p0=0.7):\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "            model.head.bias.data.fill_(b)\n",
    "\n",
    "def freeze_all(m):\n",
    "    for p in m.parameters(): p.requires_grad = False\n",
    "\n",
    "def unfreeze_head_only(m):\n",
    "    freeze_all(m)\n",
    "    assert hasattr(m, \"head\"), \"model has no .head\"\n",
    "    for p in m.head.parameters(): p.requires_grad = True\n",
    "\n",
    "@torch.no_grad()\n",
    "def quick_prob_stats(model, loader, n_batches=3, thr=0.5):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    pos_means, neg_means = [], []\n",
    "    tp=fp=fn=0.0\n",
    "    t0=time.time()\n",
    "    for b,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "        p = torch.sigmoid(logits)\n",
    "        if (yb_r>0.5).any(): pos_means.append(float(p[yb_r>0.5].mean()))\n",
    "        neg_means.append(float(p[yb_r<=0.5].mean()))\n",
    "        pv,tv = p.view(-1), yb_r.view(-1)\n",
    "        pred = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if b>=n_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "    import numpy as np\n",
    "    print(f\"[quick_prob_stats] batches={min(n_batches,b)} | pos≈{(np.mean(pos_means) if pos_means else float('nan')):.4f} \"\n",
    "          f\"| neg≈{np.mean(neg_means):.4f} | P {P:.3f} R {R:.3f} F1 {F1:.3f} @ thr={thr:.3f} | {time.time()-t0:.1f}s\")\n",
    "    return dict(P=P,R=R,F1=F1,pos_mean=(float('nan') if not pos_means else float(sum(pos_means)/len(pos_means))),\n",
    "                neg_mean=float(sum(neg_means)/len(neg_means)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0ae8ee-0ab4-44b1-a188-6bc09c2aae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — loss definition (finalist)\n",
    "class AsymFocalTversky(nn.Module):\n",
    "    def __init__(self, alpha=0.45, beta=0.55, gamma=1.3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.gamma, self.eps = alpha, beta, gamma, eps\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        p = torch.sigmoid(logits).clamp(self.eps, 1-self.eps)\n",
    "        p = p.view(p.size(0), -1); t = t.view(t.size(0), -1)\n",
    "        TP = (p*t).sum(1); FP = ((1-t)*p).sum(1); FN = (t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps) / (TP + self.alpha*FP + self.beta*FN + self.eps)\n",
    "        return torch.pow(1.0 - tv, self.gamma).mean()\n",
    "\n",
    "class AFTL_BCE(nn.Module):\n",
    "    \"\"\"\n",
    "    total = λ_BCE * BCE(pos_weight) + (1-λ_BCE) * Asym Focal Tversky\n",
    "    Defaults from your refinement winner.\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight=6.0, lambda_bce=0.2, alpha=0.45, beta=0.55, gamma=1.3):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        self.aftl = AsymFocalTversky(alpha=alpha, beta=beta, gamma=gamma)\n",
    "        self.lb = float(lambda_bce)\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        return self.lb * self.bce(logits, t) + (1.0-self.lb) * self.aftl(logits, t)\n",
    "\n",
    "def make_final_loss():\n",
    "    return AFTL_BCE(pos_weight=6.0, lambda_bce=0.2, alpha=0.45, beta=0.55, gamma=1.3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2192e8c-dbeb-4fec-b695-f674b82ad11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — tiny head-only fine tune (BCE-only) to keep calibration\n",
    "def fit_quick_head_only(model, loader, epochs=2, max_batches=200, lr=3e-5, metric_thr=0.15, pos_weight=5.0):\n",
    "    dev = next(model.parameters()).device\n",
    "    unfreeze_head_only(model)\n",
    "    head_params = [p for p in model.head.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.Adam(head_params, lr=lr, weight_decay=0.0)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); seen=tp=fp=fn=0.0; loss_sum=0.0\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits)\n",
    "                pv,tv = p.view(-1), yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[HEAD] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdbb7f7e-3fdd-47c8-804d-d9050fc2c3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup…\n",
      "[WARMUP] ep1 loss 0.3364 | F1 0.016 P 0.008 R 0.362\n",
      "[thr0] ≈ 0.196\n",
      "[HEAD] ep1 loss 0.1003 | F1 0.033 P 0.017 R 0.312\n",
      "[HEAD] ep2 loss 0.0914 | F1 0.030 P 0.016 R 0.201\n",
      "[quick_prob_stats] batches=6 | pos≈0.0989 | neg≈0.0422 | P 0.011 R 0.132 F1 0.021 @ thr=0.196 | 0.7s\n",
      "[QP] batch 500/3200 | loss=0.8152 | over>=thr 0.016 | 28.5s\n",
      "[QP] ep1 loss 0.8152 | F1 0.037 P 0.022 R 0.117\n",
      "[QP] batch 500/3200 | loss=0.8157 | over>=thr 0.019 | 28.4s\n",
      "[QP] ep2 loss 0.8157 | F1 0.032 P 0.020 R 0.084\n",
      "[quick_prob_stats] batches=6 | pos≈0.0958 | neg≈0.0321 | P 0.025 R 0.119 F1 0.041 @ thr=0.196 | 0.7s\n",
      "[FINAL] thr = 0.137 | P≈0.017 R≈0.249 F≈0.032 pos_rate≈0.051\n",
      "[quick_prob_stats] batches=6 | pos≈0.0903 | neg≈0.0321 | P 0.029 R 0.370 F1 0.055 @ thr=0.137 | 0.7s\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — end-to-end run & save\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Build & warm up\n",
    "probe = UNetResSEASPP(in_ch=1, out_ch=1).to(device)\n",
    "init_head_bias_to_prior(probe, p0=0.80)\n",
    "print(\"Warmup…\")\n",
    "fit_quick_warmup(\n",
    "    probe, train_loader_small,\n",
    "    epochs=1, max_batches=400,\n",
    "    lr=2e-4, metric_thr=0.20,\n",
    "    pos_weight=30.0\n",
    ")\n",
    "\n",
    "# Save a clean warm state\n",
    "warm_state = copy.deepcopy(probe.state_dict())\n",
    "\n",
    "# Recall-friendly pixel-threshold seed (use your fast picker)\n",
    "thr0, *_ = pick_thr_with_floor(\n",
    "    probe, val_loader_small,\n",
    "    max_batches=40, n_bins=256, beta=2.0,\n",
    "    min_pos_rate=0.03, max_pos_rate=0.10\n",
    ")\n",
    "thr0 = float(max(0.08, min(0.22, thr0)))  # clamp into a sane band\n",
    "print(f\"[thr0] ≈ {thr0:.3f}\")\n",
    "\n",
    "# Head-only calibration\n",
    "probe.load_state_dict(warm_state, strict=True)\n",
    "fit_quick_head_only(probe, train_loader_small, epochs=2, max_batches=300, lr=3e-5, metric_thr=thr0, pos_weight=5.0)\n",
    "quick_prob_stats(probe, train_loader_small, n_batches=6, thr=thr0)\n",
    "\n",
    "# Finalist loss pass (AFTL + BCE)\n",
    "probe.load_state_dict(warm_state, strict=True)     # start from the same warm base\n",
    "final_loss = make_final_loss()\n",
    "# light unfreeze: head + (optionally) last conv if your model exposes it; otherwise head-only is fine\n",
    "freeze_all(probe)\n",
    "for p in probe.head.parameters(): p.requires_grad = True\n",
    "\n",
    "fit_quick(\n",
    "    probe, final_loss, train_loader_small,\n",
    "    epochs=2, max_batches=500, lr=1.5e-4,\n",
    "    metric_thr=thr0, weight_decay=1e-4\n",
    ")\n",
    "quick_prob_stats(probe, train_loader_small, n_batches=6, thr=thr0)\n",
    "\n",
    "# Choose a working operating point (slightly precision-friendlier window)\n",
    "thr_final, (P,R,F1), aux = pick_thr_with_floor(\n",
    "    probe, val_loader_small,\n",
    "    max_batches=60, n_bins=256, beta=1.0,\n",
    "    min_pos_rate=0.05, max_pos_rate=0.12\n",
    ")\n",
    "print(f\"[FINAL] thr = {thr_final:.3f} | P≈{P:.3f} R≈{R:.3f} F≈{F1:.3f} pos_rate≈{aux['pos_rate']:.3f}\")\n",
    "final_stats = quick_prob_stats(probe, train_loader_small, n_batches=6, thr=thr_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba5b392b-a6f9-46df-9a0f-960e8f552286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint → probe_final.pth\n",
      "Saved metadata   → probe_final_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — save artifacts\n",
    "save_path = \"probe_final.pth\"\n",
    "meta_path = \"probe_final_meta.json\"\n",
    "\n",
    "meta = {\n",
    "    \"loss\": {\"type\":\"AFTL_BCE\", \"alpha\":0.45, \"beta\":0.55, \"gamma\":1.3, \"pos_weight\":6.0, \"lambda_bce\":0.2},\n",
    "    \"thr0\": thr0,\n",
    "    \"thr_final\": float(thr_final),\n",
    "    \"val_metrics\": {\"P\": float(P), \"R\": float(R), \"F\": float(F1), \"pos_rate\": float(aux[\"pos_rate\"])},\n",
    "    \"train_quick_stats\": final_stats,\n",
    "    \"notes\": \"Probe lock-in: warmup → head-cal → AFTL+BCE. Use thr_final for streaming eval.\"\n",
    "}\n",
    "\n",
    "torch.save({\n",
    "    \"model_state\": probe.state_dict(),\n",
    "    \"meta\": meta,\n",
    "}, save_path)\n",
    "\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved checkpoint → {save_path}\\nSaved metadata   → {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841136f-070c-4678-8a04-4c00940832f2",
   "metadata": {},
   "source": [
    "### Quick polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d255210-d5a5-4087-a141-3dfd7fcf3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head-only calibration (BCE-only)…\n",
      "[HEAD] ep1 loss 0.0978 | F1 0.038 P 0.021 R 0.255\n",
      "[quick_prob_stats] batches=6 | pos≈0.1018 | neg≈0.0281 | P 0.022 R 0.271 F1 0.040 @ thr=0.150 | 0.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P': 0.021611142167299025,\n",
       " 'R': 0.2710591745400298,\n",
       " 'F1': 0.04003069681024892,\n",
       " 'pos_mean': 0.10177816338837146,\n",
       " 'neg_mean': 0.02810964547097683}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 1: head-only calibration ===\n",
    "import torch, torch.nn as nn\n",
    "from contextlib import nullcontext\n",
    "\n",
    "device = next(probe.parameters()).device\n",
    "\n",
    "# helpers (lightweight fallbacks)\n",
    "def _set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters(): p.requires_grad = flag\n",
    "\n",
    "def freeze_all(model): _set_requires_grad(model, False)\n",
    "\n",
    "def unfreeze_head_only(model):\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        _set_requires_grad(model.head, True)\n",
    "    else:\n",
    "        raise AttributeError(\"Model has no attribute 'head'.\")\n",
    "\n",
    "print(\"Head-only calibration (BCE-only)…\")\n",
    "fit_quick_head_only(\n",
    "    probe, train_loader_small,\n",
    "    epochs=1, max_batches=500, lr=2e-5,\n",
    "    metric_thr=thr_current, pos_weight=6.0\n",
    ")\n",
    "\n",
    "# quick pixel stats at current thr\n",
    "quick_prob_stats(probe, train_loader_small, n_batches=6, thr=thr_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65d37984-da7c-4cc5-ae6b-8317857a4b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QP] batch 600/3200 | loss=0.6405 | over>=thr 0.050 | 52.8s\n",
      "[QP] ep1 loss 0.6405 | F1 0.036 P 0.019 R 0.284\n",
      "thr_step2 = 0.114 | P≈0.018 R≈0.261 F≈0.034 | pos_rate≈0.052\n",
      "[quick_prob_stats] batches=6 | pos≈0.0476 | neg≈0.0256 | P 0.007 R 0.268 F1 0.014 @ thr=0.114 | 0.7s\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: precision-leaning short pass (BCE + AFTL) ===\n",
    "class BCEPlusAFTL(nn.Module):\n",
    "    \"\"\"\n",
    "    λ_BCE = 0.35 (lean toward precision), pos_weight=8\n",
    "    AFTL: α=0.45, β=0.55, γ=1.3 (recall-friendly but not too FP-heavy)\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_bce=0.35, pos_weight=8.0, alpha=0.45, beta=0.55, gamma=1.3):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=next(probe.parameters()).device))\n",
    "        self.aftl = AsymFocalTversky(alpha=alpha, beta=beta, gamma=gamma)\n",
    "        self.lbce = float(lambda_bce)\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        return self.lbce*self.bce(logits, t) + (1.0-self.lbce)*self.aftl(logits, t)\n",
    "\n",
    "# Unfreeze head + tiny tail if you have helper; otherwise head-only fallback\n",
    "def unfreeze_head_and_last_conv(model):\n",
    "    # fallback-safe: head only; try to open a last conv if present\n",
    "    _set_requires_grad(model, False)\n",
    "    if hasattr(model, \"head\"): _set_requires_grad(model.head, True)\n",
    "    for attr in [\"u4\", \"up4\", \"decoder4\", \"dec4\"]:\n",
    "        if hasattr(model, attr):\n",
    "            block = getattr(model, attr)\n",
    "            try:\n",
    "                _set_requires_grad(block, True)  # gentle: if it unfreezes too much, it’s fine for a short pass\n",
    "            except Exception:\n",
    "                pass\n",
    "            break\n",
    "\n",
    "unfreeze_head_and_last_conv(probe)\n",
    "\n",
    "crit_mix = BCEPlusAFTL(lambda_bce=0.4, pos_weight=8.0,\n",
    "                       alpha=0.45, beta=0.55, gamma=1.3).to(device)\n",
    "\n",
    "# Short pass: 1 epoch, 400–600 batches, LR 1e-4, wd 1e-4\n",
    "fit_quick(\n",
    "    probe, crit_mix, train_loader_small,\n",
    "    epochs=1, max_batches=600, lr=1e-4,\n",
    "    metric_thr=thr_current, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Re-pick a threshold with a precision-friendlier band (5–10% pos-rate)\n",
    "thr_step2, (P2,R2,F2), aux2 = pick_thr_with_floor(\n",
    "    probe, val_loader_small,\n",
    "    max_batches=80, n_bins=256, beta=1.0,\n",
    "    min_pos_rate=0.05, max_pos_rate=0.10\n",
    ")\n",
    "print(f\"thr_step2 = {thr_step2:.3f} | P≈{P2:.3f} R≈{R2:.3f} F≈{F2:.3f} | pos_rate≈{aux2['pos_rate']:.3f}\")\n",
    "quick_prob_stats(probe, train_loader_small, n_batches=6, thr=thr_step2)\n",
    "\n",
    "# keep this as the new working threshold\n",
    "thr_current = float(thr_step2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54364a2e-05c6-4e9e-afe6-1dd5eeb9ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[check] current pos_rate≈0.052 @ thr≈0.114\n",
      "[FINAL] thr = 0.098 | P≈0.020 R≈0.327 F≈0.037 | pos_rate≈0.061\n",
      "[quick_prob_stats] batches=6 | pos≈0.0591 | neg≈0.0256 | P 0.012 R 0.617 F1 0.024 @ thr=0.098 | 0.7s\n",
      "\n",
      "ACCEPT if: val P ≥ 0.03–0.04 and val F ≥ 0.05 at [FINAL] thr.\n",
      "If not: re-run Cell 2 with lambda_bce=0.40 (slightly more precision-lean) or add a tiny Sobel edge term (λ_edge=0.03–0.05) for one short pass.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: tiny OHEM nibble (optional) + final threshold & acceptance check ===\n",
    "import torch\n",
    "# quick pos-rate probe on val with current thr\n",
    "_, (_, _, _), aux_chk = pick_thr_with_floor(\n",
    "    probe, val_loader_small,\n",
    "    max_batches=30, n_bins=128, beta=1.0,\n",
    "    min_pos_rate=0.05, max_pos_rate=0.20  # just to read back current pos_rate fast\n",
    ")\n",
    "pos_rate_now = aux_chk['pos_rate']\n",
    "print(f\"[check] current pos_rate≈{pos_rate_now:.3f} @ thr≈{thr_current:.3f}\")\n",
    "\n",
    "# If pos_rate > ~0.10, do a tiny OHEM BCE pass to trim glow (½–1 epoch)\n",
    "if pos_rate_now > 0.10:\n",
    "    print(\"Running tiny OHEM pass (neg_percent=0.02)…\")\n",
    "    crit_ohem = StreakSegLossFP(\n",
    "        w_aftl=0.0, w_lovasz=0.0, w_ohem=1.0,\n",
    "        ohem_neg_percent=0.02, bg_lambda=0.0,\n",
    "        pos_weight=torch.tensor(2.0, device=device)\n",
    "    ).to(device)\n",
    "    fit_quick(\n",
    "        probe, crit_ohem, train_loader_small,\n",
    "        epochs=1, max_batches=400, lr=5e-5,\n",
    "        metric_thr=thr_current, weight_decay=0.0\n",
    "    )\n",
    "\n",
    "# Final threshold: β=1.0, target 6–10% pos-rate\n",
    "thr_final, (Pf,Rf,Ff), auxf = pick_thr_with_floor(\n",
    "    probe, val_loader_small,\n",
    "    max_batches=100, n_bins=256, beta=1.0,\n",
    "    min_pos_rate=0.06, max_pos_rate=0.10\n",
    ")\n",
    "print(f\"[FINAL] thr = {thr_final:.3f} | P≈{Pf:.3f} R≈{Rf:.3f} F≈{Ff:.3f} | pos_rate≈{auxf['pos_rate']:.3f}\")\n",
    "quick_prob_stats(probe, train_loader_small, n_batches=6, thr=thr_final)\n",
    "\n",
    "# Acceptance hint (print only)\n",
    "print(\"\\nACCEPT if: val P ≥ 0.03–0.04 and val F ≥ 0.05 at [FINAL] thr.\")\n",
    "print(\"If not: re-run Cell 2 with lambda_bce=0.40 (slightly more precision-lean) or add a tiny Sobel edge term (λ_edge=0.03–0.05) for one short pass.\")\n",
    "# keep for downstream\n",
    "metric_thr = float(thr_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff2f3a-df1c-427d-8fc4-cc23d721ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] warm_state_after_head_gentle not found; using current probe weights as snapshot.\n",
      "== Running Option A (from snapshot) ==\n",
      "[QP] batch 500/3200 | loss=0.5892 | over>=thr 0.001 | 44.3s"
     ]
    }
   ],
   "source": [
    "# === Fair A/B from the same snapshot ===\n",
    "# Assumes you already have:\n",
    "# - probe class UNetResSEASPP\n",
    "# - train_loader_small / val_loader_small\n",
    "# - resize_masks_to, fit_quick, pick_thr_with_floor, quick_prob_stats\n",
    "# - a good snapshot in `warm_state_after_head_gentle` (falls back to probe.state_dict())\n",
    "\n",
    "import copy, time, torch, torch.nn as nn, torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------- Loss helpers ----------\n",
    "class AsymFocalTversky(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5, gamma=1.3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.gamma, self.eps = alpha, beta, gamma, eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        t = t.view(t.size(0), -1)\n",
    "        TP = (p*t).sum(1)\n",
    "        FP = ((1-t)*p).sum(1)\n",
    "        FN = (t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps) / (TP + self.alpha*FP + self.beta*FN + self.eps)\n",
    "        return torch.pow(1.0 - tv, self.gamma).mean()\n",
    "\n",
    "class SoftIoU(nn.Module):\n",
    "    def __init__(self, eps=1e-6): super().__init__(); self.eps = eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits); t = targets.clamp(0,1)\n",
    "        inter = (p*t).sum(dim=(1,2,3))\n",
    "        union = (p + t - p*t).sum(dim=(1,2,3)) + self.eps\n",
    "        iou = (inter + self.eps) / union\n",
    "        return (1.0 - iou).mean()\n",
    "\n",
    "class BCE_AFTL(nn.Module):\n",
    "    \"\"\"Option A: BCE-lean AFTL (pos_weight≈6, AFTL α≈0.5 β≈0.5 γ≈1.3, w_bce≈0.45)\"\"\"\n",
    "    def __init__(self, pos_weight=6.0, alpha=0.5, beta=0.5, gamma=1.3, w_bce=0.45):\n",
    "        super().__init__()\n",
    "        self.w_bce = float(w_bce)\n",
    "        self.bce   = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        self.aftl  = AsymFocalTversky(alpha=alpha, beta=beta, gamma=gamma)\n",
    "        self.register_buffer(\"posw\", torch.tensor(float(pos_weight)))\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        loss_bce  = F.binary_cross_entropy_with_logits(logits, t, pos_weight=self.posw)\n",
    "        loss_aftl = self.aftl(logits, t)\n",
    "        return self.w_bce*loss_bce + (1.0-self.w_bce)*loss_aftl\n",
    "\n",
    "class BCE_SoftIoU_Sobel(nn.Module):\n",
    "    \"\"\"Option B: SoftIoU+BCE (+ small Sobel edge)  λ_bce≈0.3, posw≈10, λ_edge≈0.05\"\"\"\n",
    "    def __init__(self, lambda_bce=0.30, pos_weight=10.0, lambda_edge=0.05):\n",
    "        super().__init__()\n",
    "        self.l_bce, self.l_edge = float(lambda_bce), float(lambda_edge)\n",
    "        self.siou = SoftIoU()\n",
    "        self.register_buffer(\"posw\", torch.tensor(float(pos_weight)))\n",
    "        # Sobel kernels as buffers (float32); we cast at use time to logits dtype\n",
    "        kx = torch.tensor([[[-1,0,1],[-2,0,2],[-1,0,1]]], dtype=torch.float32)\n",
    "        ky = torch.tensor([[[-1,-2,-1],[0,0,0],[1,2,1]]], dtype=torch.float32)\n",
    "        self.register_buffer(\"kx\", kx.view(1,1,3,3))\n",
    "        self.register_buffer(\"ky\", ky.view(1,1,3,3))\n",
    "    def sobel_mag(self, x):\n",
    "        # cast kernels to input dtype/device to avoid dtype/device mismatch\n",
    "        kx = self.kx.to(dtype=x.dtype, device=x.device)\n",
    "        ky = self.ky.to(dtype=x.dtype, device=x.device)\n",
    "        gx = F.conv2d(x, kx, padding=1)\n",
    "        gy = F.conv2d(x, ky, padding=1)\n",
    "        return torch.sqrt(gx*gx + gy*gy + 1e-12)\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        loss_siou = self.siou(logits, t)\n",
    "        loss_bce  = F.binary_cross_entropy_with_logits(logits, t, pos_weight=self.posw)\n",
    "        if self.l_edge > 0:\n",
    "            p  = torch.sigmoid(logits)\n",
    "            e_pred = self.sobel_mag(p)\n",
    "            e_true = self.sobel_mag(t)\n",
    "            loss_edge = F.l1_loss(e_pred, e_true)\n",
    "        else:\n",
    "            loss_edge = 0.0\n",
    "        return (1.0 - self.l_bce)*loss_siou + self.l_bce*loss_bce + self.l_edge*loss_edge\n",
    "\n",
    "# ---------- Metric helper (quiet) ----------\n",
    "@torch.no_grad()\n",
    "def pixel_prf(model, loader, thr=0.5, n_batches=6):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    tp=fp=fn=0.0; posm=[]; negm=[]\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yb_r   = resize_masks_to(logits, yb)\n",
    "        p      = torch.sigmoid(logits)\n",
    "        if (yb_r>0.5).any(): posm.append(float(p[yb_r>0.5].mean()))\n",
    "        negm.append(float(p[yb_r<=0.5].mean()))\n",
    "        pv, tv = p.view(-1), yb_r.view(-1)\n",
    "        pred = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if i>=n_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F = 2*P*R/max(P+R,1e-8)\n",
    "    import numpy as np\n",
    "    return dict(P=P, R=R, F=F, pos_mean=float(np.mean(posm) if posm else float(\"nan\")), neg_mean=float(np.mean(negm)))\n",
    "\n",
    "# ---------- Runner for one option ----------\n",
    "def run_option(label, make_criterion, snapshot, train_lr=1e-4, max_batches=600):\n",
    "    model = UNetResSEASPP(in_ch=1, out_ch=1).to(device)\n",
    "    model.load_state_dict(snapshot, strict=True)\n",
    "    crit  = make_criterion().to(device)\n",
    "    # short, equal-length pass\n",
    "    fit_quick(model, crit, train_loader_small,\n",
    "              epochs=1, max_batches=max_batches, lr=train_lr,\n",
    "              metric_thr=0.20, weight_decay=0.0)\n",
    "    # pick thr in 5–10% pos-rate band (beta=1 → F1-ish)\n",
    "    thr, (P,R,F1), aux = pick_thr_with_floor(model, val_loader_small,\n",
    "                                             max_batches=60, n_bins=256, beta=1.0,\n",
    "                                             min_pos_rate=0.05, max_pos_rate=0.10)\n",
    "    val_stats = pixel_prf(model, val_loader_small, thr=thr, n_batches=6)\n",
    "    print(f\"\\n[{label}] thr={thr:.3f} | val: P {val_stats['P']:.3f} R {val_stats['R']:.3f} F {val_stats['F']:.3f} | pos_rate≈{aux['pos_rate']:.3f}\")\n",
    "    # (optional) quick train-side sanity at the same thr\n",
    "    trn_stats = pixel_prf(model, train_loader_small, thr=thr, n_batches=6)\n",
    "    return dict(label=label, thr=float(thr), pos_rate=float(aux['pos_rate']),\n",
    "                val=val_stats, trn=trn_stats, model=model)\n",
    "\n",
    "# ---------- Snapshot to use ----------\n",
    "if 'warm_state_after_head_gentle' in globals():\n",
    "    SNAP = warm_state_after_head_gentle\n",
    "else:\n",
    "    print(\"[warn] warm_state_after_head_gentle not found; using current probe weights as snapshot.\")\n",
    "    probe.load_state_dict(torch.load(\"probe_final.pth\", weights_only=True)[\"model_state\"])\n",
    "    SNAP = copy.deepcopy(probe.state_dict())\n",
    "# ---------- Define the two options ----------\n",
    "def make_option_A():\n",
    "    # BCE-lean AFTL: pos_weight≈6, α=β=0.5, γ=1.3, w_bce≈0.45\n",
    "    return BCE_AFTL(pos_weight=6.0, alpha=0.5, beta=0.5, gamma=1.3, w_bce=0.45)\n",
    "\n",
    "def make_option_B():\n",
    "    # SoftIoU+BCE+Sobel: λ_bce=0.30, posw=10, λ_edge=0.05\n",
    "    return BCE_SoftIoU_Sobel(lambda_bce=0.30, pos_weight=10.0, lambda_edge=0.05)\n",
    "\n",
    "# ---------- Run A then B (each from the same snapshot) ----------\n",
    "print(\"== Running Option A (from snapshot) ==\")\n",
    "resA = run_option(\"Option A: BCE-lean AFTL\", make_option_A, SNAP, train_lr=1e-4, max_batches=600)\n",
    "\n",
    "print(\"\\n== Running Option B (from snapshot) ==\")\n",
    "resB = run_option(\"Option B: SoftIoU+BCE+Sobel\", make_option_B, SNAP, train_lr=1e-4, max_batches=600)\n",
    "\n",
    "# ---------- Side-by-side summary ----------\n",
    "def _fmt(res):\n",
    "    return (f\"{res['label']} | thr={res['thr']:.3f} | \"\n",
    "            f\"val: P {res['val']['P']:.3f} R {res['val']['R']:.3f} F {res['val']['F']:.3f} \"\n",
    "            f\"| pos_rate≈{res['pos_rate']:.3f}\")\n",
    "\n",
    "print(\"\\n=== Summary (val_F, then val_P tie-break) ===\")\n",
    "print(_fmt(resA))\n",
    "print(_fmt(resB))\n",
    "\n",
    "best = resA if resA['val']['F']>resB['val']['F'] or (abs(resA['val']['F']-resB['val']['F'])<1e-6 and resA['val']['P']>resB['val']['P']) else resB\n",
    "print(\"\\nBest option:\", best['label'])\n",
    "print(\"Use this model+thr for the next stage.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Asteroid detection",
   "language": "python",
   "name": "asteroid_detection_cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
