{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c858f10-99bf-415d-b702-b24ec41df849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- config ---\n",
    "DATA = {\n",
    "    \"train_h5\": \"/home/karlo/train_chunked.h5\",\n",
    "    \"test_h5\":  \"../DATA/test.h5\",\n",
    "    \"train_csv\": \"../DATA/train.csv\",   # or None if you don’t have it\n",
    "    \"test_csv\":  \"../DATA/test.csv\",\n",
    "}\n",
    "TILE         = 128\n",
    "BATCH        = 128\n",
    "NUM_WORKERS  = 2          # HDF5 is happier with 0–2\n",
    "SEED         = 1337\n",
    "EPOCHS       = 20\n",
    "MAX_LR       = 1.5e-4\n",
    "SAVE_BEST    = \"./best_unet_resse_aspp.pt\"\n",
    "SAVE_LAST    = \"./last_unet_resse.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce357f68-f532-4b6a-90e3-5480a4d457ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, math, random, h5py\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(s=1337):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(SEED)\n",
    "\n",
    "def resize_masks_to(logits, masks):\n",
    "    \"\"\"\n",
    "    Resize binary masks to the spatial size of logits.\n",
    "    - nearest neighbor to avoid soft edges\n",
    "    - keep masks in {0,1}\n",
    "    \"\"\"\n",
    "    H, W = logits.shape[-2:]\n",
    "    if masks.dtype != torch.float32:\n",
    "        masks = masks.float()\n",
    "    if masks.dim() == 3:  # (B,H,W) -> (B,1,H,W)\n",
    "        masks = masks.unsqueeze(1)\n",
    "    out = torch.nn.functional.interpolate(masks, size=(H, W), mode='nearest')\n",
    "    # keep it strictly 0/1 after nearest\n",
    "    return (out > 0.5).float()\n",
    "\n",
    "\n",
    "def _free_cuda():\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c33a01-45c0-49e4-b593-fe75013bedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stats_mad(arr):\n",
    "    med = np.median(arr); mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)\n",
    "    return np.float32(med), np.float32(1.0 if not np.isfinite(sigma) or sigma<=0 else sigma)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"Stream tiles from big (H,W) images, robust-normalize per-image, k-sigma clip, pad edges.\"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, crop_for_stats=512):\n",
    "        self.h5_path, self.tile, self.k_sigma, self.crop_for_stats = h5_path, int(tile), float(k_sigma), int(crop_for_stats)\n",
    "        self._h5 = self._x = self._y = None\n",
    "        self._stats_cache = {}\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "        Hb = math.ceil(self.H/self.tile); Wb = math.ceil(self.W/self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "    def _ensure(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self._x, self._y = self._h5[\"images\"], self._h5[\"masks\"]\n",
    "    def _image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        s = min(self.crop_for_stats, self.H, self.W)\n",
    "        h0, w0 = (self.H-s)//2, (self.W-s)//2\n",
    "        crop = self._x[i, h0:h0+s, w0:w0+s].astype(\"float32\")\n",
    "        med, sig = robust_stats_mad(crop); self._stats_cache[i] = (med, sig); return med, sig\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure()\n",
    "        i, r, c = self.indices[idx]; t = self.tile\n",
    "        r0, c0 = r*t, c*t; r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "        x = self._x[i, r0:r1, c0:c1].astype(\"float32\"); y = self._y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        if x.shape != (t,t):\n",
    "            xp = np.zeros((t,t), np.float32); yp = np.zeros((t,t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x; yp[:y.shape[0], :y.shape[1]] = y; x, y = xp, yp\n",
    "        med, sig = self._image_stats(i); x = np.clip((x-med)/sig, -5, 5)\n",
    "        return torch.from_numpy(x[None,...]), torch.from_numpy(y[None,...])\n",
    "\n",
    "class SubsetDS(Dataset):\n",
    "    \"\"\"Select full panels by id while reusing tiling of base dataset.\"\"\"\n",
    "    def __init__(self, base, panel_ids):\n",
    "        self.base, self.panel_ids = base, np.asarray(panel_ids)\n",
    "        t = base.tile; Hb, Wb = math.ceil(base.H/t), math.ceil(base.W/t)\n",
    "        base_map = {(i,r,c):k for k,(i,r,c) in enumerate(base.indices)}\n",
    "        self.map = [base_map[(i,r,c)] for i in self.panel_ids for r in range(Hb) for c in range(Wb)]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, k): return self.base[self.map[k]]\n",
    "\n",
    "def tile_pos_weights(h5_path, tile=128):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        Y = f[\"masks\"]; N,H,W = Y.shape\n",
    "    Hb, Wb = math.ceil(H/tile), math.ceil(W/tile)\n",
    "    w = []\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        Y = f[\"masks\"]\n",
    "        for i in range(N):\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0,c0=r*tile,c*tile; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    w.append(1.0 + 9.0*(Y[i,r0:r1,c0:c1].any()))\n",
    "    return np.asarray(w, np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a21a47e-7bf4-44a7-b09e-706b8f7e3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self,c,r=8): super().__init__(); self.fc1=nn.Conv2d(c,c//r,1); self.fc2=nn.Conv2d(c//r,c,1)\n",
    "    def forward(self,x): s=F.adaptive_avg_pool2d(x,1); s=F.silu(self.fc1(s),inplace=True); s=torch.sigmoid(self.fc2(s)); return x*s\n",
    "def _norm(c, groups=8): g=min(groups,c) if c%groups==0 else 1; return nn.GroupNorm(g,c)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); p=1\n",
    "    def __init__(self,c_in,c_out,k=3,act=nn.SiLU,se=True):\n",
    "        super().__init__(); p=k//2\n",
    "        self.proj = nn.Identity() if c_in==c_out else nn.Conv2d(c_in,c_out,1)\n",
    "        self.bn1=_norm(c_in); self.c1=nn.Conv2d(c_in,c_out,k,padding=p,bias=False)\n",
    "        self.bn2=_norm(c_out); self.c2=nn.Conv2d(c_out,c_out,k,padding=p,bias=False)\n",
    "        self.act=act(); self.se=SEBlock(c_out) if se else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        h=self.act(self.bn1(x)); h=self.c1(h)\n",
    "        h=self.act(self.bn2(h)); h=self.c2(h)\n",
    "        h=self.se(h); return h + self.proj(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); self.pool=nn.MaxPool2d(2); self.rb=ResBlock(c_in,c_out)\n",
    "    def forward(self,x): return self.rb(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self,c_in,c_skip,c_out): super().__init__(); self.up=nn.ConvTranspose2d(c_in,c_in,2,stride=2); self.rb1=ResBlock(c_in+c_skip,c_out); self.rb2=ResBlock(c_out,c_out)\n",
    "    def forward(self,x,skip):\n",
    "        x=self.up(x)\n",
    "        dh=skip.size(-2)-x.size(-2); dw=skip.size(-1)-x.size(-1)\n",
    "        if dh or dw: x=F.pad(x,(0,max(0,dw),0,max(0,dh)))\n",
    "        x=torch.cat([x,skip],1); x=self.rb1(x); x=self.rb2(x); return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self,c,r=[1,6,12,18]):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList([nn.Sequential(nn.Conv2d(c,c//4,3,padding=d,dilation=d,bias=False), nn.BatchNorm2d(c//4), nn.SiLU(True)) for d in r])\n",
    "        self.project=nn.Conv2d(c,c,1)\n",
    "    def forward(self,x): return self.project(torch.cat([b(x) for b in self.blocks],1))\n",
    "\n",
    "class UNetResSE(nn.Module):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(); w=widths\n",
    "        self.stem=nn.Sequential(nn.Conv2d(in_ch,w[0],3,padding=1,bias=False), nn.BatchNorm2d(w[0]), nn.SiLU(True), ResBlock(w[0],w[0]))\n",
    "        self.d1=Down(w[0],w[1]); self.d2=Down(w[1],w[2]); self.d3=Down(w[2],w[3]); self.d4=Down(w[3],w[4])\n",
    "        self.u1=Up(w[4],w[3],w[3]); self.u2=Up(w[3],w[2],w[2]); self.u3=Up(w[2],w[1],w[1]); self.u4=Up(w[1],w[0],w[0])\n",
    "        self.head=nn.Conv2d(w[0],out_ch,1)\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x) # logits\n",
    "\n",
    "class UNetResSEASPP(UNetResSE):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(in_ch,out_ch,widths); self.aspp=ASPP(widths[-1]); self.d4=Down(widths[3],widths[4])\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3); b=self.aspp(b)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self,eps=1e-6): super().__init__(); self.eps=eps\n",
    "    def forward(self,logits,targets):\n",
    "        p=torch.sigmoid(logits); p=p.view(p.size(0),-1); t=targets.view(targets.size(0),-1)\n",
    "        inter=(p*t).sum(1); denom=p.sum(1)+t.sum(1); dice=(2*inter+self.eps)/(denom+self.eps); return 1-dice.mean()\n",
    "\n",
    "class AdaptiveComboLoss(nn.Module):\n",
    "    \"\"\"Recall/precision-adaptive blend (FT + BCE [+ Dice]).\"\"\"\n",
    "    def __init__(self, w_ft=0.6, w_bce=0.4, w_dice=0.0, alpha=0.3, beta=0.7, gamma=0.75, ema=0.9, clamp=1e-6, use_dice=False, bce_pos_weight=None):\n",
    "        super().__init__(); self.w_ft0, self.w_bce0, self.w_dice0 = w_ft,w_bce,w_dice\n",
    "        self.alpha0,self.beta0,self.gamma = alpha,beta,gamma; self.ema=ema; self.clamp=clamp; self.use_dice=use_dice\n",
    "        self.register_buffer('ema_prec', torch.tensor(0.5)); self.register_buffer('ema_rec', torch.tensor(0.5))\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=bce_pos_weight) if bce_pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "    @staticmethod\n",
    "    def _dice_prob(p,t,eps): p=p.view(p.size(0),-1); t=t.view(t.size(0),-1); inter=(p*t).sum(1); return (2*inter+eps)/(p.sum(1)+t.sum(1)+eps)\n",
    "    def forward(self,logits,targets):\n",
    "        p=torch.sigmoid(logits).clamp(self.clamp,1-self.clamp); t=targets.clamp(0,1)\n",
    "        with torch.no_grad():\n",
    "            ph=(p>=0.5).float(); tp=(ph*t).sum().item(); fp=(ph*(1-t)).sum().item(); fn=((1-ph)*t).sum().item()\n",
    "            prec=tp/(tp+fp+1e-8); rec=tp/(tp+fn+1e-8)\n",
    "            self.ema_prec = self.ema*self.ema_prec + (1-self.ema)*p.new_tensor(prec)\n",
    "            self.ema_rec  = self.ema*self.ema_rec  + (1-self.ema)*p.new_tensor(rec)\n",
    "        pr_gap=(self.ema_prec-self.ema_rec).clamp(-0.5,0.5)   # >0 means precision>recall\n",
    "        alpha=(self.alpha0+0.3*pr_gap).clamp(0.05,0.95); beta=(self.beta0-0.3*pr_gap).clamp(0.05,0.95)\n",
    "        pv=p.view(p.size(0),-1); tv=t.view(t.size(0),-1)\n",
    "        TP=(pv*tv).sum(1); FP=((1-tv)*pv).sum(1); FN=(tv*(1-pv)).sum(1)\n",
    "        tversky=(TP+self.clamp)/(TP+alpha*FP+beta*FN+self.clamp)\n",
    "        ft_loss=torch.pow(1.0-tversky, self.gamma).mean()\n",
    "        bce_loss=self.bce(logits,t)\n",
    "        dice_loss=(1.0-self._dice_prob(p,t,self.clamp).mean()) if self.use_dice else logits.new_tensor(0.0)\n",
    "        ft_w=(self.w_ft0 + 0.4*(-pr_gap)).clamp(0.2,0.8); bce_w=(self.w_bce0 + 0.4*(pr_gap)).clamp(0.2,0.8)\n",
    "        return ft_w*ft_loss + bce_w*bce_loss + self.w_dice0*dice_loss\n",
    "\n",
    "def estimate_pos_weight(h5_path, max_panels=64, tile=TILE):\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        Y=f[\"masks\"]; N,H,W=Y.shape\n",
    "        idx=np.random.choice(N, min(N,max_panels), replace=False)\n",
    "        pos=tot=0\n",
    "        for i in idx:\n",
    "            Hb,Wb=math.ceil(H/tile),math.ceil(W/tile)\n",
    "            y=Y[i]\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0,c0=r*tile,c*tile; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    patch=y[r0:r1,c0:c1]; pos+=patch.sum(); tot+=patch.size\n",
    "    p=max(1e-8, pos/max(1,tot))\n",
    "    return torch.tensor((1-p)/p, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf7a13b-a3e2-42df-bf83-b02bd1427621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Lovasz hinge (binary) --------\n",
    "# https://arxiv.org/abs/1705.08790 (adapted, compact)\n",
    "def lovasz_grad(gt_sorted):\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1.0 - intersection / torch.clamp(union, min=1e-12)\n",
    "    if p > 1:\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:p-1]\n",
    "    return jaccard\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    # logits: (P,), labels: (P,) in {0,1}\n",
    "    if logits.numel() == 0:\n",
    "        return logits*0.0\n",
    "    signs = 2.0 * labels.float() - 1.0\n",
    "    errors = (1.0 - logits * signs)  # margin errors\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.clamp(errors_sorted, min=0).dot(grad)\n",
    "    return loss\n",
    "\n",
    "def lovasz_hinge(logits, targets):\n",
    "    # logits/targets: (B,1,H,W)\n",
    "    losses = []\n",
    "    B = logits.size(0)\n",
    "    for b in range(B):\n",
    "        l = logits[b].view(-1)\n",
    "        t = targets[b].view(-1).float()\n",
    "        losses.append(lovasz_hinge_flat(l, t))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "# -------- Asymmetric Focal Tversky --------\n",
    "class AsymFocalTversky(nn.Module):\n",
    "    def __init__(self, alpha=0.2, beta=0.8, gamma=0.75, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.gamma, self.eps = alpha, beta, gamma, eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits).clamp(self.eps, 1 - self.eps)\n",
    "        t = targets.clamp(0,1)\n",
    "        # flatten per-example\n",
    "        p = p.view(p.size(0), -1)\n",
    "        t = t.view(t.size(0), -1)\n",
    "        TP = (p * t).sum(1)\n",
    "        FP = ((1 - t) * p).sum(1)\n",
    "        FN = (t * (1 - p)).sum(1)\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha * FP + self.beta * FN + self.eps)\n",
    "        loss = torch.pow(1.0 - tversky, self.gamma)\n",
    "        return loss.mean()\n",
    "\n",
    "# -------- Hard-negative-mining BCE --------\n",
    "class OHEMBCE(nn.Module):\n",
    "    def __init__(self, neg_percent=0.1, pos_weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.neg_percent = neg_percent\n",
    "        self.pos_weight = pos_weight  # tensor or None\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        # per-pixel BCE (no reduction)\n",
    "        if self.pos_weight is not None:\n",
    "            bce = F.binary_cross_entropy_with_logits(\n",
    "                logits, targets, pos_weight=self.pos_weight, reduction='none'\n",
    "            )\n",
    "        else:\n",
    "            bce = F.binary_cross_entropy_with_logits(\n",
    "                logits, targets, reduction='none'\n",
    "            )\n",
    "        # Separate pos/neg pixels\n",
    "        pos_mask = (targets > 0.5)\n",
    "        neg_mask = ~pos_mask\n",
    "\n",
    "        pos_loss = bce[pos_mask]\n",
    "        neg_loss = bce[neg_mask]\n",
    "\n",
    "        if neg_loss.numel() > 0 and self.neg_percent > 0:\n",
    "            k = max(1, int(self.neg_percent * neg_loss.numel()))\n",
    "            hard_neg, _ = torch.topk(neg_loss.view(-1), k, sorted=False)\n",
    "            loss = torch.cat([pos_loss.view(-1), hard_neg], 0)\n",
    "        else:\n",
    "            loss = bce.view(-1)\n",
    "\n",
    "        return loss.mean() if self.reduction == 'mean' else loss.sum()\n",
    "\n",
    "# -------- Composite loss --------\n",
    "class StreakSegLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    total = w_aftl * Asymmetric Focal Tversky\n",
    "          + w_lovasz * Lovasz Hinge (IoU surrogate on logits)\n",
    "          + w_ohem  * OHEM-BCE (all positives + hardest q% negatives)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 w_aftl=0.5, w_lovasz=0.3, w_ohem=0.2,\n",
    "                 aftl_alpha=0.2, aftl_beta=0.8, aftl_gamma=0.75,\n",
    "                 ohem_neg_percent=0.1,\n",
    "                 pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.w_aftl   = w_aftl\n",
    "        self.w_lovasz = w_lovasz\n",
    "        self.w_ohem   = w_ohem\n",
    "        self.aftl = AsymFocalTversky(alpha=aftl_alpha, beta=aftl_beta, gamma=aftl_gamma)\n",
    "        self.ohem = OHEMBCE(neg_percent=ohem_neg_percent, pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        la = self.aftl(logits, targets)\n",
    "        ll = lovasz_hinge(logits, (targets>0.5).float())\n",
    "        lo = self.ohem(logits, targets)\n",
    "        return self.w_aftl * la + self.w_lovasz * ll + self.w_ohem * lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7016a42c-1c59-425a-bcbd-7aeaf69bf76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(device.type=='cuda'))\n",
    "\n",
    "def _auc_from_hists(pos_hist, neg_hist):\n",
    "    tp=np.cumsum(pos_hist[::-1]); fp=np.cumsum(neg_hist[::-1])\n",
    "    if tp[-1]==0 or fp[-1]==0: return float('nan')\n",
    "    tpr=tp/tp[-1]; fpr=fp/fp[-1]\n",
    "    tpr=np.concatenate(([0],tpr)); fpr=np.concatenate(([0],fpr))\n",
    "    return np.trapz(tpr,fpr)\n",
    "\n",
    "def run_epoch(loader, model, criterion, optimizer=None, train=True, tag=\"Train\", print_every=10, n_bins=512, grad_clip=1.0):\n",
    "    model.train(train); start=time.time()\n",
    "    total=len(loader.dataset); seen=0; loss_sum=0.0; tp=fp=fn=0.0\n",
    "    pos_hist=np.zeros(n_bins,np.float64); neg_hist=np.zeros(n_bins,np.float64)\n",
    "    amp_ctx=torch.amp.autocast('cuda', enabled=(device.type=='cuda'))\n",
    "    for b,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(device,non_blocking=True), yb.to(device,non_blocking=True)\n",
    "        ctx = nullcontext() if train else torch.inference_mode()\n",
    "        with ctx:\n",
    "            with amp_ctx:\n",
    "                logits = model(xb)\n",
    "                yb_r = resize_masks_to(logits, yb)\n",
    "                loss  = criterion(logits, yb_r)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "        bs=xb.size(0); seen+=bs; loss_sum += float(loss.item())*bs\n",
    "        with torch.no_grad():\n",
    "            p=torch.sigmoid(logits).detach().view(-1).cpu(); t=yb_r.detach().view(-1).cpu()\n",
    "            pb=(p>=0.5).float(); tp+=float((pb*t).sum()); fp+=float((pb*(1-t)).sum()); fn+=float(((1-pb)*t).sum())\n",
    "            idx=torch.clamp((p*(n_bins-1)).long(),0,n_bins-1)\n",
    "            pos_hist += np.bincount(idx[t>0.5].numpy(), minlength=n_bins)\n",
    "            neg_hist += np.bincount(idx[t<=0.5].numpy(), minlength=n_bins)\n",
    "        if ((b%print_every==0) or (seen==total)) and train:\n",
    "            P=tp/max(tp+fp,1); R=tp/max(tp+fn,1); F1=2*P*R/max(P+R,1e-8)\n",
    "            print(f\"\\r[{tag}] batch {b}/{len(loader)} | {seen}/{total} ex | loss={loss_sum/seen:.4f} | F1 {F1:.4f} | P {P:.4f} | R {R:.4f} | {time.time()-start:.1f}s\", end='', flush=True)\n",
    "    if train: print()\n",
    "    auc=_auc_from_hists(pos_hist,neg_hist); P=tp/max(tp+fp,1); R=tp/max(tp+fn,1); F1=2*P*R/max(P+R,1e-8)\n",
    "    return (loss_sum/total), auc, P, R, F1\n",
    "\n",
    "def rebalance_loss_blend(loss_obj, ep, total):\n",
    "    t = min(1.0, max(0.0, ep/float(max(total-1,1))))\n",
    "    loss_obj.w_ft0 = 0.8 - 0.2*t\n",
    "    loss_obj.w_bce0 = 0.2 + 0.2*t\n",
    "\n",
    "def fit(model, train_loader, val_loader, criterion, epochs=EPOCHS, max_lr=MAX_LR, save_path=SAVE_BEST, early_stop_patience=10):\n",
    "    model=model.to(device)\n",
    "    opt=torch.optim.Adam(model.parameters(), lr=max_lr, weight_decay=1e-5)\n",
    "    sched=torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=max_lr, steps_per_epoch=len(train_loader), epochs=epochs, pct_start=0.1, anneal_strategy='cos')\n",
    "    best_f1=-1; no_improve=0\n",
    "    for ep in range(1,epochs+1):\n",
    "        rebalance_loss_blend(criterion, ep-1, epochs)\n",
    "        t0=time.time()\n",
    "        trL,_,tP,tR,tF1 = run_epoch(train_loader, model, criterion, opt, train=True, tag=\"Train\")\n",
    "        _free_cuda()\n",
    "        vaL,vaA,vaP,vaR,vaF1 = run_epoch(val_loader,   model, criterion, None, train=False, tag=\"Val\")\n",
    "        _free_cuda()\n",
    "        sched.step()\n",
    "        print(f\"Epoch {ep:03d} | Train L {trL:.4f} F1 {tF1:.4f} P {tP:.4f} R {tR:.4f} || Val L {vaL:.4f} AUC {vaA:.4f} F1 {vaF1:.4f} P {vaP:.4f} R {vaR:.4f} | {time.time()-t0:.1f}s\")\n",
    "        torch.save({\"state_dict\": model.state_dict()}, SAVE_LAST)\n",
    "        if vaF1 > best_f1 + 1e-4:\n",
    "            best_f1, no_improve = vaF1, 0\n",
    "            torch.save({\"state_dict\": model.state_dict()}, save_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stop_patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8107fca-9537-4f92-a3fa-6429ae50d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA[\"train_h5\"], \"r\") as f:\n",
    "    N = f[\"images\"].shape[0]\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "split = int(0.9*N); idx_tr, idx_va = np.sort(idx[:split]), np.sort(idx[split:])\n",
    "\n",
    "ds_full  = H5TiledDataset(DATA[\"train_h5\"], tile=TILE, k_sigma=5.0)\n",
    "train_ds = SubsetDS(ds_full, idx_tr)\n",
    "val_ds   = SubsetDS(ds_full, idx_va)\n",
    "test_ds  = H5TiledDataset(DATA[\"test_h5\"], tile=TILE, k_sigma=5.0)\n",
    "\n",
    "# Weighted sampler (positives 10x)\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "w_per_tile  = tile_pos_weights(DATA[\"train_h5\"], tile=TILE)\n",
    "w_for_train = torch.tensor([w_per_tile[k] for k in train_ds.map], dtype=torch.double)\n",
    "sampler     = WeightedRandomSampler(w_for_train, num_samples=len(train_ds), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=max(32,BATCH//2), shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c6ed5fa-3a13-4d82-bd9e-89db0dd8d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label as cc_label, find_objects, binary_opening\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def cuda_oom_guard():\n",
    "    try: yield\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache(); raise MemoryError(\"cuda_oom\")\n",
    "        raise\n",
    "\n",
    "@torch.no_grad()\n",
    "def stream_panels_direct(h5_path, model, tile=TILE, start_batch_tiles=64):\n",
    "    model.eval(); dev=next(model.parameters()).device\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        X=f['images']; N,H,W=X.shape\n",
    "    Hb,Wb=math.ceil(H/tile), math.ceil(W/tile); canvas=np.zeros((Hb*tile,Wb*tile), np.float32)\n",
    "    bt=start_batch_tiles\n",
    "    for pid in range(N):\n",
    "        with h5py.File(h5_path,'r') as f:\n",
    "            img=f['images'][pid].astype(np.float32)\n",
    "        med=np.median(img); mad=np.median(np.abs(img-med))+1e-12; sigma=1.4826*mad\n",
    "        img=np.clip((img-med)/max(sigma,1e-6), -5, 5)\n",
    "        coords=[(r*tile,c*tile) for r in range(Hb) for c in range(Wb)]; canvas.fill(0.0)\n",
    "        k=0\n",
    "        while k<len(coords):\n",
    "            tried=False\n",
    "            while not tried:\n",
    "                tried=True; this_bt=min(bt, len(coords)-k)\n",
    "                patches=[]\n",
    "                for j in range(this_bt):\n",
    "                    r0,c0=coords[k+j]; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    patch=np.zeros((tile,tile),np.float32); patch[:r1-r0,:c1-c0]=img[r0:r1,c0:c1]\n",
    "                    patches.append(patch[None,None,...])\n",
    "                xb=torch.from_numpy(np.concatenate(patches,0)).to(dev, non_blocking=True)\n",
    "                try:\n",
    "                    with cuda_oom_guard(), torch.amp.autocast('cuda', enabled=(dev.type=='cuda')):\n",
    "                        probs=torch.sigmoid(model(xb)).float().cpu().numpy()[:,0]\n",
    "                except MemoryError:\n",
    "                    bt=max(1, bt//2); tried=False; del xb; torch.cuda.empty_cache(); continue\n",
    "                for p,(r0,c0) in zip(probs, coords[k:k+this_bt]): canvas[r0:r0+tile, c0:c0+tile] = p\n",
    "                k += this_bt; del xb, probs; torch.cuda.empty_cache()\n",
    "                if bt<64: bt=min(64, bt*2)\n",
    "        yield pid, canvas[:H,:W].copy()\n",
    "\n",
    "def postprocess(bin_img, open_iters=1): return binary_opening(bin_img, iterations=open_iters).astype(np.uint8) if open_iters>0 else bin_img\n",
    "\n",
    "def component_PR_one(pred_bin, gt, iou_thr=0.1, min_pix=120, min_elong=2.5):\n",
    "    TP=FP=0\n",
    "    L,_=cc_label(pred_bin, structure=np.ones((3,3),np.uint8))\n",
    "    for k,sl in enumerate(find_objects(L) or [], start=1):\n",
    "        comp=(L[sl]==k); area=int(comp.sum())\n",
    "        if area < min_pix: continue\n",
    "        h=sl[0].stop-sl[0].start; w=sl[1].stop-sl[1].start\n",
    "        elong=max(h,w)/max(1,min(h,w))\n",
    "        if elong < min_elong: continue\n",
    "        gtl=gt[sl].astype(bool); inter=(comp & gtl).sum(); uni=(comp | gtl).sum()\n",
    "        if uni and inter/uni >= iou_thr: TP+=1\n",
    "        else: FP+=1\n",
    "    _,GT=cc_label(gt, structure=np.ones((3,3),np.uint8))\n",
    "    return TP,FP,int(GT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cc0ffe9-9a56-4d2b-8dbd-5b362b078ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] batch 2800/5760 | 358400/737280 ex | loss=3.0523 | F1 0.0478 | P 0.0245 | R 0.9931 | 719.9s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m bce_pw = estimate_pos_weight(DATA[\u001b[33m\"\u001b[39m\u001b[33mtrain_h5\u001b[39m\u001b[33m\"\u001b[39m], max_panels=\u001b[32m64\u001b[39m, tile=TILE).to(device)\n\u001b[32m      7\u001b[39m criterion = StreakSegLoss(\n\u001b[32m      8\u001b[39m     w_aftl=\u001b[32m0.5\u001b[39m, w_lovasz=\u001b[32m0.3\u001b[39m, w_ohem=\u001b[32m0.2\u001b[39m,\n\u001b[32m      9\u001b[39m     aftl_alpha=\u001b[32m0.2\u001b[39m, aftl_beta=\u001b[32m0.8\u001b[39m, aftl_gamma=\u001b[32m0.75\u001b[39m,\n\u001b[32m     10\u001b[39m     ohem_neg_percent=\u001b[32m0.10\u001b[39m,\n\u001b[32m     11\u001b[39m     pos_weight=bce_pw\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m _ = fit(model, train_loader, val_loader, criterion, epochs=EPOCHS, max_lr=MAX_LR, save_path=SAVE_BEST)\n\u001b[32m     14\u001b[39m torch.save({\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m: model.state_dict()}, \u001b[33m\"\u001b[39m\u001b[33m./last_unet_resse.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, epochs, max_lr, save_path, early_stop_patience)\u001b[39m\n\u001b[32m     55\u001b[39m rebalance_loss_blend(criterion, ep-\u001b[32m1\u001b[39m, epochs)\n\u001b[32m     56\u001b[39m t0=time.time()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m trL,_,tP,tR,tF1 = run_epoch(train_loader, model, criterion, opt, train=\u001b[38;5;28;01mTrue\u001b[39;00m, tag=\u001b[33m\"\u001b[39m\u001b[33mTrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m _free_cuda()\n\u001b[32m     59\u001b[39m vaL,vaA,vaP,vaR,vaF1 = run_epoch(val_loader,   model, criterion, \u001b[38;5;28;01mNone\u001b[39;00m, train=\u001b[38;5;28;01mFalse\u001b[39;00m, tag=\u001b[33m\"\u001b[39m\u001b[33mVal\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(loader, model, criterion, optimizer, train, tag, print_every, n_bins, grad_clip)\u001b[39m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     28\u001b[39m             scaler.unscale_(optimizer); nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         scaler.step(optimizer); scaler.update()\n\u001b[32m     30\u001b[39m bs=xb.size(\u001b[32m0\u001b[39m); seen+=bs; loss_sum += \u001b[38;5;28mfloat\u001b[39m(loss.item())*bs\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/amp/grad_scaler.py:461\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    458\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    459\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m retval = \u001b[38;5;28mself\u001b[39m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n\u001b[32m    463\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/amp/grad_scaler.py:355\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/amp/grad_scaler.py:355\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    349\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m     **kwargs: Any,\n\u001b[32m    353\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    354\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    356\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = UNetResSEASPP(in_ch=1, out_ch=1)\n",
    "#criterion = AdaptiveComboLoss(w_ft=0.6, w_bce=0.2, w_dice=0.2, alpha=0.25, beta=0.75, gamma=0.75, use_dice=True).to(device)\n",
    "#criterion.bce = nn.BCEWithLogitsLoss(pos_weight=estimate_pos_weight(DATA[\"train_h5\"], max_panels=64, tile=TILE))\n",
    "# build pos_weight on the correct device (optional but often helpful)\n",
    "bce_pw = estimate_pos_weight(DATA[\"train_h5\"], max_panels=64, tile=TILE).to(device)\n",
    "\n",
    "criterion = StreakSegLoss(\n",
    "    w_aftl=0.5, w_lovasz=0.3, w_ohem=0.2,\n",
    "    aftl_alpha=0.2, aftl_beta=0.8, aftl_gamma=0.75,\n",
    "    ohem_neg_percent=0.10,\n",
    "    pos_weight=bce_pw\n",
    ")\n",
    "_ = fit(model, train_loader, val_loader, criterion, epochs=EPOCHS, max_lr=MAX_LR, save_path=SAVE_BEST)\n",
    "torch.save({\"state_dict\": model.state_dict()}, \"./last_unet_resse.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555ea5b0-ca91-46ee-b6dc-9978eec733d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m F1>best[\u001b[32m0\u001b[39m]: best=(F1,t)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pid, probs \u001b[38;5;129;01min\u001b[39;00m stream_panels_direct(DATA[\u001b[33m\"\u001b[39m\u001b[33mtrain_h5\u001b[39m\u001b[33m\"\u001b[39m], model, tile=TILE, start_batch_tiles=\u001b[32m64\u001b[39m):\n\u001b[32m     17\u001b[39m     processed += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m val_set:\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "ths = np.linspace(0.15, 0.95, 33)\n",
    "val_counts = {t:{'TP':0,'FP':0,'GT':0} for t in ths}\n",
    "val_set = set(map(int, idx_va))\n",
    "\n",
    "start = time.time(); processed=used=0\n",
    "with h5py.File(DATA[\"train_h5\"], 'r') as f: N_total = len(f['masks'])\n",
    "\n",
    "def _best(counts):\n",
    "    best=(-1.0, None)\n",
    "    for t in ths:\n",
    "        TP,FP,GT = counts[t]['TP'], counts[t]['FP'], counts[t]['GT']\n",
    "        FN = max(GT-TP,0); P = TP/max(TP+FP,1); R = TP/max(TP+FN,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "        if F1>best[0]: best=(F1,t)\n",
    "    return best\n",
    "\n",
    "for pid, probs in stream_panels_direct(DATA[\"train_h5\"], model, tile=TILE, start_batch_tiles=64):\n",
    "    processed += 1\n",
    "    if pid in val_set:\n",
    "        used += 1\n",
    "        with h5py.File(DATA[\"train_h5\"],'r') as f:\n",
    "            gt = f['masks'][pid][:].astype(np.uint8)\n",
    "        for t in ths:\n",
    "            pred_bin = (probs>=t).astype(np.uint8)\n",
    "            TP,FP,GT = component_PR_one(pred_bin, gt, iou_thr=0.1, min_pix=30, min_elong=1.8)\n",
    "            d=val_counts[t]; d['TP']+=TP; d['FP']+=FP; d['GT']+=GT\n",
    "    if processed%5==0 or processed==N_total:\n",
    "        elapsed=time.time()-start; rate=processed/max(elapsed,1e-6); eta=(N_total-processed)/max(rate,1e-6)\n",
    "        curF1,curT = _best(val_counts)\n",
    "        print(f\"\\rPanels {processed}/{N_total} | used {used} | best F1≈{curF1:.3f} @ thr≈{curT if curT else float('nan'):.3f} | {elapsed/60:.1f}m | ETA {eta/60:.1f}m\", end='', flush=True)\n",
    "print()\n",
    "\n",
    "best_f1=-1; best_thr=None; best_stats=None\n",
    "for t in ths:\n",
    "    TP,FP,GT = val_counts[t]['TP'], val_counts[t]['FP'], val_counts[t]['GT']\n",
    "    FN=GT-TP; P=TP/max(TP+FP,1); R=TP/max(TP+FN,1); F1=2*P*R/max(P+R,1e-8)\n",
    "    if F1>best_f1: best_f1, best_thr, best_stats = F1, float(t), (P,R,TP,FP,GT)\n",
    "print(f\"[VAL] best thr={best_thr:.3f} F1={best_f1:.3f} P={best_stats[0]:.3f} R={best_stats[1]:.3f} | TP={best_stats[2]} FP={best_stats[3]} GT={best_stats[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b194b0f6-362b-4855-abd2-98e03516f38b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(DATA[\u001b[33m\"\u001b[39m\u001b[33mtest_h5\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     N_test = f[\u001b[33m'\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pid, probs \u001b[38;5;129;01min\u001b[39;00m stream_panels_direct(DATA[\u001b[33m\"\u001b[39m\u001b[33mtest_h5\u001b[39m\u001b[33m\"\u001b[39m], model, tile=TILE):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pid >= N_test: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(DATA[\u001b[33m\"\u001b[39m\u001b[33mtest_h5\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# PR on TEST at best_thr\n",
    "TP=FP=GT=0\n",
    "with h5py.File(DATA[\"test_h5\"], 'r') as f:\n",
    "    N_test = f['masks'].shape[0]\n",
    "for pid, probs in stream_panels_direct(DATA[\"test_h5\"], model, tile=TILE):\n",
    "    if pid >= N_test: break\n",
    "    with h5py.File(DATA[\"test_h5\"], 'r') as f:\n",
    "        gt = f['masks'][pid][:].astype(np.uint8)\n",
    "    pred_bin = postprocess((probs>=best_thr).astype(np.uint8), open_iters=1)\n",
    "    tpi,fpi,gti = component_PR_one(pred_bin, gt, iou_thr=0.1, min_pix=120, min_elong=2.5)\n",
    "    TP += tpi; FP += fpi; GT += gti\n",
    "FN = GT-TP; P = TP/max(TP+FP,1); R = TP/max(TP+FN,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "print(f\"[TEST] thr={best_thr:.3f} P={P:.3f} R={R:.3f} F1={F1:.3f} | TP={TP} FP={FP} GT={GT}\")\n",
    "\n",
    "# Histograms (stream + mark detections against CSV)\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "def stream_mark_nn_and_stack(csv_path, h5_path, model, thr, radius=3):\n",
    "    cat = pd.read_csv(csv_path).copy()\n",
    "    if \"stack_detection\" in cat.columns: cat[\"stack_detected\"] = cat[\"stack_detection\"].astype(bool)\n",
    "    elif \"stack_mag\" in cat.columns:     cat[\"stack_detected\"] = ~cat[\"stack_mag\"].isna()\n",
    "    else:                                 cat[\"stack_detected\"] = False\n",
    "    nn = np.zeros(len(cat), dtype=bool)\n",
    "    groups = {int(pid): grp.index.to_numpy() for pid, grp in cat.groupby(\"image_id\")}\n",
    "    with h5py.File(h5_path,'r') as f: _,H,W=f['images'].shape\n",
    "    for pid, probs in stream_panels_direct(h5_path, model, tile=TILE):\n",
    "        if pid not in groups: continue\n",
    "        mask = (probs>=thr).astype(np.uint8)\n",
    "        idxs = groups[pid]\n",
    "        xs=np.clip(cat.loc[idxs,\"x\"].to_numpy(int),0,W-1); ys=np.clip(cat.loc[idxs,\"y\"].to_numpy(int),0,H-1)\n",
    "        for j,(x,y) in zip(idxs, zip(xs,ys)):\n",
    "            y0,y1=max(0,y-radius),min(H,y+radius+1); x0,x1=max(0,x-radius),min(W,x+radius+1)\n",
    "            nn[j] = (mask[y0:y1, x0:x1].max()>0)\n",
    "    cat[\"nn_detected\"] = nn\n",
    "    return cat\n",
    "\n",
    "def plot_detect_hist(cat, field, bins=12, title=None):\n",
    "    vals=cat[field].to_numpy(); vals=vals[np.isfinite(vals)]\n",
    "    vmin,vmax=np.nanmin(vals),np.nanmax(vals)\n",
    "    if vmax/max(vmin,1e-6) > 50: edges=np.geomspace(max(vmin,1e-3), vmax, bins+1)\n",
    "    else: edges=np.linspace(vmin, vmax, bins+1)\n",
    "    nn_det=cat[cat[\"nn_detected\"]]; stk_det=cat[cat[\"stack_detected\"]]; cum=cat[nn_det.index.union(stk_det.index)]\n",
    "    fig,ax=plt.subplots(figsize=(6.6,4.4))\n",
    "    ax.hist(cat[field], bins=edges, histtype=\"step\", label=\"All injected\", alpha=0.7)\n",
    "    ax.hist(cum[field], bins=edges, histtype=\"step\", label=\"Cumulative (NN ∪ LSST)\")\n",
    "    ax.hist(nn_det[field], bins=edges, histtype=\"step\", label=\"NN detected\")\n",
    "    ax.hist(stk_det[field], bins=edges, histtype=\"step\", label=\"LSST stack detected\")\n",
    "    ax.set_xlabel(field.replace(\"_\",\" \")); ax.set_ylabel(\"Count per bin\")\n",
    "    if title: ax.set_title(title); ax.legend(); ax.grid(True, alpha=0.3); plt.show()\n",
    "\n",
    "# Build marked catalogs and plot both histograms\n",
    "cat_test = stream_mark_nn_and_stack(DATA[\"test_csv\"], DATA[\"test_h5\"], model, thr=best_thr, radius=3)\n",
    "mag_field = \"integrated_mag\" if \"integrated_mag\" in cat_test.columns else (\"PSF_mag\" if \"PSF_mag\" in cat_test.columns else \"mag\")\n",
    "plot_detect_hist(cat_test, field=mag_field,     bins=12, title=f\"Detections vs magnitude (thr={best_thr:.2f})\")\n",
    "plot_detect_hist(cat_test, field=\"trail_length\", bins=12, title=f\"Detections vs trail length (thr={best_thr:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2902413-4095-4633-ba74-19c880861633",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd13b98-3419-4f32-a6d3-a73ac036cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "# IMPORTANT: turn OFF AMP and any heavy augmentations for the probe\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Lovasz ---\n",
    "def _lovasz_grad(gt_sorted):\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    inter = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jacc = 1.0 - inter / torch.clamp(union, min=1e-12)\n",
    "    if p > 1:\n",
    "        jacc[1:p] = jacc[1:p] - jacc[0:p-1]\n",
    "    return jacc\n",
    "\n",
    "def _lovasz_hinge_flat(logits, labels):\n",
    "    if logits.numel() == 0:\n",
    "        return logits*0.0\n",
    "    signs  = 2.0*labels.float() - 1.0\n",
    "    errors = 1.0 - logits * signs\n",
    "    errs_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = _lovasz_grad(gt_sorted)\n",
    "    return torch.clamp(errs_sorted, min=0).dot(grad)\n",
    "\n",
    "def lovasz_hinge(logits, targets):\n",
    "    B = logits.size(0)\n",
    "    losses = []\n",
    "    for b in range(B):\n",
    "        l = logits[b].reshape(-1)\n",
    "        t = targets[b].reshape(-1).float()\n",
    "        losses.append(_lovasz_hinge_flat(l, t))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "# --- Asymmetric Focal Tversky (FP-heavy) ---\n",
    "class AsymFocalTversky(nn.Module):\n",
    "    def __init__(self, alpha=0.85, beta=0.15, gamma=1.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.gamma, self.eps = alpha, beta, gamma, eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits).clamp(self.eps, 1-self.eps)\n",
    "        t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        t = t.view(t.size(0), -1)\n",
    "        TP = (p*t).sum(1)\n",
    "        FP = ((1-t)*p).sum(1)\n",
    "        FN = (t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps) / (TP + self.alpha*FP + self.beta*FN + self.eps)\n",
    "        return torch.pow(1.0 - tv, self.gamma).mean()\n",
    "\n",
    "# --- OHEM BCE (sample more negatives) ---\n",
    "class OHEMBCE(nn.Module):\n",
    "    def __init__(self, neg_percent=0.4, pos_weight=None, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.neg_percent = float(neg_percent)\n",
    "        self.pos_weight  = pos_weight  # keep None to avoid positive bias\n",
    "        self.reduction   = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets,\n",
    "                                                 pos_weight=self.pos_weight,\n",
    "                                                 reduction='none')\n",
    "        pos_mask = (targets > 0.5)\n",
    "        neg_mask = ~pos_mask\n",
    "        pos_loss = bce[pos_mask]\n",
    "        neg_loss = bce[neg_mask]\n",
    "        if neg_loss.numel() > 0 and self.neg_percent > 0:\n",
    "            k = max(1, int(self.neg_percent * neg_loss.numel()))\n",
    "            hard_neg, _ = torch.topk(neg_loss.reshape(-1), k, sorted=False)\n",
    "            loss = torch.cat([pos_loss.reshape(-1), hard_neg], 0)\n",
    "        else:\n",
    "            loss = bce.reshape(-1)\n",
    "        return loss.mean() if self.reduction=='mean' else loss.sum()\n",
    "\n",
    "# --- Composite with background suppression ---\n",
    "class StreakSegLossFP(nn.Module):\n",
    "    \"\"\"\n",
    "    Strong FP control:\n",
    "      - Asym Focal Tversky with high alpha (penalize FP)\n",
    "      - Lovasz hinge for region quality\n",
    "      - OHEM-BCE with many negatives\n",
    "      - Small background mean-prob penalty\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 w_aftl=0.45, w_lovasz=0.35, w_ohem=0.20,\n",
    "                 aftl_alpha=0.85, aftl_beta=0.15, aftl_gamma=1.0,\n",
    "                 ohem_neg_percent=0.40,\n",
    "                 bg_lambda=0.02, bg_gamma=2.0,      \n",
    "                 pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.w_aftl, self.w_lovasz, self.w_ohem = w_aftl, w_lovasz, w_ohem\n",
    "        self.bg_lambda, self.bg_gamma = bg_lambda, bg_gamma\n",
    "        self.aftl = AsymFocalTversky(aftl_alpha, aftl_beta, aftl_gamma)\n",
    "        self.ohem = OHEMBCE(ohem_neg_percent, pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        la = self.aftl(logits, t)\n",
    "        ll = lovasz_hinge(logits, (t>0.5).float())\n",
    "        lo = self.ohem(logits, t)\n",
    "\n",
    "        # --- background focal suppression ---\n",
    "        p = torch.sigmoid(logits)\n",
    "        bg_mask = (t < 0.5).float()\n",
    "        denom = torch.clamp(bg_mask.sum(dim=(1,2,3)), min=1.0)\n",
    "        bg_p = (p * bg_mask).sum(dim=(1,2,3)) / denom\n",
    "        bg_focal = torch.pow(bg_p, self.bg_gamma).mean()   # sharper on mid probs\n",
    "\n",
    "        return self.w_aftl*la + self.w_lovasz*ll + self.w_ohem*lo + self.bg_lambda*bg_focal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb560811-9a74-448d-916d-3d612dba9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-panel median/MAD normalize, clip like stream_panels_direct\n",
    "def norm_medmad_clip(x, clip=5.0, eps=1e-6):\n",
    "    # x: torch.Tensor [B,1,H,W] or [1,H,W]\n",
    "    if x.ndim == 4:\n",
    "        med = x.median(dim=-1, keepdim=True).values.median(dim=-2, keepdim=True).values\n",
    "    else:  # [1,H,W]\n",
    "        med = x.median()\n",
    "        med = med.view(1,1,1)\n",
    "    mad = (x - med).abs().median()\n",
    "    sigma = 1.4826 * mad + eps\n",
    "    z = (x - med) / sigma\n",
    "    return z.clamp_(-clip, clip)\n",
    "\n",
    "class WithTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.base[i]\n",
    "        x = norm_medmad_clip(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2a61e7-f235-464b-9690-bb30197b9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos px ratio ≈ 0.003537 | non-empty masks in 8 batches: 8/8\n"
     ]
    }
   ],
   "source": [
    "# Collect panel-level “has positive” flags quickly from the H5\n",
    "import h5py, numpy as np, math\n",
    "def panels_with_positives(h5_path, tile=128, max_panels=None):\n",
    "    ids=[]\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        Y=f['masks']; N,H,W=Y.shape\n",
    "        rng = np.random.default_rng(0)\n",
    "        order = rng.permutation(N) if max_panels else np.arange(N)\n",
    "        for i in order:\n",
    "            yi = Y[i]\n",
    "            if yi.any(): ids.append(i)\n",
    "            if max_panels and len(ids)>=max_panels: break\n",
    "    return np.array(sorted(ids))\n",
    "\n",
    "def quick_mask_stats(loader, n_batches=5):\n",
    "    import numpy as np, torch\n",
    "    pos_px = neg_px = 0\n",
    "    nonempty_masks = 0\n",
    "    for b,(xb,yb) in enumerate(loader,1):\n",
    "        y = yb.numpy()\n",
    "        pos_px += (y>0.5).sum()\n",
    "        neg_px += (y<=0.5).sum()\n",
    "        nonempty_masks += int((y>0.5).any())\n",
    "        if b>=n_batches: break\n",
    "    total = pos_px+neg_px\n",
    "    print(f\"Pos px ratio ≈ {pos_px/max(total,1):.6f} | non-empty masks in {n_batches} batches: {nonempty_masks}/{n_batches}\")\n",
    "\n",
    "pos_panels = panels_with_positives(\"/home/karlo/train_chunked.h5\", max_panels=2000)\n",
    "# Sample your small train/val from the intersection with your original idx_tr/idx_va\n",
    "sub_tr = np.random.default_rng(42).choice(np.intersect1d(idx_tr, pos_panels), size=min(200, len(pos_panels)), replace=False)\n",
    "sub_va = np.random.default_rng(43).choice(np.intersect1d(idx_va, pos_panels), size=min(80,  len(pos_panels)), replace=False)\n",
    "\n",
    "train_ds_small = SubsetDS(ds_full, np.sort(sub_tr))\n",
    "val_ds_small   = SubsetDS(ds_full, np.sort(sub_va))\n",
    "train_loader_small = DataLoader(train_ds_small, batch_size=64, shuffle=True,  num_workers=10, pin_memory=(device.type=='cuda'))\n",
    "val_loader_small   = DataLoader(val_ds_small,   batch_size=64, shuffle=False, num_workers=10, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "train_ds_small = WithTransform(train_ds_small)\n",
    "val_ds_small   = WithTransform(val_ds_small)\n",
    "\n",
    "quick_mask_stats(train_loader_small, n_batches=8)  # re-check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050f286b-ab0b-4160-aed6-745294bead54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_under_min(model, loader, max_batches=40, n_bins=256, beta=1.0):\n",
    "    \"\"\"\n",
    "    Ultra-fast threshold finder from pixel histograms.\n",
    "    - No CC / IoU; just pixel-level P/R/F1.\n",
    "    - Processes `max_batches` from `loader` (use val loader).\n",
    "    Returns: best_thr, (P, R, F1), dict(histograms)\n",
    "    \"\"\"\n",
    "    model.eval().to(device)\n",
    "    pos_hist = np.zeros(n_bins, np.float64)\n",
    "    neg_hist = np.zeros(n_bins, np.float64)\n",
    "\n",
    "    t0 = time.time()\n",
    "    processed = 0\n",
    "    amp = torch.amp.autocast('cuda', enabled=(device.type=='cuda'))\n",
    "\n",
    "    for b, (xb, yb) in enumerate(loader, start=1):\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        with amp:\n",
    "            logits = model(xb)\n",
    "            yb_r   = resize_masks_to(logits, yb)           # your helper\n",
    "            p      = torch.sigmoid(logits).float()         # (B,1,t,t)\n",
    "\n",
    "        # flatten\n",
    "        p = p.view(-1).detach()\n",
    "        t = yb_r.view(-1).detach()\n",
    "\n",
    "        # build histograms on GPU then move to CPU (fast)\n",
    "        # subsample negatives so they don't dominate time\n",
    "        neg_mask = (t <= 0.5)\n",
    "        pos_mask = (t > 0.5)\n",
    "\n",
    "        # cap negatives to ~2e6 pixels per batch (adjust if needed)\n",
    "        max_neg = 2_000_000\n",
    "        n_neg   = int(neg_mask.sum().item())\n",
    "        if n_neg > 0:\n",
    "            if n_neg > max_neg:\n",
    "                # random subsample negatives\n",
    "                idx = torch.randperm(n_neg, device=device)[:max_neg]\n",
    "                p_neg = p[neg_mask][idx]\n",
    "            else:\n",
    "                p_neg = p[neg_mask]\n",
    "            neg_hist += torch.histc(p_neg, bins=n_bins, min=0.0, max=1.0).cpu().numpy()\n",
    "\n",
    "        if int(pos_mask.sum().item()) > 0:\n",
    "            p_pos = p[pos_mask]\n",
    "            pos_hist += torch.histc(p_pos, bins=n_bins, min=0.0, max=1.0).cpu().numpy()\n",
    "\n",
    "        processed += 1\n",
    "        if (processed % 5 == 0) or (processed == max_batches):\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"\\r[FAST-T] batches {processed}/{max_batches} | elapsed {elapsed:.1f}s\", end='', flush=True)\n",
    "\n",
    "        if processed >= max_batches:\n",
    "            break\n",
    "\n",
    "    print()  # newline\n",
    "\n",
    "    # choose threshold by Fβ\n",
    "    # cumulative from high→low\n",
    "    pos_cum = np.cumsum(pos_hist[::-1])\n",
    "    neg_cum = np.cumsum(neg_hist[::-1])\n",
    "    TP = pos_cum\n",
    "    FP = neg_cum\n",
    "    FN = (pos_cum[-1] - TP).clip(min=0)\n",
    "\n",
    "    P  = TP / np.maximum(TP + FP, 1)\n",
    "    R  = TP / np.maximum(pos_cum[-1], 1)\n",
    "    beta2 = beta * beta\n",
    "    Fbeta = (1+beta2) * P * R / np.maximum(beta2*P + R, 1e-8)\n",
    "\n",
    "    best_bin = int(np.nanargmax(Fbeta))\n",
    "    best_thr = (best_bin + 0.5) / n_bins\n",
    "    best_P, best_R, best_F = float(P[best_bin]), float(R[best_bin]), float(Fbeta[best_bin])\n",
    "\n",
    "    print(f\"[FAST-T] best thr≈{best_thr:.3f} | P≈{best_P:.3f} R≈{best_R:.3f} F{beta:.1f}≈{best_F:.3f} \"\n",
    "          f\"| time {time.time()-t0:.1f}s\")\n",
    "\n",
    "    return best_thr, (best_P, best_R, best_F), {\"pos_hist\":pos_hist, \"neg_hist\":neg_hist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e088e0-00ef-4417-a3e7-073d2f687d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "\n",
    "def init_head_bias_to_prior(model, p0=0.20):\n",
    "    # p0 in (0,1); logit(p0) biases initial sigmoid outputs toward p0\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        model.head.bias.data.fill_(b)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sanitize_batch(x, y):\n",
    "    \"\"\"Replace non-finite with 0 and clamp labels to [0,1].\"\"\"\n",
    "    x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0).clamp_(0.0, 1.0)\n",
    "    return x, y\n",
    "\n",
    "class WarmupBCE(torch.nn.Module):\n",
    "    \"\"\"Simple, stable warm-up loss (no Dice).\"\"\"\n",
    "    def __init__(self, pos_weight=10.0):\n",
    "        super().__init__()\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor(float(pos_weight))\n",
    "        )\n",
    "    def forward(self, logits, targets):\n",
    "        return self.bce(logits, targets)\n",
    "\n",
    "def init_head_bias_to_prior(model, p0=0.20):\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "            model.head.bias.data.fill_(b)\n",
    "\n",
    "def fit_quick_warmup(model, loader, epochs=2, max_batches=250,\n",
    "                     lr=1e-4, metric_thr=0.20, pos_weight=10.0):\n",
    "    device = next(model.parameters()).device\n",
    "    amp_ctx = nullcontext()\n",
    "    crit = WarmupBCE(pos_weight=pos_weight).to(device)\n",
    "    opt  = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); seen=tp=fp=fn=0; loss_sum=0.0; t0=time.time()\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            with amp_ctx:\n",
    "                # forward\n",
    "                logits = model(xb)\n",
    "                yb_r   = resize_masks_to(logits, yb)\n",
    "\n",
    "                # sanitize AFTER resize (resize can create NaNs if input has them)\n",
    "                xb, yb_r = sanitize_batch(xb, yb_r)\n",
    "                logits   = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                # compute loss\n",
    "                loss = crit(logits, yb_r)\n",
    "\n",
    "            # detect NaN early, print once and break\n",
    "            if not torch.isfinite(loss):\n",
    "                with torch.no_grad():\n",
    "                    bad_x = ~torch.isfinite(xb).any().item()\n",
    "                    bad_y = ~torch.isfinite(yb_r).any().item()\n",
    "                    print(f\"\\n[NaN] batch {b}: loss={loss.item()} | \"\n",
    "                          f\"x finite? {torch.isfinite(xb).all().item()} | \"\n",
    "                          f\"y finite? {torch.isfinite(yb_r).all().item()} | \"\n",
    "                          f\"logits finite? {torch.isfinite(logits).all().item()}\")\n",
    "                break\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            # quick metrics\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits)\n",
    "                pv = p.view(-1); tv = yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "\n",
    "                # Only print “pos/neg” means if there ARE positives to avoid NaN in the print\n",
    "                if (yb_r>0.5).any():\n",
    "                    pos_prob = float(p[yb_r>0.5].mean())\n",
    "                else:\n",
    "                    pos_prob = float('nan')\n",
    "                neg_prob = float(p[yb_r<=0.5].mean())\n",
    "                frac_over = float((p>=0.5).float().mean())\n",
    "\n",
    "            if b % 5 == 0 or b == max_batches:\n",
    "                print(f\"\\r[WARMUP] ep{ep} batch {b}/{max_batches} \"\n",
    "                      f\"| loss={loss_sum/seen:.4f} | over>=0.5 {frac_over:.4f} \"\n",
    "                      f\"| pos {pos_prob:.4f} | neg {neg_prob:.4f} \"\n",
    "                      f\"| {time.time()-t0:.1f}s\", end='', flush=True)\n",
    "            if b>=max_batches: break\n",
    "\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"\\n[WARMUP] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f} | {(time.time()-t0):.1f}s\")\n",
    "\n",
    "def fit_quick(model, criterion, train_loader, epochs=2, max_batches=250, lr=3e-4, metric_thr=0.20, weight_decay=0.0):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    model.to(device)\n",
    "    amp_ctx = nullcontext()\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); seen=tp=fp=fn=0; loss_sum=0.0; t0=time.time()\n",
    "        for b,(xb,yb) in enumerate(train_loader,1):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            #with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n",
    "            with amp_ctx:\n",
    "                logits = model(xb)\n",
    "                yb_r   = resize_masks_to(logits, yb)\n",
    "                # quick sanity: ensure there ARE positives after resize\n",
    "                if (yb_r>0.5).sum() == 0:\n",
    "                    pass  # comment out, but keep for debugging if needed\n",
    "                loss   = criterion(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits)\n",
    "                frac_over = float((p >= metric_thr).float().mean())\n",
    "\n",
    "                pos_prob  = float(p[yb_r>0.5].mean()) if (yb_r>0.5).any() else float('nan')\n",
    "                neg_prob  = float(p[yb_r<=0.5].mean())\n",
    "\n",
    "                posv = p[yb_r > 0.5]; negv = p[yb_r <= 0.5]\n",
    "                sep = (float(posv.mean()) if posv.numel() else float('nan')) - float(negv.mean())\n",
    "\n",
    "                pv = p.view(-1); tv = yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "\n",
    "            if b>=max_batches: break\n",
    "            if b % 5 == 0:\n",
    "                print(f\"\\r[QP] batch {b}/{max_batches} | loss={loss_sum/seen:.4f} | over>={metric_thr:.4f} {frac_over:.4f} | \"\n",
    "                      f\"pos {pos_prob:.3f} neg {neg_prob:.3f} | sep {sep:.4f} | {time.time()-t0:.1f}s\", end='', flush=True)\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"\\n[QP] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f} | {(time.time()-t0):.1f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d228f45-93e9-48c6-9fb1-57f01d14920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop-in replacement: recall-friendly threshold with a positive-rate floor\n",
    "def pick_thr_with_floor(model, loader, max_batches=20, n_bins=256, beta=2.0,\n",
    "                        min_pos_rate=0.01, max_pos_rate=0.10):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "\n",
    "    # histograms over probs for pos/neg pixels + overall count for pos-rate\n",
    "    pos_hist = np.zeros(n_bins, np.int64)\n",
    "    neg_hist = np.zeros(n_bins, np.int64)\n",
    "    all_hist = np.zeros(n_bins, np.int64)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for b, (xb, yb) in enumerate(loader, 1):\n",
    "            xb = xb.to(dev, non_blocking=True)\n",
    "            yb = yb.to(dev, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            yb_r   = resize_masks_to(logits, yb)\n",
    "            p      = torch.sigmoid(logits).clamp(1e-6, 1-1e-6).float().cpu()\n",
    "\n",
    "            t  = (yb_r>0.5).cpu()\n",
    "            idx = torch.clamp((p*(n_bins-1)).long(), 0, n_bins-1)\n",
    "\n",
    "            # update hist\n",
    "            for k in range(n_bins):\n",
    "                pass\n",
    "            flat_idx = idx.view(-1)\n",
    "            flat_t   = t.view(-1)\n",
    "            pos_hist += np.bincount(flat_idx[flat_t].numpy(), minlength=n_bins)\n",
    "            neg_hist += np.bincount(flat_idx[~flat_t].numpy(), minlength=n_bins)\n",
    "            all_hist += np.bincount(flat_idx.numpy(), minlength=n_bins)\n",
    "\n",
    "            if b >= max_batches: break\n",
    "\n",
    "    # sweep thresholds\n",
    "    pos_cum = pos_hist[::-1].cumsum()     # TP as threshold lowers\n",
    "    neg_cum = neg_hist[::-1].cumsum()     # FP as threshold lowers\n",
    "    all_cum = all_hist[::-1].cumsum()     # predicted-positive count\n",
    "\n",
    "    TP = pos_cum\n",
    "    FP = neg_cum\n",
    "    FN = pos_hist.sum() - TP\n",
    "    P = TP / np.maximum(TP+FP, 1)\n",
    "    R = TP / np.maximum(TP+FN, 1)\n",
    "\n",
    "    # F-beta (beta=2 -> recall-friendly)\n",
    "    beta2 = beta*beta\n",
    "    F = (1+beta2) * P * R / np.maximum(beta2*P + R, 1e-12)\n",
    "\n",
    "    # enforce predicted-positive rate floor/ceiling\n",
    "    total_px = all_hist.sum()\n",
    "    pos_rate = all_cum / max(total_px, 1)\n",
    "    mask = (pos_rate >= min_pos_rate) & (pos_rate <= max_pos_rate)\n",
    "\n",
    "    if not mask.any():\n",
    "        # fallback: choose the closest rate above min_pos_rate\n",
    "        k = np.argmin(np.abs(pos_rate - min_pos_rate))\n",
    "    else:\n",
    "        k = np.argmax(F * mask)\n",
    "\n",
    "    thr = (n_bins-1 - k) / (n_bins-1)\n",
    "    return float(thr), (float(P[k]), float(R[k]), float(F[k])), dict(pos_rate=float(pos_rate[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40dec931-3e14-43ee-b468-ee91190c5af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up (BCE-only)…\n",
      "[WARMUP] ep1 batch 800/800 | loss=0.3742 | over>=0.5 0.0100 | pos nan | neg 0.0854 | 120.9s.1s\n",
      "[WARMUP] ep1 loss 0.3742 | F1 0.021 P 0.011 R 0.486 | 120.9s\n",
      "[WARMUP] ep2 batch 800/800 | loss=0.3516 | over>=0.5 0.0352 | pos 0.4961 | neg 0.1326 | 120.9s\n",
      "[WARMUP] ep2 loss 0.3516 | F1 0.028 P 0.014 R 0.517 | 121.0s\n",
      "[FAST-T] batches 200/200 | elapsed 7.6s\n",
      "[FAST-T] best thr≈0.447 | P≈0.030 R≈0.241 F2.0≈0.101 | time 7.6s\n",
      "thr0 = 0.15\n",
      "[HEAD] ep1 loss 0.1301 | F1 0.027 P 0.014 R 0.583\n",
      "[HEAD] ep2 loss 0.0908 | F1 0.040 P 0.021 R 0.467\n",
      "[quick_prob_stats] batches=30 | pos_mean≈0.1681 | neg_mean≈0.0326 | P 0.028 R 0.439 F1 0.053 @ thr=0.150 | 2.2s\n",
      "[QP] batch 2495/2500 | loss=0.1590 | over>=0.1500 0.0043 | pos 0.074 neg 0.047 | sep 0.0266 | 99.9ss\n",
      "[QP] ep1 loss 0.1590 | F1 0.026 P 0.014 R 0.136 | 100.2s\n",
      "[QP] batch 2495/2500 | loss=0.1524 | over>=0.1500 0.0006 | pos 0.066 neg 0.026 | sep 0.0401 | 99.9ss\n",
      "[QP] ep2 loss 0.1524 | F1 0.041 P 0.048 R 0.036 | 100.1s\n",
      "[quick_prob_stats] batches=30 | pos_mean≈0.0468 | neg_mean≈0.0252 | P 0.072 R 0.020 F1 0.032 @ thr=0.150 | 2.2s\n",
      "thr1 = 0.059 | P≈0.029 R≈0.236 F≈0.097 pos_rate≈0.032\n",
      "[quick_prob_stats] batches=30 | pos_mean≈0.0474 | neg_mean≈0.0252 | P 0.024 R 0.289 F1 0.044 @ thr=0.059 | 2.1s\n",
      "[QP] batch 1495/1500 | loss=0.7510 | over>=0.0588 0.9954 | pos 0.226 neg 0.180 | sep 0.0461 | 77.2ss\n",
      "[QP] ep1 loss 0.7511 | F1 0.007 P 0.003 R 0.952 | 77.5s\n",
      "thr2 = 0.267 | P≈0.010 R≈0.144 F≈0.038 pos_rate≈0.051\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.2184 | neg_mean≈0.1800 | P 0.015 R 0.215 F1 0.028 @ thr=0.267 | 0.8s\n",
      "[QP] batch 1995/2000 | loss=0.3208 | over>=0.2667 0.0000 | pos 0.151 neg 0.137 | sep 0.0137 | 102.6ss\n",
      "[QP] ep1 loss 0.3208 | F1 0.001 P 0.014 R 0.001 | 102.9s\n",
      "[FINAL] thr = 0.173 | P≈0.005 R≈0.149 F≈0.009 pos_rate≈0.110\n",
      "[quick_prob_stats] batches=3 | pos_mean≈0.1420 | neg_mean≈0.1330 | P 0.002 R 0.071 F1 0.004 @ thr=0.173 | 0.8s\n"
     ]
    }
   ],
   "source": [
    "# ======= One-Click Probe Tuning (no re-sweeps) =======\n",
    "import copy, time, torch, torch.nn as nn\n",
    "\n",
    "#device = next(iter(train_loader_small))[0].device if hasattr(train_loader_small, '__iter__') else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -- tiny helpers (safe to redefine) --\n",
    "def _set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters(): p.requires_grad = flag\n",
    "\n",
    "def freeze_all(model): _set_requires_grad(model, False)\n",
    "\n",
    "def unfreeze_head_only(model):\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        _set_requires_grad(model.head, True)\n",
    "    else:\n",
    "        raise AttributeError(\"Model has no attribute 'head'.\")\n",
    "\n",
    "def unfreeze_head_and_last_conv(model):\n",
    "    freeze_all(model)\n",
    "    # head\n",
    "    if hasattr(model, \"head\"):\n",
    "        _set_requires_grad(model.head, True)\n",
    "    # last conv of last up block (u4.rb2.c2) if present\n",
    "    try:\n",
    "        _set_requires_grad(model.u4.rb2.c2, True)\n",
    "    except AttributeError:\n",
    "        # fallback: just leave head if model layout differs\n",
    "        pass\n",
    "\n",
    "# define once if missing\n",
    "if \"fit_quick_head_only\" not in globals():\n",
    "    def fit_quick_head_only(model, loader, epochs=2, max_batches=200, lr=5e-5, metric_thr=0.20, pos_weight=2.0):\n",
    "        amp_ctx = nullcontext()\n",
    "        dev = next(model.parameters()).device\n",
    "        unfreeze_head_only(model)\n",
    "        head_params = [p for p in model.head.parameters() if p.requires_grad]\n",
    "        opt = torch.optim.Adam(head_params, lr=lr, weight_decay=0.0)\n",
    "        bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "        for ep in range(1, epochs+1):\n",
    "            model.train()\n",
    "            seen=tp=fp=fn=0.0; loss_sum=0.0; t0=time.time()\n",
    "            for b,(xb,yb) in enumerate(loader, 1):\n",
    "                xb,yb = xb.to(dev), yb.to(dev)\n",
    "                with amp_ctx:\n",
    "                    logits = model(xb)\n",
    "                    yb_r = resize_masks_to(logits, yb)\n",
    "                    loss = bce(logits, yb_r)\n",
    "                opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "                with torch.no_grad():\n",
    "                    p = torch.sigmoid(logits)\n",
    "                    pv, tv = p.view(-1), yb_r.view(-1)\n",
    "                    pred = (pv>=metric_thr).float()\n",
    "                    tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                    loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "                if b>=max_batches: break\n",
    "            P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "            print(f\"[HEAD] ep{ep} loss {loss_sum/seen:.4f} | F1 {F1:.3f} P {P:.3f} R {R:.3f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def quick_prob_stats(model, loader, n_batches=3, thr=0.5):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    pos_means, neg_means = [], []\n",
    "    tp = fp = fn = 0.0\n",
    "    t0=time.time()\n",
    "    for b, (xb, yb) in enumerate(loader, 1):\n",
    "        xb, yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yb_r = resize_masks_to(logits, yb)\n",
    "        p = torch.sigmoid(logits)\n",
    "        if (yb_r > 0.5).any():\n",
    "            pos_means.append(float(p[yb_r > 0.5].mean().item()))\n",
    "        neg_means.append(float(p[yb_r <= 0.5].mean().item()))\n",
    "        pv, tv = p.view(-1), yb_r.view(-1)\n",
    "        pred = (pv >= thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if b>=n_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F1 = 2*P*R/max(P+R,1e-8)\n",
    "    import numpy as np\n",
    "    print(f\"[quick_prob_stats] batches={min(n_batches, b)} | pos_mean≈{(np.mean(pos_means) if pos_means else float('nan')):.4f} | neg_mean≈{np.mean(neg_means):.4f} | P {P:.3f} R {R:.3f} F1 {F1:.3f} @ thr={thr:.3f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "def init_head_bias_to_prior(model, p0=0.70):\n",
    "    import math\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "            model.head.bias.data.fill_(b)\n",
    "\n",
    "# ===== main recipe =====\n",
    "def run_probe_recipe(model_cls, p0=0.80):\n",
    "    # 0) build probe + warmup once\n",
    "    probe = model_cls(in_ch=1, out_ch=1).to(device)\n",
    "    init_head_bias_to_prior(probe, p0=p0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(\"Warm-up (BCE-only)…\")\n",
    "    fit_quick_warmup(probe, train_loader_small, epochs=2, max_batches=800, lr=2e-4, metric_thr=0.20, pos_weight=40.0)\n",
    "\n",
    "    # lock warm state\n",
    "    warm_state = copy.deepcopy(probe.state_dict())\n",
    "\n",
    "    # 1) fast, recall-friendly threshold (pixel hist)\n",
    "    thr0, *_ = pick_thr_under_min(probe, val_loader_small, max_batches=200, n_bins=256, beta=2.0)\n",
    "    thr0 = float(__import__(\"numpy\").clip(thr0, 0.05, 0.15))\n",
    "    print(\"thr0 =\", thr0)\n",
    "\n",
    "    # 2) head-only BCE calibration\n",
    "    probe.load_state_dict(warm_state, strict=True)\n",
    "    fit_quick_head_only(probe, train_loader_small, epochs=2, max_batches=2000, lr=3e-5, metric_thr=thr0, pos_weight=5.0)\n",
    "    quick_prob_stats(probe, train_loader_small, n_batches=30, thr=thr0)\n",
    "\n",
    "    # 3) unfreeze head+tiny tail; gentle BCE+Tversky (no OHEM/BG)\n",
    "    class _BCEPlusTversky(nn.Module):\n",
    "        def __init__(self, pos_weight=2.0, alpha=0.30, beta=0.70, gamma=1.1, w_bce=0.85, w_tv=0.15):\n",
    "            super().__init__()\n",
    "            self.bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "            # reuse your AsymFocalTversky from StreakSegLossFP module if available\n",
    "            self.aftl = AsymFocalTversky(alpha=alpha, beta=beta, gamma=gamma)\n",
    "            self.w_bce, self.w_tv = w_bce, w_tv\n",
    "        def forward(self, logits, targets):\n",
    "            t = targets.clamp(0,1)\n",
    "            return self.w_bce*self.bce(logits, t) + self.w_tv*self.aftl(logits, t)\n",
    "\n",
    "    probe.load_state_dict(warm_state, strict=True)\n",
    "    unfreeze_head_and_last_conv(probe)\n",
    "    crit_gentle = _BCEPlusTversky(pos_weight=2.0, alpha=0.30, beta=0.70, gamma=1.1, w_bce=0.9, w_tv=0.1).to(device)\n",
    "    fit_quick(probe, crit_gentle, train_loader_small, epochs=2, max_batches=2500, lr=1.5e-4, metric_thr=thr0, weight_decay=1e-4)\n",
    "    quick_prob_stats(probe, train_loader_small, n_batches=30, thr=thr0)\n",
    "\n",
    "    # 4) pick threshold with a sensible positive-rate band (recall bias)\n",
    "    thr1, (P1, R1, F1), aux = pick_thr_with_floor(probe, val_loader_small, max_batches=20, n_bins=256, beta=2.0, min_pos_rate=0.03, max_pos_rate=0.10)\n",
    "    print(f\"thr1 = {thr1:.3f} | P≈{P1:.3f} R≈{R1:.3f} F≈{F1:.3f} pos_rate≈{aux['pos_rate']:.3f}\")\n",
    "    quick_prob_stats(probe, train_loader_small, n_batches=30, thr=thr1)\n",
    "\n",
    "    # 5) tiny OHEM nudge to trim glow (optional, very short)\n",
    "    crit_ohem = StreakSegLossFP(w_aftl=0.0, w_lovasz=0.0, w_ohem=1.0,\n",
    "                                ohem_neg_percent=0.02, bg_lambda=0.0,\n",
    "                                pos_weight=torch.tensor(2.0, device=device)).to(device)\n",
    "    fit_quick(probe, crit_ohem, train_loader_small, epochs=1, max_batches=1500, lr=5e-5, metric_thr=thr1, weight_decay=0.0)\n",
    "\n",
    "    # re-pick within a narrower 3–8% band\n",
    "    thr2, (P2, R2, F2), aux2 = pick_thr_with_floor(probe, val_loader_small, max_batches=200, n_bins=256, beta=2.0, min_pos_rate=0.03, max_pos_rate=0.08)\n",
    "    print(f\"thr2 = {thr2:.3f} | P≈{P2:.3f} R≈{R2:.3f} F≈{F2:.3f} pos_rate≈{aux2['pos_rate']:.3f}\")\n",
    "    quick_prob_stats(probe, train_loader_small, n_batches=3, thr=thr2)\n",
    "\n",
    "    # 6) light mixed pass to lift precision a touch (keep recall)\n",
    "    crit_mix = StreakSegLossFP(w_aftl=0.30, w_lovasz=0.10, w_ohem=0.02,\n",
    "                               aftl_alpha=0.35, aftl_beta=0.65, aftl_gamma=1.2,\n",
    "                               ohem_neg_percent=0.02, bg_lambda=0.00,\n",
    "                               pos_weight=torch.tensor(1.0, device=device)).to(device)\n",
    "    fit_quick(probe, crit_mix, train_loader_small, epochs=1, max_batches=2000, lr=1.5e-4, metric_thr=thr2, weight_decay=0.0)\n",
    "\n",
    "    # final threshold in a precision-friendlier band (8–12%)\n",
    "    thr_final, (Pf, Rf, Ff), auxf = pick_thr_with_floor(probe, val_loader_small, max_batches=200, n_bins=256, beta=1.0, min_pos_rate=0.08, max_pos_rate=0.12)\n",
    "    print(f\"[FINAL] thr = {thr_final:.3f} | P≈{Pf:.3f} R≈{Rf:.3f} F≈{Ff:.3f} pos_rate≈{auxf['pos_rate']:.3f}\")\n",
    "    quick_prob_stats(probe, train_loader_small, n_batches=3, thr=thr_final)\n",
    "\n",
    "    return probe, thr_final\n",
    "\n",
    "# ===== Run it =====\n",
    "probe, metric_thr = run_probe_recipe(UNetResSEASPP, p0=0.80)\n",
    "print (\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6de1c9-e006-448a-8ff0-f031c31e7374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Asteroid detection",
   "language": "python",
   "name": "asteroid_detection_cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
