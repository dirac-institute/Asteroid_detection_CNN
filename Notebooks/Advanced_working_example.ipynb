{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:37:39.817643Z",
     "start_time": "2025-06-10T19:37:37.251706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sklearn"
   ],
   "id": "c757f4099bf6030e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 21:37:38.443352: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-10 21:37:38.532524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/karlo/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T21:58:00.743256Z",
     "start_time": "2025-06-10T21:58:00.727288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def activation_parser(activation_str):\n",
    "    \"\"\"\n",
    "    Parse a string to return the corresponding activation function.\n",
    "    Supported strings: 'relu', 'sigmoid', 'tanh', 'leaky_relu'.\n",
    "    \"\"\"\n",
    "    if activation_str.lower() == \"elu\":\n",
    "        return nn.ELU(inplace=True)\n",
    "    elif activation_str.lower() == \"hardshrink\":\n",
    "        return nn.Hardshrink(lambd=0.5)\n",
    "    elif activation_str.lower() == \"hardtanh\":\n",
    "        return nn.Hardtanh(min_val=-1, max_val=1, inplace=True)\n",
    "    elif activation_str.lower() == \"logsigmoid\":\n",
    "        return nn.LogSigmoid()\n",
    "    elif activation_str.lower() == \"relu6\":\n",
    "        return nn.ReLU6(inplace=True)\n",
    "    elif activation_str.lower() == \"softplus\":\n",
    "        return nn.Softplus()\n",
    "    elif activation_str.lower() == \"selu\":\n",
    "        return nn.SELU(inplace=True)\n",
    "    elif activation_str.lower() == \"prelu\":\n",
    "        return nn.PReLU()\n",
    "    elif activation_str.lower() == \"softmax\":\n",
    "        return nn.Softmax(dim=1)  # softmax along channel dimension\n",
    "    if activation_str.lower() == 'relu':\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif activation_str.lower() == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation_str.lower() == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation_str.lower() == 'leaky_relu':\n",
    "        return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // ratio, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(in_channels // ratio, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.norm = nn.LayerNorm(in_channels)\n",
    "        self.norm = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        #nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)        # (B, C, 1, 1)\n",
    "        avg_out = avg_out.view(avg_out.size(0), avg_out.size(1))  # (B, C)\n",
    "        avg_out = self.fc1(avg_out)       # (B, C//ratio)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.fc2(avg_out)       # (B, C)\n",
    "\n",
    "        max_out = self.max_pool(x)        # (B, C, 1, 1)\n",
    "        max_out = max_out.view(max_out.size(0), max_out.size(1))  # (B, C)\n",
    "        max_out = self.fc1(max_out)       # (B, C//ratio)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.fc2(max_out)       # (B, C)\n",
    "\n",
    "        out = avg_out + max_out           # (B, C)\n",
    "        out = self.norm(out)              # (B, C)\n",
    "        scale = self.sigmoid(out)         # (B, C)\n",
    "        scale = scale.view(scale.size(0), scale.size(1), 1, 1)  # (B, C, 1, 1)\n",
    "        return x * scale                  # broadcast along H, W\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 5, 7)\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.BatchNorm2d(1)\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_in', nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)     # (B, 1, H, W)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)   # (B, 1, H, W)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)    # (B, 2, H, W)\n",
    "        attn = self.conv(concat)                         # (B, 1, H, W)\n",
    "        attn = self.norm(attn)                           # (B, 1, H, W)\n",
    "        attn = self.sigmoid(attn)\n",
    "        return x * attn                                  # broadcast across C\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_channels, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        x = self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, kernel_size, padding, dilation=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_ch, in_ch, kernel_size=kernel_size,\n",
    "            padding=padding, dilation=dilation,\n",
    "            groups=in_ch, bias=True\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=True)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act = activation_parser(activation)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.depthwise.weight, mode='fan_out', nonlinearity=activation)\n",
    "        nn.init.constant_(self.depthwise.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.pointwise.weight, mode='fan_out', nonlinearity=activation)\n",
    "        nn.init.constant_(self.pointwise.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.norm(x))\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        dilations = [1, 2, 3, 4]\n",
    "        kernels   = [1, 3, 5, 7]\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d, k in zip(dilations, kernels):\n",
    "            pad = (k // 2) * d\n",
    "            self.branches.append(\n",
    "                SepConv(in_ch, out_ch, activation, kernel_size=k, padding=pad, dilation=d)\n",
    "            )\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(dilations) * out_ch, out_ch, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation)\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.merge[0].weight, mode='fan_out', nonlinearity=activation)\n",
    "        nn.init.constant_(self.merge[0].bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [branch(x) for branch in self.branches]\n",
    "        x = torch.cat(outs, dim=1)\n",
    "        return self.merge(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation)\n",
    "        )\n",
    "        for m in self.block.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=activation)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        # W_g projects gating signal\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # W_x projects skip connection\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # psi computes 1‐channel attention map\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, F_g, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_g),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.W_g[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.W_g[0].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.W_x[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.W_x[0].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.psi[0].weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        nn.init.constant_(self.psi[0].bias, 0)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        \"\"\"\n",
    "        g: gating signal from decoder, shape (B, F_g, H, W)\n",
    "        x: skip connection from encoder, shape (B, F_l, H, W)\n",
    "        \"\"\"\n",
    "        g1 = self.W_g(g)   # (B, F_int, H, W)\n",
    "        x1 = self.W_x(x)   # (B, F_int, H, W)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)   # (B, 1, H, W)\n",
    "        return x * psi        # broadcast along channel\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, dropout_prob=0.0, attention=True, pool=True, ASPP_blocks=True):\n",
    "        super().__init__()\n",
    "        if ASPP_blocks:\n",
    "            # Use ASPP instead of DoubleConv\n",
    "            self.conv = ASPP(in_ch, out_ch, activation)\n",
    "        else:\n",
    "            # Use DoubleConv if ASPP_blocks is False\n",
    "            self.conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "        self.pool        = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = x.clone()\n",
    "        if self.pool:\n",
    "            x = F.max_pool2d(x, kernel_size=2)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, activation, dropout_prob=0.0, attention=True, upsample=True, ASPP_blocks=True):\n",
    "        \"\"\"\n",
    "        in_ch:   channels from previous layer (bottleneck or previous decoder)\n",
    "        skip_ch: channels in the corresponding encoder skip\n",
    "        out_ch:  desired output channels for this decoder block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.skip_ch = skip_ch\n",
    "\n",
    "        if self.upsample:\n",
    "            # ConvTranspose2d(in_ch → skip_ch) to match spatial & channel dims\n",
    "            self.up = nn.ConvTranspose2d(in_ch, skip_ch, kernel_size=3,\n",
    "                                         stride=2, padding=1, output_padding=1, bias=True)\n",
    "            nn.init.kaiming_normal_(self.up.weight, mode='fan_out', nonlinearity='relu')\n",
    "            self.bn_up = nn.BatchNorm2d(skip_ch)\n",
    "            self.act_up = activation_parser(activation)\n",
    "            self.attention = AttentionGate(F_g=skip_ch, F_l=skip_ch, F_int=skip_ch // 2) if attention else nn.Identity()\n",
    "        else:\n",
    "            self.up = None\n",
    "            self.bn_up = None\n",
    "            self.act_up = None\n",
    "            self.attention = AttentionGate(F_g=in_ch, F_l=in_ch, F_int=in_ch // 2) if attention else nn.Identity()\n",
    "\n",
    "        #self.double_conv = DoubleConv(in_double, out_ch, activation)\n",
    "        if ASPP_blocks:\n",
    "            # Use ASPP instead of DoubleConv\n",
    "            self.conv = ASPP(in_ch, out_ch, activation)\n",
    "        else:\n",
    "            # Use DoubleConv if ASPP_blocks is False\n",
    "            self.conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        if self.upsample:\n",
    "            x = self.up(x)       # (B, skip_ch, H*2, W*2)\n",
    "            x = self.bn_up(x)\n",
    "            x = self.act_up(x)\n",
    "        if skip is not None:\n",
    "            skip = self.attention(g=x, x=skip)\n",
    "            x = torch.cat([x, skip], dim=1)  # (B, 2*skip_ch, H*2, W*2)\n",
    "        x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleneckTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a tensor of shape (B, C, H, W), flattens the H×W patches into tokens,\n",
    "    runs a small TransformerEncoder over them, then reshapes back to (B, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=8, depth=3, mlp_dim=None):\n",
    "        super().__init__()\n",
    "        mlp_dim = mlp_dim or dim * 4\n",
    "        # one TransformerEncoderLayer (or more, if depth>1)\n",
    "        layer_e = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        layer_d = nn.TransformerDecoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            activation='relu',\n",
    "            norm_first=True,  # important for TransformerDecoder\n",
    "            #batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(layer_e, num_layers=depth//2 if depth > 1 else depth)\n",
    "        self.norm    = nn.LayerNorm(dim)\n",
    "        if depth > 1:\n",
    "            self.decoder = nn.TransformerDecoder(layer_d, num_layers=depth - depth//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # flatten spatial dims:\n",
    "        # → (B, C, H*W) then permute to (H*W, B, C) for PyTorch’s MHSA\n",
    "        tokens = x.flatten(2).permute(2, 0, 1)   # (H*W, B, C)\n",
    "        # run through TransformerEncoder\n",
    "        out   = self.encoder(tokens)             # (H*W, B, C)\n",
    "        # run through TransformerDecoder (optional, if depth > 1)\n",
    "        if hasattr(self, 'decoder'):\n",
    "            out = self.decoder(out, out)          # (H*W, B, C)\n",
    "        # put back into (B, C, H, W) after a LayerNorm on each token\n",
    "        out   = out.permute(1, 2, 0).view(B, C, H, W)\n",
    "        return self.norm(out.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        # explanation of the two permutes:\n",
    "        #  - out.permute(1,2,0)→(B, C, H*W) then .view(B, C, H, W)\n",
    "        #  - we want LN over the C‐dimension, so we permute to (B, H, W, C), apply LayerNorm,\n",
    "        #    then back to (B, C, H, W).\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 out_channels=1,\n",
    "                 down_filters=None,\n",
    "                 down_activations=None,\n",
    "                 up_filters=None,\n",
    "                 up_activations=None,\n",
    "                 bottleneck_transformer=True,\n",
    "                 ASPP_blocks=True):\n",
    "        super().__init__()\n",
    "        assert len(down_filters) == len(down_activations)\n",
    "        assert len(up_filters)   == len(up_activations)\n",
    "\n",
    "        # Build Encoder path\n",
    "        self.input_norm = nn.BatchNorm2d(in_channels)\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.bottleneck_transformer = bottleneck_transformer\n",
    "        prev_ch = in_channels\n",
    "        for i, out_ch in enumerate(down_filters):\n",
    "            act_str = down_activations[i].lower()\n",
    "            self.encoders.append(\n",
    "                EncoderBlock(in_ch=prev_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_str,\n",
    "                             dropout_prob=0.0,\n",
    "                             attention=(i != 0),\n",
    "                             pool=True,\n",
    "                             ASPP_blocks=ASPP_blocks)\n",
    "            )\n",
    "            prev_ch = out_ch\n",
    "\n",
    "        # Bottleneck:\n",
    "        if bottleneck_transformer:\n",
    "            self.bottleneck  = BottleneckTransformer(dim=down_filters[-1],\n",
    "                                                           heads=4,\n",
    "                                                           depth=4)\n",
    "        else:\n",
    "            self.bottleneck = nn.Identity()\n",
    "\n",
    "        # Build Decoder path\n",
    "        self.decoders = nn.ModuleList()\n",
    "        N = len(down_filters)\n",
    "        for i in range(len(up_filters)):\n",
    "            act_str = up_activations[i].lower()\n",
    "            # Corresponding skip channels from encoder\n",
    "            skip_ch = down_filters[N - 1 - i]\n",
    "            # Input channels for this decoder block\n",
    "            out_ch = up_filters[i]\n",
    "            in_ch_dec = (down_filters[-1] * 1) if (i == 0) else up_filters[i - 1]\n",
    "\n",
    "            self.decoders.append(\n",
    "                DecoderBlock(in_ch=in_ch_dec,\n",
    "                             skip_ch=skip_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_str,\n",
    "                             dropout_prob=0.0,\n",
    "                             attention= True,\n",
    "                             upsample=True,\n",
    "                             ASPP_blocks=ASPP_blocks)\n",
    "            )\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(up_filters[-1], out_channels, kernel_size=5, padding=2, bias=True),\n",
    "            nn.Sigmoid())\n",
    "        nn.init.kaiming_normal_(self.final_conv[0].weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        nn.init.constant_(self.final_conv[0].bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)  # Normalize input\n",
    "        # x: (B, 1, 128, 128)\n",
    "        skips = []\n",
    "        for enc in self.encoders[:-1]:  # skip last encoder (bottleneck)\n",
    "            x, skip = enc(x)\n",
    "            skips.append(skip)\n",
    "\n",
    "        # Bottleneck:\n",
    "        x, _ = self.encoders[-1](x) # last encoder does not return a skip\n",
    "        skips.append(None)\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoders[0](x, skips[-1])  # first decoder uses the last encoder skip\n",
    "\n",
    "        skips = skips[::-1]              # reverse order for decoding\n",
    "\n",
    "        for i in range(1, len(self.decoders)):\n",
    "            skip_feat = skips[i]\n",
    "            x = self.decoders[i](x, skip_feat)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ],
   "id": "6b0ee8133ceb7c68",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:49:56.619390Z",
     "start_time": "2025-06-10T19:49:56.503686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) pip install torchinfo\n",
    "#    (if you haven’t already)\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "down_filters     = [32, 64, 128, 256, 512]\n",
    "down_activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "\n",
    "up_filters       = [512, 256, 128]\n",
    "up_activations   = ['relu', 'relu', 'relu']\n",
    "\n",
    "# 2) Re‐instantiate your UNet exactly as in your training code:\n",
    "model = UNet(\n",
    "        down_filters=down_filters,\n",
    "        down_activations=down_activations,\n",
    "        up_filters=up_filters,\n",
    "        up_activations=up_activations,\n",
    "bottleneck_transformer=False,\n",
    "ASPP_blocks=False)\n",
    "\n",
    "# 3) Ask for a summary on a dummy (1×1×128×128) input:\n",
    "_ = summary(\n",
    "    model,\n",
    "    input_size=(128, 1, 128, 128),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "ead34bf5eaaf1c3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Trainable\n",
      "======================================================================================================================================================\n",
      "UNet                                               [128, 1, 128, 128]        [128, 1, 32, 32]          --                        True\n",
      "├─BatchNorm2d: 1-1                                 [128, 1, 128, 128]        [128, 1, 128, 128]        2                         True\n",
      "├─ModuleList: 1-2                                  --                        --                        --                        True\n",
      "│    └─EncoderBlock: 2-1                           [128, 1, 128, 128]        [128, 32, 64, 64]         --                        True\n",
      "│    │    └─DoubleConv: 3-1                        [128, 1, 128, 128]        [128, 32, 128, 128]       9,696                     True\n",
      "│    │    └─Identity: 3-2                          [128, 32, 128, 128]       [128, 32, 128, 128]       --                        --\n",
      "│    │    └─Identity: 3-3                          [128, 32, 128, 128]       [128, 32, 128, 128]       --                        --\n",
      "│    └─EncoderBlock: 2-2                           [128, 32, 64, 64]         [128, 64, 32, 32]         --                        True\n",
      "│    │    └─DoubleConv: 3-4                        [128, 32, 64, 64]         [128, 64, 64, 64]         55,680                    True\n",
      "│    │    └─CBAMBlock: 3-5                         [128, 64, 64, 64]         [128, 64, 64, 64]         1,252                     True\n",
      "│    │    └─Identity: 3-6                          [128, 64, 64, 64]         [128, 64, 64, 64]         --                        --\n",
      "│    └─EncoderBlock: 2-3                           [128, 64, 32, 32]         [128, 128, 16, 16]        --                        True\n",
      "│    │    └─DoubleConv: 3-7                        [128, 64, 32, 32]         [128, 128, 32, 32]        221,952                   True\n",
      "│    │    └─CBAMBlock: 3-8                         [128, 128, 32, 32]        [128, 128, 32, 32]        4,452                     True\n",
      "│    │    └─Identity: 3-9                          [128, 128, 32, 32]        [128, 128, 32, 32]        --                        --\n",
      "│    └─EncoderBlock: 2-4                           [128, 128, 16, 16]        [128, 256, 8, 8]          --                        True\n",
      "│    │    └─DoubleConv: 3-10                       [128, 128, 16, 16]        [128, 256, 16, 16]        886,272                   True\n",
      "│    │    └─CBAMBlock: 3-11                        [128, 256, 16, 16]        [128, 256, 16, 16]        16,996                    True\n",
      "│    │    └─Identity: 3-12                         [128, 256, 16, 16]        [128, 256, 16, 16]        --                        --\n",
      "│    └─EncoderBlock: 2-5                           [128, 256, 8, 8]          [128, 512, 4, 4]          --                        True\n",
      "│    │    └─DoubleConv: 3-13                       [128, 256, 8, 8]          [128, 512, 8, 8]          3,542,016                 True\n",
      "│    │    └─CBAMBlock: 3-14                        [128, 512, 8, 8]          [128, 512, 8, 8]          66,660                    True\n",
      "│    │    └─Identity: 3-15                         [128, 512, 8, 8]          [128, 512, 8, 8]          --                        --\n",
      "├─ModuleList: 1-3                                  --                        --                        --                        True\n",
      "│    └─DecoderBlock: 2-6                           [128, 512, 4, 4]          [128, 512, 8, 8]          396,288                   True\n",
      "│    │    └─ConvTranspose2d: 3-16                  [128, 512, 4, 4]          [128, 512, 8, 8]          2,359,808                 True\n",
      "│    │    └─BatchNorm2d: 3-17                      [128, 512, 8, 8]          [128, 512, 8, 8]          1,024                     True\n",
      "│    │    └─ReLU: 3-18                             [128, 512, 8, 8]          [128, 512, 8, 8]          --                        --\n",
      "│    │    └─DoubleConv: 3-19                       [128, 512, 8, 8]          [128, 512, 8, 8]          4,721,664                 True\n",
      "│    │    └─CBAMBlock: 3-20                        [128, 512, 8, 8]          [128, 512, 8, 8]          66,660                    True\n",
      "│    │    └─Identity: 3-21                         [128, 512, 8, 8]          [128, 512, 8, 8]          --                        --\n",
      "│    └─DecoderBlock: 2-7                           [128, 512, 8, 8]          [128, 256, 16, 16]        --                        True\n",
      "│    │    └─ConvTranspose2d: 3-22                  [128, 512, 8, 8]          [128, 256, 16, 16]        1,179,904                 True\n",
      "│    │    └─BatchNorm2d: 3-23                      [128, 256, 16, 16]        [128, 256, 16, 16]        512                       True\n",
      "│    │    └─ReLU: 3-24                             [128, 256, 16, 16]        [128, 256, 16, 16]        --                        --\n",
      "│    │    └─AttentionGate: 3-25                    --                        [128, 256, 16, 16]        99,840                    True\n",
      "│    │    └─DoubleConv: 3-26                       [128, 512, 16, 16]        [128, 256, 16, 16]        1,771,008                 True\n",
      "│    │    └─CBAMBlock: 3-27                        [128, 256, 16, 16]        [128, 256, 16, 16]        16,996                    True\n",
      "│    │    └─Identity: 3-28                         [128, 256, 16, 16]        [128, 256, 16, 16]        --                        --\n",
      "│    └─DecoderBlock: 2-8                           [128, 256, 16, 16]        [128, 128, 32, 32]        --                        True\n",
      "│    │    └─ConvTranspose2d: 3-29                  [128, 256, 16, 16]        [128, 128, 32, 32]        295,040                   True\n",
      "│    │    └─BatchNorm2d: 3-30                      [128, 128, 32, 32]        [128, 128, 32, 32]        256                       True\n",
      "│    │    └─ReLU: 3-31                             [128, 128, 32, 32]        [128, 128, 32, 32]        --                        --\n",
      "│    │    └─AttentionGate: 3-32                    --                        [128, 128, 32, 32]        25,344                    True\n",
      "│    │    └─DoubleConv: 3-33                       [128, 256, 32, 32]        [128, 128, 32, 32]        443,136                   True\n",
      "│    │    └─CBAMBlock: 3-34                        [128, 128, 32, 32]        [128, 128, 32, 32]        4,452                     True\n",
      "│    │    └─Identity: 3-35                         [128, 128, 32, 32]        [128, 128, 32, 32]        --                        --\n",
      "├─Sequential: 1-4                                  [128, 128, 32, 32]        [128, 1, 32, 32]          --                        True\n",
      "│    └─Conv2d: 2-9                                 [128, 128, 32, 32]        [128, 1, 32, 32]          3,201                     True\n",
      "│    └─Sigmoid: 2-10                               [128, 1, 32, 32]          [128, 1, 32, 32]          --                        --\n",
      "======================================================================================================================================================\n",
      "Total params: 16,190,111\n",
      "Trainable params: 16,190,111\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 394.54\n",
      "======================================================================================================================================================\n",
      "Input size (MB): 8.39\n",
      "Forward/backward pass size (MB): 6413.24\n",
      "Params size (MB): 63.18\n",
      "Estimated Total Size (MB): 6484.80\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:49:58.235462Z",
     "start_time": "2025-06-10T19:49:58.232283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds:   Tensor (B,1,H,W) after Sigmoid\n",
    "        targets: Tensor (B,1,H,W) binary {0,1}\n",
    "        \"\"\"\n",
    "        p_flat = preds.view(-1)\n",
    "        t_flat = targets.view(-1)\n",
    "        intersection = (p_flat * t_flat).sum()\n",
    "        dice_coeff = (2. * intersection + self.smooth) / (p_flat.sum() + t_flat.sum() + self.smooth)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma, self.eps = alpha, gamma, eps\n",
    "        self.beta = 1 - alpha  # Ensure alpha + beta = 1\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        TP = (preds * targets).sum()\n",
    "        FP = (preds * (1 - targets)).sum()\n",
    "        FN = ((1 - preds) * targets).sum()\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return torch.pow((1 - tversky), self.gamma)\n",
    "\n",
    "class ComboLossTF(nn.Module):\n",
    "    def __init__(self, bce_weight=0.33, dice_weight=0.33, focal_twersky_weight=0.33):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss(smooth=1e-6)\n",
    "        self.FW = FocalTverskyLoss (alpha = 0.92, gamma=3.1)\n",
    "        self.bw, self.dw, self.fw = bce_weight, dice_weight, focal_twersky_weight\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds, targets both (B,1,H,W)\n",
    "        total_loss = 0\n",
    "        if self.bw > 0:\n",
    "            l_bce = self.bce(preds, targets)\n",
    "            total_loss += self.bw * l_bce\n",
    "        if self.dw > 0:\n",
    "            l_dice = self.dice(preds, targets)\n",
    "            total_loss += self.dw * l_dice\n",
    "        if self.fw > 0:\n",
    "            l_focal_tversky = self.FW(preds, targets)\n",
    "            total_loss += self.fw * l_focal_tversky\n",
    "        return total_loss"
   ],
   "id": "4668bc53416e595c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:49:59.420982Z",
     "start_time": "2025-06-10T19:49:59.417494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigzi(x, axis=None):\n",
    "    \"\"\"\n",
    "Compute the interquartile range (IQR) of x along the specified axis.\n",
    "    Args:\n",
    "        x: array-like, shape (P, H, W) or (H, W) or (N, C, H, W)\n",
    "        axis: axis along which to compute the IQR.\n",
    "              If None, computes over the flattened array.\n",
    "\n",
    "    Returns: float, the IQR of x.\n",
    "\n",
    "    \"\"\"\n",
    "    return 0.741 * (np.percentile(x, 75, axis=axis) - np.percentile(x, 25, axis=axis))\n",
    "\n",
    "def split_stack(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Split a stack of 2D panels into (nrows × ncols) tiles.\n",
    "    arr: ndarray, shape (P, H, W)\n",
    "    Returns: ndarray, shape (P * (H//nrows)*(W//ncols), nrows, ncols)\n",
    "    \"\"\"\n",
    "    P, H, W = arr.shape\n",
    "    pad_h = (-H) % nrows\n",
    "    pad_w = (-W) % ncols\n",
    "    if pad_h or pad_w:\n",
    "        arr = np.pad(arr,\n",
    "                     ((0, 0),\n",
    "                      (0, pad_h),\n",
    "                      (0, pad_w)),\n",
    "                     mode='constant',\n",
    "                     constant_values=0)\n",
    "    H2, W2 = arr.shape[1], arr.shape[2]\n",
    "    blocks = (arr\n",
    "              .reshape(P,\n",
    "                       H2 // nrows, nrows,\n",
    "                       W2 // ncols, ncols)\n",
    "              .swapaxes(2, 3))\n",
    "    P2, Hb, Wb, nr, nc = blocks.shape\n",
    "    out = blocks.reshape(P2 * Hb * Wb, nr, nc)\n",
    "    return out\n",
    "\n",
    "def build_datasets(npz_file, tile_size=128):\n",
    "    \"\"\"\n",
    "    Load data from .npz, clip exactly as TF did, split into tiles, return PyTorch tensors.\n",
    "      - Clips x to [-166.43, 169.96]\n",
    "      - Splits each large image into (tile_size × tile_size) patches\n",
    "      - Adds a channel dimension (→ shape (N, 1, tile_size, tile_size))\n",
    "    \"\"\"\n",
    "    data = np.load(npz_file)\n",
    "    x = data['x']  # shape (P, H, W)\n",
    "    y = data['y']\n",
    "\n",
    "    x = x/sigzi(x)  # normalize by interquartile range\n",
    "    x = np.clip(x, -7, 7) # clip to [-5, 5]\n",
    "\n",
    "    # Split into tiles (tile_size × tile_size)\n",
    "    x_tiles = split_stack(x, tile_size, tile_size)  # (N_tiles, tile_size, tile_size)\n",
    "    y_tiles = split_stack(y, tile_size, tile_size)\n",
    "\n",
    "    # Convert to FloatTensor and add channel dimension\n",
    "    x_tiles = torch.from_numpy(x_tiles).float().unsqueeze(1)  # (N, 1, tile_size, tile_size)\n",
    "    y_tiles = torch.from_numpy(y_tiles).float().unsqueeze(1)  # (N, 1, tile_size, tile_size)\n",
    "\n",
    "    return x_tiles, y_tiles\n",
    "\n",
    "def reshape_masks(masks, new_size):\n",
    "    \"\"\"\n",
    "    Resize binary masks (0/1) to `new_size`:\n",
    "      - Uses bilinear interpolation (same as TF’s tf.image.resize with bilinear)\n",
    "      - Applies torch.ceil(...) to recover {0,1} values exactly.\n",
    "    Input:\n",
    "      - masks: either a Tensor of shape (N, 1, H_orig, W_orig)\n",
    "               or a numpy array of shape (N, H_orig, W_orig)\n",
    "      - new_size: tuple (new_H, new_W)\n",
    "    Returns:\n",
    "      - Tensor of shape (N, 1, new_H, new_W), values in {0,1}\n",
    "    \"\"\"\n",
    "    if isinstance(masks, np.ndarray):\n",
    "        m = torch.from_numpy(masks).float().unsqueeze(1)  # → (N,1,H,W)\n",
    "    else:\n",
    "        m = masks  # assume already FloatTensor (N,1,H,W)\n",
    "    m_resized = F.interpolate(m, size=new_size, mode='bilinear', align_corners=False)\n",
    "    m_resized = torch.ceil(m_resized)\n",
    "    return m_resized.clamp(0, 1)\n",
    "\n",
    "def split_train_val(x_tiles, y_tiles, train_frac=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Shuffle and split x_tiles, y_tiles into two TensorDatasets: train (80%) and val (20%).\n",
    "    \"\"\"\n",
    "    n = x_tiles.shape[0]\n",
    "    idx = torch.randperm(n, generator=torch.Generator().manual_seed(seed))\n",
    "    split = int(train_frac * n)\n",
    "    train_idx = idx[:split]\n",
    "    val_idx   = idx[split:]\n",
    "    train_idx, val_idx = train_idx.sort().values, val_idx.sort().values\n",
    "    x_tr, y_tr = x_tiles[train_idx], y_tiles[train_idx]\n",
    "    x_val, y_val = x_tiles[val_idx], y_tiles[val_idx]\n",
    "    return TensorDataset(x_tr, y_tr), TensorDataset(x_val, y_val)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:50:00.021354Z",
     "start_time": "2025-06-10T19:50:00.016323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_ds, val_ds, epochs=100, batch_size=32, lr=1e-3, device=None):\n",
    "    \"\"\"\n",
    "    Train the model on train_ds, validate on val_ds, and print losses + F1 each epoch.\n",
    "    Resizes all masks to `output_size` so that preds and targets match in spatial dims.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 1) Figure out the model’s output spatial size by pushing a dummy 128×128 patch.\n",
    "    model.eval()  # ensure BatchNorm uses running‐stats, not “batch” stats\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1, 1, 128, 128).to(device)\n",
    "        out_dummy = model(dummy)\n",
    "        output_size = (out_dummy.shape[-2], out_dummy.shape[-1])  # e.g. (32,32) for your JSON\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "\n",
    "    criterion = ComboLossTF(bce_weight=0.0, dice_weight=0.0, focal_twersky_weight=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=lr,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=epochs,\n",
    "                                                pct_start=0.1,\n",
    "                                                anneal_strategy='cos')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ——— Training ———\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        tp = fp = fn = 0\n",
    "        for batch_num, (imgs, masks) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)  # (B,1,128,128)\n",
    "\n",
    "            # Resize the ground‐truth masks to output_size (e.g. (32,32))\n",
    "            m_resized = reshape_masks(masks, new_size=output_size).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)              # (B,1, output_H, output_W)\n",
    "            loss = criterion(preds, m_resized)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            sched.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_bin = (preds > 0.5).float()\n",
    "                t = m_resized\n",
    "                tp += (pred_bin * t).sum().item()\n",
    "                fp += (pred_bin * (1 - t)).sum().item()\n",
    "                fn += ((1 - pred_bin) * t).sum().item()\n",
    "\n",
    "            prec = tp / (tp + fp + 1e-8)\n",
    "            rec  = tp / (tp + fn + 1e-8)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            print (f\"\\rEpoch {epoch:03d}  \"\n",
    "                   f\"Batch {batch_num+1:03d}/{len(train_loader)}  \"\n",
    "                   f\"Batch Loss: {loss.item():.4f}  \"\n",
    "                   f\"| train F1: {f1:.4f}  | train precision: {prec:.4f}  | train recall: {rec:.4f}\", end='\\r')\n",
    "\n",
    "        train_loss = running_loss / len(train_ds)\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "\n",
    "        # ——— Validation ———\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        tp = fp = fn = 0\n",
    "        val_y = []\n",
    "        pred_val = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                m_resized = reshape_masks(masks, new_size=output_size).to(device)\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, m_resized)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                pred_bin = (preds > 0.5).float()\n",
    "                tp += (pred_bin * m_resized).sum().item()\n",
    "                fp += (pred_bin * (1 - m_resized)).sum().item()\n",
    "                fn += ((1 - pred_bin) * m_resized).sum().item()\n",
    "                val_y.append(m_resized.cpu().numpy())\n",
    "                pred_val.append(preds.cpu().numpy())\n",
    "        # Collect all validation masks for AUC calculation\n",
    "        val_y = np.concatenate(val_y, axis=0)\n",
    "        preds_val = np.concatenate(pred_val, axis=0)  # (N, 1, Hout, Wout)\n",
    "        val_loss = val_loss / len(val_ds)\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1_val = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "        auc_val = sklearn.metrics.roc_auc_score(val_y.flatten(), preds_val.flatten() )\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}  \"\n",
    "              f\"| Val Loss: {val_loss:.4f}  \"\n",
    "              f\"| Train F1: {f1:.4f}  \"\n",
    "              f\"| Val F1: {f1_val:.4f}  \"\n",
    "              f\"| Val Prec: {prec:.4f}  \"\n",
    "              f\"| Val Rec: {rec:.4f}\"\n",
    "              f\"| Val AUC: {auc_val:.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "b42c2e411098d4b5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T19:38:56.029359Z",
     "start_time": "2025-06-10T19:37:41.424579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "npz_file = \"../DATA/train.npz\"\n",
    "x_tiles, y_tiles = build_datasets(npz_file, tile_size=128)\n",
    "train_ds, val_ds = split_train_val(x_tiles, y_tiles, train_frac=0.8, seed=42)\n",
    "del x_tiles, y_tiles"
   ],
   "id": "cdca56a2e5d4e6d2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-10T21:58:39.503754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "down_filters     = [32, 64, 128, 256, 512]\n",
    "down_activations = ['relu', 'sigmoid', 'relu', 'relu', 'relu']\n",
    "\n",
    "up_filters       = [512, 256, 128]\n",
    "up_activations   = ['relu', 'relu', 'relu']\n",
    "\n",
    "model = UNet(\n",
    "        down_filters=down_filters,\n",
    "        down_activations=down_activations,\n",
    "        up_filters=up_filters,\n",
    "        up_activations=up_activations,\n",
    "        bottleneck_transformer=False,\n",
    "        ASPP_blocks=False)\n",
    "\n",
    "\n",
    "trained_model = train_model(model,\n",
    "                            train_ds, val_ds,\n",
    "                            epochs=150,\n",
    "                            batch_size=128,\n",
    "                            lr=0.0015)"
   ],
   "id": "dbe140c0ff8133fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014  Train Loss: 0.5858  | Val Loss: 0.6368  | Train F1: 0.0663  | Val F1: 0.0323  | Val Prec: 0.0168  | Val Rec: 0.4643| Val AUC: 0.7108\n",
      "Epoch 015  Train Loss: 0.5706  | Val Loss: 0.5600  | Train F1: 0.0774  | Val F1: 0.1270  | Val Prec: 0.0810  | Val Rec: 0.2934| Val AUC: 0.6653\n",
      "Epoch 016  Train Loss: 0.5205  | Val Loss: 0.5386  | Train F1: 0.0971  | Val F1: 0.0926  | Val Prec: 0.0533  | Val Rec: 0.3516| Val AUC: 0.6945\n",
      "Epoch 017  Train Loss: 0.5214  | Val Loss: 0.5089  | Train F1: 0.1054  | Val F1: 0.1383  | Val Prec: 0.0871  | Val Rec: 0.3364| Val AUC: 0.6856\n",
      "Epoch 018  Train Loss: 0.4921  | Val Loss: 0.5418  | Train F1: 0.1157  | Val F1: 0.0654  | Val Prec: 0.0357  | Val Rec: 0.3838| Val AUC: 0.6825\n",
      "Epoch 019  Train Loss: 0.4944  | Val Loss: 0.5522  | Train F1: 0.1252  | Val F1: 0.1333  | Val Prec: 0.0854  | Val Rec: 0.3040| Val AUC: 0.6593\n",
      "Epoch 020  Train Loss: 0.5223  | Val Loss: 0.5140  | Train F1: 0.1028  | Val F1: 0.0961  | Val Prec: 0.0552  | Val Rec: 0.3695| Val AUC: 0.6858\n",
      "Epoch 021  Train Loss: 0.4840  | Val Loss: 0.5122  | Train F1: 0.1328  | Val F1: 0.1019  | Val Prec: 0.0589  | Val Rec: 0.3765| Val AUC: 0.6776\n",
      "Epoch 022  Train Loss: 0.4493  | Val Loss: 0.4917  | Train F1: 0.1462  | Val F1: 0.1010  | Val Prec: 0.0578  | Val Rec: 0.3998| Val AUC: 0.6854\n",
      "Epoch 023  Train Loss: 0.4548  | Val Loss: 0.4839  | Train F1: 0.1412  | Val F1: 0.1197  | Val Prec: 0.0710  | Val Rec: 0.3814| Val AUC: 0.6639\n",
      "Epoch 024  Train Loss: 0.4514  | Val Loss: 0.4644  | Train F1: 0.1503  | Val F1: 0.1549  | Val Prec: 0.0977  | Val Rec: 0.3733| Val AUC: 0.6855\n",
      "Epoch 025  Train Loss: 0.4444  | Val Loss: 0.4841  | Train F1: 0.1481  | Val F1: 0.2098  | Val Prec: 0.1518  | Val Rec: 0.3393| Val AUC: 0.6659\n",
      "Epoch 026  Train Loss: 0.4562  | Val Loss: 0.4850  | Train F1: 0.1521  | Val F1: 0.1707  | Val Prec: 0.1136  | Val Rec: 0.3424| Val AUC: 0.7077\n",
      "Epoch 027  Train Loss: 0.4385  | Val Loss: 0.5123  | Train F1: 0.1617  | Val F1: 0.2295  | Val Prec: 0.1869  | Val Rec: 0.2973| Val AUC: 0.6784\n",
      "Epoch 028  Train Loss: 0.4377  | Val Loss: 0.4532  | Train F1: 0.1632  | Val F1: 0.1796  | Val Prec: 0.1195  | Val Rec: 0.3614| Val AUC: 0.6896\n",
      "Epoch 029  Train Loss: 0.4316  | Val Loss: 0.4664  | Train F1: 0.1606  | Val F1: 0.1952  | Val Prec: 0.1338  | Val Rec: 0.3608| Val AUC: 0.6909\n",
      "Epoch 030  Train Loss: 0.4339  | Val Loss: 0.4638  | Train F1: 0.1452  | Val F1: 0.1946  | Val Prec: 0.1336  | Val Rec: 0.3581| Val AUC: 0.6815\n",
      "Epoch 031  Train Loss: 0.3878  | Val Loss: 0.5085  | Train F1: 0.1819  | Val F1: 0.0992  | Val Prec: 0.0572  | Val Rec: 0.3739| Val AUC: 0.6833\n",
      "Epoch 032  Train Loss: 0.4259  | Val Loss: 0.4793  | Train F1: 0.1612  | Val F1: 0.1395  | Val Prec: 0.0859  | Val Rec: 0.3714| Val AUC: 0.6726\n",
      "Epoch 033  Train Loss: 0.4368  | Val Loss: 0.5257  | Train F1: 0.1430  | Val F1: 0.0802  | Val Prec: 0.0448  | Val Rec: 0.3854| Val AUC: 0.6840\n",
      "Epoch 034  Train Loss: 0.4182  | Val Loss: 0.4744  | Train F1: 0.1626  | Val F1: 0.1659  | Val Prec: 0.1069  | Val Rec: 0.3700| Val AUC: 0.6837\n",
      "Epoch 035  Train Loss: 0.3991  | Val Loss: 0.4543  | Train F1: 0.1788  | Val F1: 0.1878  | Val Prec: 0.1264  | Val Rec: 0.3654| Val AUC: 0.6963\n",
      "Epoch 036  Train Loss: 0.4052  | Val Loss: 0.4402  | Train F1: 0.1841  | Val F1: 0.1789  | Val Prec: 0.1174  | Val Rec: 0.3753| Val AUC: 0.6846\n",
      "Epoch 037  Train Loss: 0.3984  | Val Loss: 0.4523  | Train F1: 0.1856  | Val F1: 0.1286  | Val Prec: 0.0764  | Val Rec: 0.4049| Val AUC: 0.7069\n",
      "Epoch 038  Train Loss: 0.4175  | Val Loss: 0.4437  | Train F1: 0.1569  | Val F1: 0.1793  | Val Prec: 0.1169  | Val Rec: 0.3851| Val AUC: 0.6799\n",
      "Epoch 039  Train Loss: 0.3836  | Val Loss: 0.4668  | Train F1: 0.1923  | Val F1: 0.2120  | Val Prec: 0.1501  | Val Rec: 0.3607| Val AUC: 0.6910\n",
      "Epoch 040  Train Loss: 0.3986  | Val Loss: 0.5349  | Train F1: 0.1972  | Val F1: 0.0701  | Val Prec: 0.0385  | Val Rec: 0.3910| Val AUC: 0.6862\n",
      "Epoch 041  Train Loss: 0.3953  | Val Loss: 0.4806  | Train F1: 0.1678  | Val F1: 0.1984  | Val Prec: 0.1397  | Val Rec: 0.3424| Val AUC: 0.6760\n",
      "Epoch 042  Train Loss: 0.3774  | Val Loss: 0.4607  | Train F1: 0.2045  | Val F1: 0.1474  | Val Prec: 0.0911  | Val Rec: 0.3854| Val AUC: 0.6979\n",
      "Epoch 043  Train Loss: 0.3695  | Val Loss: 0.4485  | Train F1: 0.2153  | Val F1: 0.2275  | Val Prec: 0.1643  | Val Rec: 0.3694| Val AUC: 0.6843\n",
      "Epoch 044  Train Loss: 0.3762  | Val Loss: 0.5031  | Train F1: 0.1832  | Val F1: 0.1625  | Val Prec: 0.1073  | Val Rec: 0.3344| Val AUC: 0.6770\n",
      "Epoch 045  Train Loss: 0.4027  | Val Loss: 0.4666  | Train F1: 0.1703  | Val F1: 0.1866  | Val Prec: 0.1273  | Val Rec: 0.3491| Val AUC: 0.6785\n",
      "Epoch 046  Train Loss: 0.3821  | Val Loss: 0.4428  | Train F1: 0.2111  | Val F1: 0.2138  | Val Prec: 0.1494  | Val Rec: 0.3757| Val AUC: 0.6683\n",
      "Epoch 047  Train Loss: 0.3719  | Val Loss: 0.4330  | Train F1: 0.2072  | Val F1: 0.2123  | Val Prec: 0.1466  | Val Rec: 0.3841| Val AUC: 0.6811\n",
      "Epoch 048  Train Loss: 0.3635  | Val Loss: 0.4612  | Train F1: 0.2312  | Val F1: 0.2554  | Val Prec: 0.2028  | Val Rec: 0.3448| Val AUC: 0.6672\n",
      "Epoch 049  Train Loss: 0.3947  | Val Loss: 0.4409  | Train F1: 0.1876  | Val F1: 0.2071  | Val Prec: 0.1441  | Val Rec: 0.3683| Val AUC: 0.6743\n",
      "Epoch 050  Train Loss: 0.3864  | Val Loss: 0.4409  | Train F1: 0.1812  | Val F1: 0.2238  | Val Prec: 0.1611  | Val Rec: 0.3663| Val AUC: 0.6789\n",
      "Epoch 051  Train Loss: 0.3663  | Val Loss: 0.5384  | Train F1: 0.2365  | Val F1: 0.1745  | Val Prec: 0.1248  | Val Rec: 0.2899| Val AUC: 0.6359\n",
      "Epoch 052  Train Loss: 0.3631  | Val Loss: 0.4441  | Train F1: 0.2122  | Val F1: 0.1911  | Val Prec: 0.1277  | Val Rec: 0.3798| Val AUC: 0.6747\n",
      "Epoch 053  Train Loss: 0.3663  | Val Loss: 0.4266  | Train F1: 0.2480  | Val F1: 0.2284  | Val Prec: 0.1626  | Val Rec: 0.3839| Val AUC: 0.6840\n",
      "Epoch 054  Train Loss: 0.3715  | Val Loss: 0.4687  | Train F1: 0.2432  | Val F1: 0.1147  | Val Prec: 0.0668  | Val Rec: 0.4055| Val AUC: 0.6961\n",
      "Epoch 055  Train Loss: 0.3522  | Val Loss: 0.4409  | Train F1: 0.2291  | Val F1: 0.2280  | Val Prec: 0.1654  | Val Rec: 0.3670| Val AUC: 0.6721\n",
      "Epoch 056  Train Loss: 0.3365  | Val Loss: 0.4567  | Train F1: 0.2686  | Val F1: 0.2347  | Val Prec: 0.1726  | Val Rec: 0.3667| Val AUC: 0.6721\n",
      "Epoch 057  Train Loss: 0.3526  | Val Loss: 0.4647  | Train F1: 0.2450  | Val F1: 0.1682  | Val Prec: 0.1083  | Val Rec: 0.3767| Val AUC: 0.6799\n",
      "Epoch 058  Train Loss: 0.3460  | Val Loss: 0.4330  | Train F1: 0.2195  | Val F1: 0.1869  | Val Prec: 0.1225  | Val Rec: 0.3935| Val AUC: 0.6932\n",
      "Epoch 059  Train Loss: 0.3397  | Val Loss: 0.4698  | Train F1: 0.2601  | Val F1: 0.1616  | Val Prec: 0.1037  | Val Rec: 0.3659| Val AUC: 0.6688\n",
      "Epoch 060  Train Loss: 0.3572  | Val Loss: 0.4593  | Train F1: 0.2303  | Val F1: 0.2903  | Val Prec: 0.2530  | Val Rec: 0.3405| Val AUC: 0.6784\n",
      "Epoch 061  Train Loss: 0.3300  | Val Loss: 0.4392  | Train F1: 0.2478  | Val F1: 0.2558  | Val Prec: 0.1968  | Val Rec: 0.3653| Val AUC: 0.6794\n",
      "Epoch 062  Train Loss: 0.3326  | Val Loss: 0.4476  | Train F1: 0.2742  | Val F1: 0.2066  | Val Prec: 0.1434  | Val Rec: 0.3690| Val AUC: 0.6792\n",
      "Epoch 063  Batch 442/647  Batch Loss: 0.0084  | train F1: 0.2681  | train precision: 0.1882  | train recall: 0.4658\r"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ],
   "id": "e7f5d5786ef9271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_path = \"../DATA/unet_model3.pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ],
   "id": "754226ac719ba211",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "down_filters     = [32, 64, 128, 256, 512]\n",
    "down_activations = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "\n",
    "up_filters       = [512, 256, 128]\n",
    "up_activations   = ['relu', 'relu', 'relu']\n",
    "\n",
    "model_loaded = UNet(\n",
    "    down_filters=down_filters,\n",
    "    down_activations=down_activations,\n",
    "    up_filters=up_filters,\n",
    "    up_activations=up_activations\n",
    ")\n",
    "\n",
    "# 2) Load the saved state_dict:\n",
    "checkpoint = torch.load(\"../DATA/unet_model3.pth\", map_location=\"cpu\")\n",
    "model_loaded.load_state_dict(checkpoint)\n",
    "\n",
    "# 3) Put into eval mode (if only doing inference):\n",
    "model_loaded.eval()\n"
   ],
   "id": "e4eaf1e38b6330f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict(model, dataset, batch_size=32, device=None,\n",
    "                  return_probs: bool = True,  # if False, returns binary masks (0/1)\n",
    "                  threshold: float = 0.5      # threshold for binarization if return_probs=False\n",
    "                 ):\n",
    "    \"\"\"\n",
    "    Run inference on `dataset` using `model` and return all predictions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): trained segmentation model (expects input shape (B,1,128,128) → output (B,1,Hout,Wout)).\n",
    "        dataset (torch.utils.data.Dataset): either\n",
    "            - A TensorDataset of (images, masks), or\n",
    "            - A Dataset that returns just `image` (no mask) if you only want predictions.\n",
    "        batch_size (int): batch size for DataLoader.\n",
    "        device (torch.device or str): 'cuda' or 'cpu'. If None, uses CUDA if available.\n",
    "        return_probs (bool):\n",
    "            - If True, returns the raw sigmoid‐probabilities of shape (N, 1, Hout, Wout).\n",
    "            - If False, thresholds those probabilities at `threshold` and returns binary masks (0/1).\n",
    "        threshold (float): cutoff for turning probability → 0/1 when return_probs=False.\n",
    "\n",
    "    Returns:\n",
    "        preds: numpy array of shape\n",
    "            - (N, 1, Hout, Wout) with float32 probs  in [0,1], if return_probs=True;\n",
    "            - (N, 1, Hout, Wout) with uint8 masks {0,1},       if return_probs=False.\n",
    "\n",
    "    Usage:\n",
    "        # 1) If you have (x_val, y_val) as a TensorDataset and want only predictions:\n",
    "        preds = predict_model(model, TensorDataset(torch.from_numpy(x_val).float(), torch.zeros(len(x_val),1,1,1)),\n",
    "                              batch_size=64, device='cuda', return_probs=False)\n",
    "\n",
    "        # 2) If your dataset yields only images (no masks):\n",
    "        preds = predict_model(model, test_dataset, batch_size=64, device='cuda', return_probs=True)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # We don’t need real masks during inference, so DataLoader can silently ignore them.\n",
    "    # We'll detect whether dataset returns (img, mask) or just img.\n",
    "    def _collate_fn(batch):\n",
    "        # batch is a list of dataset[i] returns.\n",
    "        # If dataset[i] is a tuple (img, mask), take only img.\n",
    "        if isinstance(batch[0], (list, tuple)):\n",
    "            imgs = torch.stack([item[0] for item in batch], dim=0)\n",
    "        else:\n",
    "            imgs = torch.stack(batch, dim=0)\n",
    "        return imgs\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=_collate_fn,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in loader:\n",
    "            imgs = imgs.to(device)                     # (B, 1, 128, 128) or similar\n",
    "            probs = model(imgs)                        # (B, 1, Hout, Wout), already in [0,1] due to final Sigmoid\n",
    "            if return_probs:\n",
    "                all_preds.append(probs.cpu())\n",
    "            else:\n",
    "                bin_masks = (probs > threshold).float()  # (B, 1, Hout, Wout) of 0.0 or 1.0\n",
    "                all_preds.append(bin_masks.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)  # (N, 1, Hout, Wout)\n",
    "    if return_probs:\n",
    "        return all_preds.numpy().astype('float32')\n",
    "    else:\n",
    "        # convert to uint8 (0/1) for easier downstream use\n",
    "        return all_preds.numpy().astype('uint8')\n"
   ],
   "id": "14d55bcde18116e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dataset_to_numpy(dataset, batch_size=64, device=None):\n",
    "    \"\"\"\n",
    "    Given a Dataset that returns either:\n",
    "      - (image_tensor, mask_tensor),  or\n",
    "      - just image_tensor\n",
    "    this function will loop once through the dataset, gather everything,\n",
    "    and return NumPy arrays.\n",
    "\n",
    "    Returns:\n",
    "      If dataset[i] returns (img, mask) for each i, then\n",
    "        imgs_np: shape (N, C, H, W) or whatever\n",
    "        masks_np: shape (N, Cm, Hm, Wm) (e.g. (N,1,128,128))\n",
    "      If dataset[i] returns only img, then\n",
    "        imgs_np: shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # We won’t actually move data to GPU here, just stack on CPU at the end.\n",
    "    # But if your dataset does expensive preprocessing on CPU, you can pin_memory=True.\n",
    "\n",
    "    def _collate_fn(batch):\n",
    "        # If each element is (img, mask), we stack only imgs and masks separately.\n",
    "        # But DataLoader collate_fn must return a single tensor; we’ll handle masks in the loop.\n",
    "        # Instead, we return the raw batch list and unpack in the loop below.\n",
    "        return batch\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=_collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    imgs_list = []\n",
    "    masks_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # batch is a list of length `batch_size` (or the remainder on the last batch).\n",
    "            # Each element is either (img, mask) or just img.\n",
    "            first_elem = batch[0]\n",
    "            if isinstance(first_elem, (tuple, list)) and len(first_elem) == 2:\n",
    "                # Dataset returns (img, mask)\n",
    "                imgs = torch.stack([item[0] for item in batch], dim=0)   # (B, C, H, W)\n",
    "                masks = torch.stack([item[1] for item in batch], dim=0)  # (B, Cm, Hm, Wm)\n",
    "                imgs_list.append(imgs.cpu().numpy())\n",
    "                masks_list.append(masks.cpu().numpy())\n",
    "            else:\n",
    "                # Dataset returns only img\n",
    "                imgs = torch.stack(batch, dim=0)  # (B, C, H, W)\n",
    "                imgs_list.append(imgs.cpu().numpy())\n",
    "\n",
    "    imgs_np = np.concatenate(imgs_list, axis=0)\n",
    "    if masks_list:\n",
    "        masks_np = np.concatenate(masks_list, axis=0)\n",
    "        return imgs_np, masks_np\n",
    "    else:\n",
    "        return imgs_np\n",
    "\n",
    "def f2_score_numpy(y_true, y_pred, threshold=0.5, eps=1e-8):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: arrays of the same shape, either (N,H,W) or (N,1,H,W).\n",
    "    threshold: cutoff on y_pred if it’s in [0,1]; if y_pred is already binary, set threshold<0 or skip binarize.\n",
    "    Returns one global F2 (scalar).\n",
    "    \"\"\"\n",
    "    # 1) Binarize predictions (if they’re probabilities)\n",
    "    if y_pred.dtype != np.uint8 and threshold >= 0:\n",
    "        p_bin = (y_pred > threshold).astype(np.uint8)\n",
    "    else:\n",
    "        p_bin = y_pred.astype(np.uint8)\n",
    "\n",
    "    # 2) Similarly ensure y_true is 0/1 uint8\n",
    "    y_bin = y_true.astype(np.uint8)\n",
    "\n",
    "    # 3) Flatten to 1D\n",
    "    if p_bin.ndim == 4 and p_bin.shape[1] == 1:\n",
    "        p_flat = p_bin.squeeze(1).ravel()\n",
    "        y_flat = y_bin.squeeze(1).ravel()\n",
    "    else:\n",
    "        p_flat = p_bin.ravel()\n",
    "        y_flat = y_bin.ravel()\n",
    "\n",
    "    # 4) Compute TP, FP, FN\n",
    "    TP = np.sum((p_flat == 1) & (y_flat == 1))\n",
    "    FP = np.sum((p_flat == 1) & (y_flat == 0))\n",
    "    FN = np.sum((p_flat == 0) & (y_flat == 1))\n",
    "\n",
    "    # 5) Precision = TP / (TP + FP), Recall = TP / (TP + FN)\n",
    "    prec = TP / (TP + FP + eps)\n",
    "    rec  = TP / (TP + FN + eps)\n",
    "\n",
    "    # 6) F2 = 5 * (prec * rec) / (4*prec + rec)\n",
    "    f2 = (1 + 2**2) * (prec * rec) / (2**2 * prec + rec + eps)\n",
    "    return f2"
   ],
   "id": "9025c0e112346b9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "p = predict(model_loaded, val_ds, batch_size=128, device='cuda', return_probs=True)\n",
    "val_x, val_y = dataset_to_numpy(val_ds, batch_size=128)\n",
    "val_y = reshape_masks(torch.from_numpy(val_y).float(), new_size=(32, 32)).numpy()  # resize to match model output size\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(val_y.flatten(), p.flatten())\n",
    "auc_score = sklearn.metrics.auc(fpr, tpr)\n",
    "ax[0].plot(fpr, tpr, label=f'AUC = {auc_score:.4f}')\n",
    "ax[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax[0].set_xlabel('False positive rate')\n",
    "ax[0].set_ylabel('True positive rate')\n",
    "ax[0].legend()\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(val_y.flatten(), p.flatten())\n",
    "ax[1].plot(precision, recall, label='Precision-Recall curve')\n",
    "ax[1].set_xlabel('Precision')\n",
    "ax[1].set_ylabel('Recall')\n",
    "ax[1].legend()\n",
    "ax[2].plot(thresholds, precision[1:], label='Precision')\n",
    "ax[2].plot(thresholds, recall[1:], label='Recall')\n",
    "ax[2].set_xlabel('Threshold')\n",
    "ax[2].set_ylabel('Score')\n",
    "ax[2].legend()\n",
    "print (f\"F2 score on validation set: {f2_score_numpy(val_y, p, threshold=0.5):.4f}\")"
   ],
   "id": "603238926fbca032",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print (f\"F2 score on validation set: {f2_score_numpy(val_y, p, threshold=0.1):.4f}\")",
   "id": "e31f80100bf5ab3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trained_model = train_model(model,\n",
    "                            train_ds, val_ds,\n",
    "                            epochs=150,\n",
    "                            batch_size=128,\n",
    "                            lr=1e-4)"
   ],
   "id": "29b3acb1925ba38a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "660ef874dd4ccc6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
