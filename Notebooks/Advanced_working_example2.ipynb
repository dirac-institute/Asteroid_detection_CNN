{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-08T14:28:00.443444Z",
     "start_time": "2025-06-08T14:27:59.217479Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sklearn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T14:28:05.565547Z",
     "start_time": "2025-06-08T14:28:05.553499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // ratio, kernel_size=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_channels // ratio, in_channels, kernel_size=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='linear')\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)        # (B, C, 1, 1)\n",
    "        avg_out = self.fc1(avg_out)       # (B, C//ratio, 1, 1)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.fc2(avg_out)       # (B, C, 1, 1)\n",
    "\n",
    "        max_out = self.max_pool(x)        # (B, C, 1, 1)\n",
    "        max_out = self.fc1(max_out)       # (B, C//ratio, 1, 1)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.fc2(max_out)       # (B, C, 1, 1)\n",
    "\n",
    "        out = avg_out + max_out           # (B, C, 1, 1)\n",
    "        scale = self.sigmoid(out)         # (B, C, 1, 1)\n",
    "        return x * scale                  # broadcast along H, W\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_in', nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)     # (B, 1, H, W)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)   # (B, 1, H, W)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)    # (B, 2, H, W)\n",
    "        attn = self.conv(concat)                         # (B, 1, H, W)\n",
    "        attn = self.sigmoid(attn)\n",
    "        return x * attn                                  # broadcast across C\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_channels, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        x = self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, kernel_size, padding, dilation=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_ch, in_ch, kernel_size=kernel_size,\n",
    "            padding=padding, dilation=dilation,\n",
    "            groups=in_ch, bias=False\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act = activation\n",
    "\n",
    "        nn.init.kaiming_normal_(self.depthwise.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.pointwise.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.norm(x))\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        dilations = [1, 2, 3, 4]\n",
    "        kernels   = [1, 3, 5, 7]\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d, k in zip(dilations, kernels):\n",
    "            pad = (k // 2) * d\n",
    "            self.branches.append(\n",
    "                SepConv(in_ch, out_ch, activation, kernel_size=k, padding=pad, dilation=d)\n",
    "            )\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(dilations) * out_ch, out_ch, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.merge[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [branch(x) for branch in self.branches]\n",
    "        x = torch.cat(outs, dim=1)\n",
    "        return self.merge(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation,\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation\n",
    "        )\n",
    "        for m in self.block.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        # W_g projects gating signal\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # W_x projects skip connection\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # psi computes 1‐channel attention map\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.W_g[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.W_x[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.psi[0].weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        \"\"\"\n",
    "        g: gating signal from decoder, shape (B, F_g, H, W)\n",
    "        x: skip connection from encoder, shape (B, F_l, H, W)\n",
    "        \"\"\"\n",
    "        g1 = self.W_g(g)   # (B, F_int, H, W)\n",
    "        x1 = self.W_x(x)   # (B, F_int, H, W)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)   # (B, 1, H, W)\n",
    "        return x * psi        # broadcast along channel\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, dropout_prob=0.0, attention=True, pool=True):\n",
    "        super().__init__()\n",
    "        #self.double_conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.aspp        = ASPP(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "        self.pool        = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.aspp(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = x.clone()\n",
    "        if self.pool:\n",
    "            x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, activation, dropout_prob=0.0, attention=True, upsample=True):\n",
    "        \"\"\"\n",
    "        in_ch:   channels from previous layer (bottleneck or previous decoder)\n",
    "        skip_ch: channels in the corresponding encoder skip\n",
    "        out_ch:  desired output channels for this decoder block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.skip_ch = skip_ch\n",
    "\n",
    "        if self.upsample:\n",
    "            # ConvTranspose2d(in_ch → skip_ch) to match spatial & channel dims\n",
    "            self.up = nn.ConvTranspose2d(in_ch, skip_ch, kernel_size=3,\n",
    "                                         stride=2, padding=1, output_padding=1, bias=False)\n",
    "            nn.init.kaiming_normal_(self.up.weight, mode='fan_out', nonlinearity='relu')\n",
    "            self.bn_up = nn.BatchNorm2d(skip_ch)\n",
    "            self.act_up = activation\n",
    "            self.attention = AttentionGate(F_g=skip_ch, F_l=skip_ch, F_int=skip_ch // 2) if attention else nn.Identity()\n",
    "            in_double = skip_ch * 2\n",
    "        else:\n",
    "            self.up = None\n",
    "            self.bn_up = None\n",
    "            self.act_up = None\n",
    "            self.attention = AttentionGate(F_g=in_ch, F_l=in_ch, F_int=in_ch // 2) if attention else nn.Identity()\n",
    "            in_double = in_ch * 2 if attention else in_ch\n",
    "\n",
    "        #self.double_conv = DoubleConv(in_double, out_ch, activation)\n",
    "        self.aspp        = ASPP(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        if self.upsample:\n",
    "            x = self.up(x)       # (B, skip_ch, H*2, W*2)\n",
    "            x = self.bn_up(x)\n",
    "            x = self.act_up(x)\n",
    "        if skip is not None and not isinstance(self.attention, nn.Identity):\n",
    "            skip = self.attention(g=x, x=skip)\n",
    "            x = torch.cat([x, skip], dim=1)  # (B, 2*skip_ch, H*2, W*2)\n",
    "        x = self.aspp(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleneckTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a tensor of shape (B, C, H, W), flattens the H×W patches into tokens,\n",
    "    runs a small TransformerEncoder over them, then reshapes back to (B, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=8, depth=3, mlp_dim=None):\n",
    "        super().__init__()\n",
    "        mlp_dim = mlp_dim or dim * 4\n",
    "        # one TransformerEncoderLayer (or more, if depth>1)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            activation='relu',\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=depth)\n",
    "        self.norm    = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # flatten spatial dims:\n",
    "        # → (B, C, H*W) then permute to (H*W, B, C) for PyTorch’s MHSA\n",
    "        tokens = x.flatten(2).permute(2, 0, 1)   # (H*W, B, C)\n",
    "        # run through TransformerEncoder\n",
    "        out   = self.encoder(tokens)             # (H*W, B, C)\n",
    "        # put back into (B, C, H, W) after a LayerNorm on each token\n",
    "        out   = out.permute(1, 2, 0).view(B, C, H, W)\n",
    "        return self.norm(out.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        # explanation of the two permutes:\n",
    "        #  - out.permute(1,2,0)→(B, C, H*W) then .view(B, C, H, W)\n",
    "        #  - we want LN over the C‐dimension, so we permute to (B, H, W, C), apply LayerNorm,\n",
    "        #    then back to (B, C, H, W).\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 out_channels=1,\n",
    "                 down_filters=None,\n",
    "                 down_activations=None,\n",
    "                 up_filters=None,\n",
    "                 up_activations=None):\n",
    "        super().__init__()\n",
    "        assert len(down_filters) == len(down_activations)\n",
    "        assert len(up_filters)   == len(up_activations)\n",
    "\n",
    "        # Build Encoder path\n",
    "        self.encoders = nn.ModuleList()\n",
    "        prev_ch = in_channels\n",
    "        for i, out_ch in enumerate(down_filters):\n",
    "            act_str = down_activations[i].lower()\n",
    "            if act_str == 'relu':\n",
    "                act_fn = nn.ReLU(inplace=True)\n",
    "            elif act_str == 'sigmoid':\n",
    "                act_fn = nn.Sigmoid()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported encoder activation: {act_str}\")\n",
    "\n",
    "            self.encoders.append(\n",
    "                EncoderBlock(in_ch=prev_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_fn,\n",
    "                             dropout_prob=0.1,\n",
    "                             attention=(i != 0),\n",
    "                             pool=True)\n",
    "            )\n",
    "            prev_ch = out_ch\n",
    "\n",
    "        # Bottleneck: DoubleConv(down_filters[-1] → down_filters[-1]*2)\n",
    "        #self.bottleneck = DoubleConv(down_filters[-1], down_filters[-1]*2, nn.ReLU(inplace=True))\n",
    "        print(down_filters[-1], down_filters[-1]*2, down_filters[-1]*4)\n",
    "        self.bottleneck_aspp   = ASPP(down_filters[-1], down_filters[-1]*2, nn.ReLU(inplace=True))\n",
    "        self.bottleneck_trans  = BottleneckTransformer(dim=down_filters[-1],\n",
    "                                                 heads=4,\n",
    "                                                 depth=4,\n",
    "                                                 mlp_dim=down_filters[-1] * 4\n",
    "                                                 )\n",
    "\n",
    "        # Build Decoder path\n",
    "        self.decoders = nn.ModuleList()\n",
    "        N = len(down_filters)\n",
    "        for i, out_ch in enumerate(up_filters):\n",
    "            act_str = up_activations[i].lower()\n",
    "            if act_str == 'relu':\n",
    "                act_fn = nn.ReLU(inplace=True)\n",
    "            elif act_str == 'sigmoid':\n",
    "                act_fn = nn.Sigmoid()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported decoder activation: {act_str}\")\n",
    "            # Corresponding skip channels from encoder\n",
    "            skip_ch = down_filters[N - 1 - i]\n",
    "            # Input channels for this decoder block\n",
    "            in_ch_dec = (down_filters[-1] * 1) if (i == 0) else up_filters[i - 1]\n",
    "            print (f\"Decoder {i}: in_ch_dec={in_ch_dec}, skip_ch={skip_ch}, out_ch={out_ch}\")\n",
    "\n",
    "            self.decoders.append(\n",
    "                DecoderBlock(in_ch=in_ch_dec,\n",
    "                             skip_ch=skip_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_fn,\n",
    "                             dropout_prob=0.1,\n",
    "                             attention= False if i == 0 else True,  # no attention in first decoder\n",
    "                             upsample=True)\n",
    "            )\n",
    "\n",
    "        # Final 3×3 conv + Sigmoid → 1 channel\n",
    "        self.final_conv = nn.Conv2d(up_filters[-1], out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        nn.init.kaiming_normal_(self.final_conv.weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        self.final_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 128, 128)\n",
    "        skips = []\n",
    "        for enc in self.encoders:\n",
    "            x, skip = enc(x)\n",
    "            skips.append(skip)\n",
    "        skips.append(None)\n",
    "\n",
    "        #x = self.bottleneck(x)          # (B, 1024, 1, 1)\n",
    "        print (f\"Bottleneck input shape: {x.shape}\")\n",
    "        #x = self.bottleneck_aspp(x)\n",
    "        #print (f\"Bottleneck ASPP output shape: {x.shape}\")\n",
    "        x = self.bottleneck_trans(x)\n",
    "        print (f\"Bottleneck Transformer output shape: {x.shape}\")\n",
    "        skips = skips[::-1]              # reverse order for decoding\n",
    "\n",
    "        for i, dec in enumerate(self.decoders):\n",
    "            skip_feat = skips[i]\n",
    "            if skip_feat is not None:\n",
    "                print (f\"Decoder {i} input shape: {x.shape}, skip_feat shape: {skip_feat.shape}\")\n",
    "            else:\n",
    "                print (f\"Decoder {i} has no skip connection\")\n",
    "            x = dec(x, skip_feat)\n",
    "            print (f\"Decoder {i} output shape: {x.shape}\")\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        x = self.final_sigmoid(x)\n",
    "        return x  # (B, 1, 32, 32) in your JSON case"
   ],
   "id": "a6c5a18501dda368",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T20:11:14.138626Z",
     "start_time": "2025-06-04T20:11:13.859020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) pip install torchinfo\n",
    "#    (if you haven’t already)\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "down_filters     = [64, 128, 256, 512]\n",
    "down_activations = ['relu'] * len(down_filters)\n",
    "\n",
    "up_filters       = [256, 128, 64]#down_filters[::-1]  # reverse the down_filters\n",
    "up_activations   = ['relu'] * len(up_filters)\n",
    "\n",
    "# 2) Re‐instantiate your UNet exactly as in your training code:\n",
    "model = UNet(\n",
    "        down_filters=down_filters,\n",
    "        down_activations=down_activations,\n",
    "        up_filters=up_filters,\n",
    "        up_activations=up_activations)\n",
    "\n",
    "# 3) Ask for a summary on a dummy (1×1×128×128) input:\n",
    "_ = summary(\n",
    "    model,\n",
    "    input_size=(128, 1, 128, 128),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "9b46da360bcba26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 1024 2048\n",
      "Decoder 0: in_ch_dec=512, skip_ch=512, out_ch=256\n",
      "Decoder 1: in_ch_dec=256, skip_ch=256, out_ch=128\n",
      "Decoder 2: in_ch_dec=128, skip_ch=128, out_ch=64\n",
      "Bottleneck input shape: torch.Size([128, 512, 8, 8])\n",
      "Bottleneck Transformer output shape: torch.Size([128, 512, 8, 8])\n",
      "Decoder 0 has no skip connection\n",
      "Decoder 0 output shape: torch.Size([128, 256, 16, 16])\n",
      "Decoder 1 input shape: torch.Size([128, 256, 16, 16]), skip_feat shape: torch.Size([128, 512, 16, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Identity: 3, Dropout2d: 3, EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, CBAMBlock: 3, ChannelAttention: 4, AdaptiveAvgPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, AdaptiveMaxPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, Sigmoid: 5, SpatialAttention: 4, Conv2d: 5, Sigmoid: 5, Dropout2d: 3, EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, CBAMBlock: 3, ChannelAttention: 4, AdaptiveAvgPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, AdaptiveMaxPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, Sigmoid: 5, SpatialAttention: 4, Conv2d: 5, Sigmoid: 5, Dropout2d: 3, EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, CBAMBlock: 3, ChannelAttention: 4, AdaptiveAvgPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, AdaptiveMaxPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, Sigmoid: 5, SpatialAttention: 4, Conv2d: 5, Sigmoid: 5, Dropout2d: 3, BottleneckTransformer: 1, TransformerEncoder: 2, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, LayerNorm: 2, DecoderBlock: 2, ConvTranspose2d: 3, BatchNorm2d: 3, ReLU: 5, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Identity: 3, Dropout2d: 3, ConvTranspose2d: 3, BatchNorm2d: 3, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torchinfo/torchinfo.py:295\u001B[39m, in \u001B[36mforward_pass\u001B[39m\u001B[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[39m\n\u001B[32m    294\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[32m--> \u001B[39m\u001B[32m295\u001B[39m     _ = model(*x, **kwargs)\n\u001B[32m    296\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1857\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1856\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1858\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1859\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1860\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1861\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1805\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1803\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1805\u001B[39m result = forward_call(*args, **kwargs)\n\u001B[32m   1806\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 357\u001B[39m, in \u001B[36mUNet.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    356\u001B[39m     \u001B[38;5;28mprint\u001B[39m (\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDecoder \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no skip connection\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m357\u001B[39m x = dec(x, skip_feat)\n\u001B[32m    358\u001B[39m \u001B[38;5;28mprint\u001B[39m (\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDecoder \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m output shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1857\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1856\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1858\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1859\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1860\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1861\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1805\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1803\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1805\u001B[39m result = forward_call(*args, **kwargs)\n\u001B[32m   1806\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 215\u001B[39m, in \u001B[36mDecoderBlock.forward\u001B[39m\u001B[34m(self, x, skip)\u001B[39m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m skip \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.attention, nn.Identity):\n\u001B[32m--> \u001B[39m\u001B[32m215\u001B[39m     skip = \u001B[38;5;28mself\u001B[39m.attention(g=x, x=skip)\n\u001B[32m    216\u001B[39m     x = torch.cat([x, skip], dim=\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# (B, 2*skip_ch, H*2, W*2)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1857\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1856\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1858\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1859\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1860\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1861\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1805\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1803\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1805\u001B[39m result = forward_call(*args, **kwargs)\n\u001B[32m   1806\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 152\u001B[39m, in \u001B[36mAttentionGate.forward\u001B[39m\u001B[34m(self, g, x)\u001B[39m\n\u001B[32m    151\u001B[39m g1 = \u001B[38;5;28mself\u001B[39m.W_g(g)   \u001B[38;5;66;03m# (B, F_int, H, W)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m152\u001B[39m x1 = \u001B[38;5;28mself\u001B[39m.W_x(x)   \u001B[38;5;66;03m# (B, F_int, H, W)\u001B[39;00m\n\u001B[32m    153\u001B[39m psi = \u001B[38;5;28mself\u001B[39m.relu(g1 + x1)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1857\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1856\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1858\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1859\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1860\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1861\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1805\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1803\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1805\u001B[39m result = forward_call(*args, **kwargs)\n\u001B[32m   1806\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m     \u001B[38;5;28minput\u001B[39m = module(\u001B[38;5;28minput\u001B[39m)\n\u001B[32m    241\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1857\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1856\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1857\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m   1858\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1859\u001B[39m     \u001B[38;5;66;03m# run always called hooks if they have not already been run\u001B[39;00m\n\u001B[32m   1860\u001B[39m     \u001B[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001B[39;00m\n\u001B[32m   1861\u001B[39m     \u001B[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1805\u001B[39m, in \u001B[36mModule._call_impl.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m   1803\u001B[39m     args = bw_hook.setup_input_hook(args)\n\u001B[32m-> \u001B[39m\u001B[32m1805\u001B[39m result = forward_call(*args, **kwargs)\n\u001B[32m   1806\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001B[39m, in \u001B[36mConv2d.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._conv_forward(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m.weight, \u001B[38;5;28mself\u001B[39m.bias)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001B[39m, in \u001B[36mConv2d._conv_forward\u001B[39m\u001B[34m(self, input, weight, bias)\u001B[39m\n\u001B[32m    538\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    539\u001B[39m         F.pad(\n\u001B[32m    540\u001B[39m             \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m._reversed_padding_repeated_twice, mode=\u001B[38;5;28mself\u001B[39m.padding_mode\n\u001B[32m   (...)\u001B[39m\u001B[32m    547\u001B[39m         \u001B[38;5;28mself\u001B[39m.groups,\n\u001B[32m    548\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m F.conv2d(\n\u001B[32m    550\u001B[39m     \u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m.stride, \u001B[38;5;28mself\u001B[39m.padding, \u001B[38;5;28mself\u001B[39m.dilation, \u001B[38;5;28mself\u001B[39m.groups\n\u001B[32m    551\u001B[39m )\n",
      "\u001B[31mRuntimeError\u001B[39m: Given groups=1, weight of size [128, 256, 1, 1], expected input[128, 512, 16, 16] to have 256 channels, but got 512 channels instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     13\u001B[39m model = UNet(\n\u001B[32m     14\u001B[39m         down_filters=down_filters,\n\u001B[32m     15\u001B[39m         down_activations=down_activations,\n\u001B[32m     16\u001B[39m         up_filters=up_filters,\n\u001B[32m     17\u001B[39m         up_activations=up_activations)\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# 3) Ask for a summary on a dummy (1×1×128×128) input:\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m _ = summary(\n\u001B[32m     21\u001B[39m     model,\n\u001B[32m     22\u001B[39m     input_size=(\u001B[32m128\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m128\u001B[39m, \u001B[32m128\u001B[39m),\n\u001B[32m     23\u001B[39m     col_names=[\u001B[33m\"\u001B[39m\u001B[33minput_size\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33moutput_size\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mnum_params\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mtrainable\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     24\u001B[39m     verbose=\u001B[32m1\u001B[39m\n\u001B[32m     25\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torchinfo/torchinfo.py:223\u001B[39m, in \u001B[36msummary\u001B[39m\u001B[34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001B[39m\n\u001B[32m    216\u001B[39m validate_user_params(\n\u001B[32m    217\u001B[39m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001B[32m    218\u001B[39m )\n\u001B[32m    220\u001B[39m x, correct_input_size = process_input(\n\u001B[32m    221\u001B[39m     input_data, input_size, batch_dim, device, dtypes\n\u001B[32m    222\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m summary_list = forward_pass(\n\u001B[32m    224\u001B[39m     model, x, batch_dim, cache_forward_pass, device, model_mode, **kwargs\n\u001B[32m    225\u001B[39m )\n\u001B[32m    226\u001B[39m formatting = FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001B[32m    227\u001B[39m results = ModelStatistics(\n\u001B[32m    228\u001B[39m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001B[32m    229\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torchinfo/torchinfo.py:304\u001B[39m, in \u001B[36mforward_pass\u001B[39m\u001B[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[39m\n\u001B[32m    302\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    303\u001B[39m     executed_layers = [layer \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m summary_list \u001B[38;5;28;01mif\u001B[39;00m layer.executed]\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    305\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFailed to run torchinfo. See above stack traces for more details. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    306\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mExecuted layers up to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexecuted_layers\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    307\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    309\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Identity: 3, Dropout2d: 3, EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, CBAMBlock: 3, ChannelAttention: 4, AdaptiveAvgPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, AdaptiveMaxPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, Sigmoid: 5, SpatialAttention: 4, Conv2d: 5, Sigmoid: 5, Dropout2d: 3, EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, CBAMBlock: 3, ChannelAttention: 4, AdaptiveAvgPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, AdaptiveMaxPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, Sigmoid: 5, SpatialAttention: 4, Conv2d: 5, Sigmoid: 5, Dropout2d: 3, EncoderBlock: 2, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, CBAMBlock: 3, ChannelAttention: 4, AdaptiveAvgPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, AdaptiveMaxPool2d: 5, Conv2d: 5, ReLU: 5, Conv2d: 5, Sigmoid: 5, SpatialAttention: 4, Conv2d: 5, Sigmoid: 5, Dropout2d: 3, BottleneckTransformer: 1, TransformerEncoder: 2, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, TransformerEncoderLayer: 4, LayerNorm: 5, MultiheadAttention: 5, Dropout: 5, LayerNorm: 5, Linear: 5, Dropout: 5, Linear: 5, Dropout: 5, LayerNorm: 2, DecoderBlock: 2, ConvTranspose2d: 3, BatchNorm2d: 3, ReLU: 5, ASPP: 3, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, SepConv: 5, Conv2d: 6, Conv2d: 6, BatchNorm2d: 6, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5, ReLU: 5, Identity: 3, Dropout2d: 3, ConvTranspose2d: 3, BatchNorm2d: 3, ReLU: 5, Sequential: 4, Conv2d: 5, BatchNorm2d: 5]"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8575a99ad9acad56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
