{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-10T08:07:42.818851Z",
     "start_time": "2025-06-10T08:07:42.340022Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sklearn"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T08:08:05.133362Z",
     "start_time": "2025-06-10T08:08:05.121859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def activation_parser(activation_str):\n",
    "    \"\"\"\n",
    "    Parse a string to return the corresponding activation function.\n",
    "    Supported strings: 'relu', 'sigmoid', 'tanh', 'leaky_relu'.\n",
    "    \"\"\"\n",
    "    if activation_str.lower() == 'relu':\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif activation_str.lower() == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation_str.lower() == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation_str.lower() == 'leaky_relu':\n",
    "        return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // ratio, kernel_size=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(in_channels // ratio, in_channels, kernel_size=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='linear')\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)        # (B, C, 1, 1)\n",
    "        avg_out = self.fc1(avg_out)       # (B, C//ratio, 1, 1)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.fc2(avg_out)       # (B, C, 1, 1)\n",
    "\n",
    "        max_out = self.max_pool(x)        # (B, C, 1, 1)\n",
    "        max_out = self.fc1(max_out)       # (B, C//ratio, 1, 1)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.fc2(max_out)       # (B, C, 1, 1)\n",
    "\n",
    "        out = avg_out + max_out           # (B, C, 1, 1)\n",
    "        scale = self.sigmoid(out)         # (B, C, 1, 1)\n",
    "        return x * scale                  # broadcast along H, W\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 7)\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        nn.init.kaiming_normal_(self.conv.weight, mode='fan_in', nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)     # (B, 1, H, W)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)   # (B, 1, H, W)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)    # (B, 2, H, W)\n",
    "        attn = self.conv(concat)                         # (B, 1, H, W)\n",
    "        attn = self.sigmoid(attn)\n",
    "        return x * attn                                  # broadcast across C\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_channels, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        x = self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, kernel_size, padding, dilation=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_ch, in_ch, kernel_size=kernel_size,\n",
    "            padding=padding, dilation=dilation,\n",
    "            groups=in_ch, bias=True\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=True)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act = activation_parser(activation)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.depthwise.weight, mode='fan_out', nonlinearity=activation)\n",
    "        nn.init.kaiming_normal_(self.pointwise.weight, mode='fan_out', nonlinearity=activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.norm(x))\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        dilations = [1, 2, 3, 4]\n",
    "        kernels   = [1, 3, 5, 7]\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d, k in zip(dilations, kernels):\n",
    "            pad = (k // 2) * d\n",
    "            self.branches.append(\n",
    "                SepConv(in_ch, out_ch, activation, kernel_size=k, padding=pad, dilation=d)\n",
    "            )\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(dilations) * out_ch, out_ch, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation)\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.merge[0].weight, mode='fan_out', nonlinearity=activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [branch(x) for branch in self.branches]\n",
    "        x = torch.cat(outs, dim=1)\n",
    "        return self.merge(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation,\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation)\n",
    "        )\n",
    "        for m in self.block.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        # W_g projects gating signal\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # W_x projects skip connection\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # psi computes 1‐channel attention map\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.W_g[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.W_x[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.psi[0].weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        \"\"\"\n",
    "        g: gating signal from decoder, shape (B, F_g, H, W)\n",
    "        x: skip connection from encoder, shape (B, F_l, H, W)\n",
    "        \"\"\"\n",
    "        g1 = self.W_g(g)   # (B, F_int, H, W)\n",
    "        x1 = self.W_x(x)   # (B, F_int, H, W)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)   # (B, 1, H, W)\n",
    "        return x * psi        # broadcast along channel\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, dropout_prob=0.0, attention=True, pool=True):\n",
    "        super().__init__()\n",
    "        #self.double_conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.aspp        = ASPP(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "        self.pool        = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.aspp(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = x.clone()\n",
    "        if self.pool:\n",
    "            x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, activation, dropout_prob=0.0, attention=True, upsample=True):\n",
    "        \"\"\"\n",
    "        in_ch:   channels from previous layer (bottleneck or previous decoder)\n",
    "        skip_ch: channels in the corresponding encoder skip\n",
    "        out_ch:  desired output channels for this decoder block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.skip_ch = skip_ch\n",
    "\n",
    "        if self.upsample:\n",
    "            # ConvTranspose2d(in_ch → skip_ch) to match spatial & channel dims\n",
    "            self.up = nn.ConvTranspose2d(in_ch, skip_ch, kernel_size=3,\n",
    "                                         stride=2, padding=1, output_padding=1, bias=True)\n",
    "            nn.init.kaiming_normal_(self.up.weight, mode='fan_out', nonlinearity='relu')\n",
    "            self.bn_up = nn.BatchNorm2d(skip_ch)\n",
    "            self.act_up = activation\n",
    "            self.attention = AttentionGate(F_g=skip_ch, F_l=skip_ch, F_int=skip_ch // 2) if attention else nn.Identity()\n",
    "            in_double = skip_ch * 2\n",
    "        else:\n",
    "            self.up = None\n",
    "            self.bn_up = None\n",
    "            self.act_up = None\n",
    "            self.attention = AttentionGate(F_g=in_ch, F_l=in_ch, F_int=in_ch // 2) if attention else nn.Identity()\n",
    "            in_double = in_ch * 2 if attention else in_ch\n",
    "\n",
    "        #self.double_conv = DoubleConv(in_double, out_ch, activation)\n",
    "        self.aspp        = ASPP(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        if self.upsample:\n",
    "            x = self.up(x)       # (B, skip_ch, H*2, W*2)\n",
    "            x = self.bn_up(x)\n",
    "            x = self.act_up(x)\n",
    "        if skip is not None and not isinstance(self.attention, nn.Identity):\n",
    "            skip = self.attention(g=x, x=skip)\n",
    "            x = torch.cat([x, skip], dim=1)  # (B, 2*skip_ch, H*2, W*2)\n",
    "        x = self.aspp(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleneckTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a tensor of shape (B, C, H, W), flattens the H×W patches into tokens,\n",
    "    runs a small TransformerEncoder over them, then reshapes back to (B, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=8, depth=3, mlp_dim=None):\n",
    "        super().__init__()\n",
    "        mlp_dim = mlp_dim or dim * 4\n",
    "        # one TransformerEncoderLayer (or more, if depth>1)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            activation='relu',\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=depth)\n",
    "        self.norm    = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # flatten spatial dims:\n",
    "        # → (B, C, H*W) then permute to (H*W, B, C) for PyTorch’s MHSA\n",
    "        tokens = x.flatten(2).permute(2, 0, 1)   # (H*W, B, C)\n",
    "        # run through TransformerEncoder\n",
    "        out   = self.encoder(tokens)             # (H*W, B, C)\n",
    "        # put back into (B, C, H, W) after a LayerNorm on each token\n",
    "        out   = out.permute(1, 2, 0).view(B, C, H, W)\n",
    "        return self.norm(out.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        # explanation of the two permutes:\n",
    "        #  - out.permute(1,2,0)→(B, C, H*W) then .view(B, C, H, W)\n",
    "        #  - we want LN over the C‐dimension, so we permute to (B, H, W, C), apply LayerNorm,\n",
    "        #    then back to (B, C, H, W).\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 out_channels=1,\n",
    "                 down_filters=None,\n",
    "                 down_activations=None,\n",
    "                 up_filters=None,\n",
    "                 up_activations=None):\n",
    "        super().__init__()\n",
    "        assert len(down_filters) == len(down_activations)\n",
    "        assert len(up_filters)   == len(up_activations)\n",
    "\n",
    "        # Build Encoder path\n",
    "        self.encoders = nn.ModuleList()\n",
    "        prev_ch = in_channels\n",
    "        for i, out_ch in enumerate(down_filters):\n",
    "            act_str = down_activations[i].lower()\n",
    "            if act_str == 'relu':\n",
    "                act_fn = nn.ReLU(inplace=True)\n",
    "            elif act_str == 'sigmoid':\n",
    "                act_fn = nn.Sigmoid()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported encoder activation: {act_str}\")\n",
    "\n",
    "            self.encoders.append(\n",
    "                EncoderBlock(in_ch=prev_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_fn,\n",
    "                             dropout_prob=0.1,\n",
    "                             attention=(i != 0),\n",
    "                             pool=True)\n",
    "            )\n",
    "            prev_ch = out_ch\n",
    "\n",
    "        # Bottleneck: DoubleConv(down_filters[-1] → down_filters[-1]*2)\n",
    "        #self.bottleneck = DoubleConv(down_filters[-1], down_filters[-1]*2, nn.ReLU(inplace=True))\n",
    "        self.bottleneck_aspp   = ASPP(down_filters[-1], down_filters[-1]*2, nn.ReLU(inplace=True))\n",
    "        self.bottleneck_trans  = BottleneckTransformer(dim=down_filters[-1]*2,\n",
    "                                                 heads=4,\n",
    "                                                 depth=4,\n",
    "                                                 mlp_dim=down_filters[-1] * 4\n",
    "                                                 )\n",
    "\n",
    "        # Build Decoder path\n",
    "        self.decoders = nn.ModuleList()\n",
    "        N = len(down_filters)\n",
    "        for i, out_ch in enumerate(up_filters):\n",
    "            act_str = up_activations[i].lower()\n",
    "            if act_str == 'relu':\n",
    "                act_fn = nn.ReLU(inplace=True)\n",
    "            elif act_str == 'sigmoid':\n",
    "                act_fn = nn.Sigmoid()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported decoder activation: {act_str}\")\n",
    "            # Corresponding skip channels from encoder\n",
    "            skip_ch = down_filters[N - 1 - i]\n",
    "            # Input channels for this decoder block\n",
    "            in_ch_dec = (down_filters[-1] * 2) if (i == 0) else up_filters[i - 1]\n",
    "\n",
    "            self.decoders.append(\n",
    "                DecoderBlock(in_ch=in_ch_dec,\n",
    "                             skip_ch=skip_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_fn,\n",
    "                             dropout_prob=0.1,\n",
    "                             attention=True,\n",
    "                             upsample=True)\n",
    "            )\n",
    "\n",
    "        # Final 3×3 conv + Sigmoid → 1 channel\n",
    "        self.final_conv  = ASPP(up_filters[-1], out_channels, nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 128, 128)\n",
    "        skips = []\n",
    "        for enc in self.encoders:\n",
    "            x, skip = enc(x)\n",
    "            skips.append(skip)\n",
    "\n",
    "        x = self.bottleneck_aspp(x)\n",
    "        x = self.bottleneck_trans(x)\n",
    "        skips = skips[::-1]              # reverse order for decoding\n",
    "\n",
    "        for i, dec in enumerate(self.decoders):\n",
    "            skip_feat = skips[i]\n",
    "            x = dec(x, skip_feat)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ],
   "id": "a6c5a18501dda368",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T08:08:57.700168Z",
     "start_time": "2025-06-10T08:08:57.411578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) pip install torchinfo\n",
    "#    (if you haven’t already)\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "down_filters     = [32, 64, 128, 256, 512]\n",
    "down_activations = ['relu'] * len(down_filters)\n",
    "\n",
    "up_filters       = [512, 256, 128]#down_filters[::-1]  # reverse the down_filters\n",
    "up_activations   = ['relu'] * len(up_filters)\n",
    "\n",
    "# 2) Re‐instantiate your UNet exactly as in your training code:\n",
    "model = UNet(\n",
    "        down_filters=down_filters,\n",
    "        down_activations=down_activations,\n",
    "        up_filters=up_filters,\n",
    "        up_activations=up_activations)\n",
    "\n",
    "# 3) Ask for a summary on a dummy (1×1×128×128) input:\n",
    "_ = summary(\n",
    "    model,\n",
    "    input_size=(128, 1, 128, 128),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "9b46da360bcba26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================================================================================================\n",
      "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #                   Trainable\n",
      "===========================================================================================================================================================\n",
      "UNet                                                    [128, 1, 128, 128]        [128, 1, 32, 32]          --                        True\n",
      "├─ModuleList: 1-1                                       --                        --                        --                        True\n",
      "│    └─EncoderBlock: 2-1                                [128, 1, 128, 128]        [128, 32, 64, 64]         --                        True\n",
      "│    │    └─ASPP: 3-1                                   [128, 1, 128, 128]        [128, 32, 128, 128]       4,792                     True\n",
      "│    │    └─Identity: 3-2                               [128, 32, 128, 128]       [128, 32, 128, 128]       --                        --\n",
      "│    │    └─Dropout2d: 3-3                              [128, 32, 128, 128]       [128, 32, 128, 128]       --                        --\n",
      "│    └─EncoderBlock: 2-2                                [128, 32, 64, 64]         [128, 64, 32, 32]         --                        True\n",
      "│    │    └─ASPP: 3-4                                   [128, 32, 64, 64]         [128, 64, 64, 64]         28,352                    True\n",
      "│    │    └─CBAMBlock: 3-5                              [128, 64, 64, 64]         [128, 64, 64, 64]         1,195                     True\n",
      "│    │    └─Dropout2d: 3-6                              [128, 64, 64, 64]         [128, 64, 64, 64]         --                        --\n",
      "│    └─EncoderBlock: 2-3                                [128, 64, 32, 32]         [128, 128, 16, 16]        --                        True\n",
      "│    │    └─ASPP: 3-7                                   [128, 64, 32, 32]         [128, 128, 32, 32]        105,856                   True\n",
      "│    │    └─CBAMBlock: 3-8                              [128, 128, 32, 32]        [128, 128, 32, 32]        4,339                     True\n",
      "│    │    └─Dropout2d: 3-9                              [128, 128, 32, 32]        [128, 128, 32, 32]        --                        --\n",
      "│    └─EncoderBlock: 2-4                                [128, 128, 16, 16]        [128, 256, 8, 8]          --                        True\n",
      "│    │    └─ASPP: 3-10                                  [128, 128, 16, 16]        [128, 256, 16, 16]        408,320                   True\n",
      "│    │    └─CBAMBlock: 3-11                             [128, 256, 16, 16]        [128, 256, 16, 16]        16,771                    True\n",
      "│    │    └─Dropout2d: 3-12                             [128, 256, 16, 16]        [128, 256, 16, 16]        --                        --\n",
      "│    └─EncoderBlock: 2-5                                [128, 256, 8, 8]          [128, 512, 4, 4]          --                        True\n",
      "│    │    └─ASPP: 3-13                                  [128, 256, 8, 8]          [128, 512, 8, 8]          1,603,072                 True\n",
      "│    │    └─CBAMBlock: 3-14                             [128, 512, 8, 8]          [128, 512, 8, 8]          66,211                    True\n",
      "│    │    └─Dropout2d: 3-15                             [128, 512, 8, 8]          [128, 512, 8, 8]          --                        --\n",
      "├─ASPP: 1-2                                             [128, 512, 4, 4]          [128, 1024, 4, 4]         --                        True\n",
      "│    └─ModuleList: 2-12                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-16                               [128, 512, 4, 4]          [128, 1024, 4, 4]         528,384                   True\n",
      "│    └─Sequential: 2-13                                 --                        --                        (recursive)               True\n",
      "│    │    └─ReLU: 3-17                                  [128, 1024, 4, 4]         [128, 1024, 4, 4]         --                        --\n",
      "│    └─ModuleList: 2-12                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-18                               [128, 512, 4, 4]          [128, 1024, 4, 4]         532,480                   True\n",
      "│    └─Sequential: 2-13                                 --                        --                        (recursive)               True\n",
      "│    │    └─ReLU: 3-19                                  [128, 1024, 4, 4]         [128, 1024, 4, 4]         --                        --\n",
      "│    └─ModuleList: 2-12                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-20                               [128, 512, 4, 4]          [128, 1024, 4, 4]         540,672                   True\n",
      "│    └─Sequential: 2-13                                 --                        --                        (recursive)               True\n",
      "│    │    └─ReLU: 3-21                                  [128, 1024, 4, 4]         [128, 1024, 4, 4]         --                        --\n",
      "│    └─ModuleList: 2-12                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-22                               [128, 512, 4, 4]          [128, 1024, 4, 4]         552,960                   True\n",
      "│    └─Sequential: 2-13                                 --                        --                        (recursive)               True\n",
      "│    │    └─ReLU: 3-23                                  [128, 1024, 4, 4]         [128, 1024, 4, 4]         --                        --\n",
      "│    └─Sequential: 2-14                                 [128, 4096, 4, 4]         [128, 1024, 4, 4]         --                        True\n",
      "│    │    └─Conv2d: 3-24                                [128, 4096, 4, 4]         [128, 1024, 4, 4]         4,195,328                 True\n",
      "│    │    └─BatchNorm2d: 3-25                           [128, 1024, 4, 4]         [128, 1024, 4, 4]         2,048                     True\n",
      "│    │    └─ReLU: 3-26                                  [128, 1024, 4, 4]         [128, 1024, 4, 4]         --                        --\n",
      "├─BottleneckTransformer: 1-3                            [128, 1024, 4, 4]         [128, 1024, 4, 4]         --                        True\n",
      "│    └─TransformerEncoder: 2-15                         [16, 128, 1024]           [16, 128, 1024]           --                        True\n",
      "│    │    └─ModuleList: 3-27                            --                        --                        33,599,488                True\n",
      "│    └─LayerNorm: 2-16                                  [128, 4, 4, 1024]         [128, 4, 4, 1024]         2,048                     True\n",
      "├─ModuleList: 1-4                                       --                        --                        --                        True\n",
      "│    └─DecoderBlock: 2-17                               [128, 1024, 4, 4]         [128, 512, 8, 8]          --                        True\n",
      "│    │    └─ConvTranspose2d: 3-28                       [128, 1024, 4, 4]         [128, 512, 8, 8]          4,719,104                 True\n",
      "│    │    └─BatchNorm2d: 3-29                           [128, 512, 8, 8]          [128, 512, 8, 8]          1,024                     True\n",
      "│    │    └─ASPP: 3-30                                  --                        --                        (recursive)               True\n",
      "│    │    └─AttentionGate: 3-31                         --                        [128, 512, 8, 8]          263,937                   True\n",
      "│    │    └─ASPP: 3-32                                  [128, 1024, 8, 8]         [128, 512, 8, 8]          3,243,520                 True\n",
      "│    │    └─CBAMBlock: 3-33                             [128, 512, 8, 8]          [128, 512, 8, 8]          66,211                    True\n",
      "│    │    └─Dropout2d: 3-34                             [128, 512, 8, 8]          [128, 512, 8, 8]          --                        --\n",
      "│    └─DecoderBlock: 2-18                               [128, 512, 8, 8]          [128, 256, 16, 16]        --                        True\n",
      "│    │    └─ConvTranspose2d: 3-35                       [128, 512, 8, 8]          [128, 256, 16, 16]        1,179,904                 True\n",
      "│    │    └─BatchNorm2d: 3-36                           [128, 256, 16, 16]        [128, 256, 16, 16]        512                       True\n",
      "│    │    └─ASPP: 3-37                                  --                        --                        (recursive)               True\n",
      "│    │    └─AttentionGate: 3-38                         --                        [128, 256, 16, 16]        66,433                    True\n",
      "│    │    └─ASPP: 3-39                                  [128, 512, 16, 16]        [128, 256, 16, 16]        835,328                   True\n",
      "│    │    └─CBAMBlock: 3-40                             [128, 256, 16, 16]        [128, 256, 16, 16]        16,771                    True\n",
      "│    │    └─Dropout2d: 3-41                             [128, 256, 16, 16]        [128, 256, 16, 16]        --                        --\n",
      "│    └─DecoderBlock: 2-19                               [128, 256, 16, 16]        [128, 128, 32, 32]        --                        True\n",
      "│    │    └─ConvTranspose2d: 3-42                       [128, 256, 16, 16]        [128, 128, 32, 32]        295,040                   True\n",
      "│    │    └─BatchNorm2d: 3-43                           [128, 128, 32, 32]        [128, 128, 32, 32]        256                       True\n",
      "│    │    └─ASPP: 3-44                                  --                        --                        (recursive)               True\n",
      "│    │    └─AttentionGate: 3-45                         --                        [128, 128, 32, 32]        16,833                    True\n",
      "│    │    └─ASPP: 3-46                                  [128, 256, 32, 32]        [128, 128, 32, 32]        221,056                   True\n",
      "│    │    └─CBAMBlock: 3-47                             [128, 128, 32, 32]        [128, 128, 32, 32]        4,339                     True\n",
      "│    │    └─Dropout2d: 3-48                             [128, 128, 32, 32]        [128, 128, 32, 32]        --                        --\n",
      "├─ASPP: 1-5                                             [128, 128, 32, 32]        [128, 1, 32, 32]          --                        True\n",
      "│    └─ModuleList: 2-26                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-49                               [128, 128, 32, 32]        [128, 1, 32, 32]          387                       True\n",
      "│    └─Sequential: 2-27                                 --                        --                        (recursive)               True\n",
      "│    │    └─Sigmoid: 3-50                               [128, 1, 32, 32]          [128, 1, 32, 32]          --                        --\n",
      "│    └─ModuleList: 2-26                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-51                               [128, 128, 32, 32]        [128, 1, 32, 32]          1,411                     True\n",
      "│    └─Sequential: 2-27                                 --                        --                        (recursive)               True\n",
      "│    │    └─Sigmoid: 3-52                               [128, 1, 32, 32]          [128, 1, 32, 32]          --                        --\n",
      "│    └─ModuleList: 2-26                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-53                               [128, 128, 32, 32]        [128, 1, 32, 32]          3,459                     True\n",
      "│    └─Sequential: 2-27                                 --                        --                        (recursive)               True\n",
      "│    │    └─Sigmoid: 3-54                               [128, 1, 32, 32]          [128, 1, 32, 32]          --                        --\n",
      "│    └─ModuleList: 2-26                                 --                        --                        (recursive)               True\n",
      "│    │    └─SepConv: 3-55                               [128, 128, 32, 32]        [128, 1, 32, 32]          6,531                     True\n",
      "│    └─Sequential: 2-27                                 --                        --                        (recursive)               True\n",
      "│    │    └─Sigmoid: 3-56                               [128, 1, 32, 32]          [128, 1, 32, 32]          --                        --\n",
      "│    └─Sequential: 2-28                                 [128, 4, 32, 32]          [128, 1, 32, 32]          --                        True\n",
      "│    │    └─Conv2d: 3-57                                [128, 4, 32, 32]          [128, 1, 32, 32]          5                         True\n",
      "│    │    └─BatchNorm2d: 3-58                           [128, 1, 32, 32]          [128, 1, 32, 32]          2                         True\n",
      "│    │    └─Sigmoid: 3-59                               [128, 1, 32, 32]          [128, 1, 32, 32]          --                        --\n",
      "===========================================================================================================================================================\n",
      "Total params: 53,134,379\n",
      "Trainable params: 53,134,379\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 284.04\n",
      "===========================================================================================================================================================\n",
      "Input size (MB): 8.39\n",
      "Forward/backward pass size (MB): 17756.60\n",
      "Params size (MB): 145.36\n",
      "Estimated Total Size (MB): 17910.35\n",
      "===========================================================================================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8575a99ad9acad56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
