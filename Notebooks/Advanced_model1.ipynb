{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f73eb8d-539b-4945-a393-14ab30e5f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- core ---\n",
    "import os, gc, time, math, random\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.ndimage import label as cc_label, find_objects, binary_opening\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def set_seed(s=1337):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(1337)\n",
    "\n",
    "# helper: resize masks to model output size (NN/metrics use nearest to preserve {0,1})\n",
    "def resize_masks_to(pred_like, masks, mode='nearest'):\n",
    "    H, W = pred_like.shape[-2:]\n",
    "    return F.interpolate(masks.float(), size=(H, W), mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51770794-e28c-48fe-99bc-f06e06a1a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def robust_stats_mad(arr):\n",
    "    med = np.median(arr)\n",
    "    mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)\n",
    "    return np.float32(med), np.float32(sigma if np.isfinite(mad) and mad>0 else 1.0)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Streams 128x128 (default) tiles from large (H,W) images in an HDF5.\n",
    "    Per-image robust standardization + k-sigma clipping.\n",
    "    Pads edge tiles to full tile size.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, crop_for_stats=512):\n",
    "        self.h5_path = h5_path\n",
    "        self.tile    = int(tile)\n",
    "        self.k_sigma = float(k_sigma)\n",
    "        self.crop_for_stats = int(crop_for_stats)\n",
    "        self._h5 = None\n",
    "        self._x = None; self._y = None\n",
    "        self._stats_cache = {}   # image_id -> (med, sigma)\n",
    "\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "\n",
    "        Hb = math.ceil(self.H / self.tile)\n",
    "        Wb = math.ceil(self.W / self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "\n",
    "    def _ensure_open(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self._x = self._h5[\"images\"]; self._y = self._h5[\"masks\"]\n",
    "\n",
    "    def _image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        H, W = self.H, self.W\n",
    "        s = min(self.crop_for_stats, H, W)\n",
    "        h0 = (H - s)//2; w0 = (W - s)//2\n",
    "        crop = self._x[i, h0:h0+s, w0:w0+s].astype(\"float32\")\n",
    "        med, sig = robust_stats_mad(crop)\n",
    "        self._stats_cache[i] = (med, sig)\n",
    "        return med, sig\n",
    "\n",
    "    def __len__(self): return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure_open()\n",
    "        i, r, c = self.indices[idx]\n",
    "        t = self.tile\n",
    "        r0, c0 = r*t, c*t\n",
    "        r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "\n",
    "        x = self._x[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        y = self._y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "\n",
    "        if x.shape != (t, t):\n",
    "            xp = np.zeros((t, t), np.float32); yp = np.zeros((t, t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x;  yp[:y.shape[0], :y.shape[1]] = y\n",
    "            x, y = xp, yp\n",
    "\n",
    "        med, sig = self._image_stats(i)\n",
    "        x = (x - med) / sig\n",
    "        x = np.clip(x, -5.0, 5.0)  # k-sigma clip\n",
    "\n",
    "        # add channel dim\n",
    "        return torch.from_numpy(x[None, ...]), torch.from_numpy(y[None, ...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e4b4d8-02da-4a97-b6a2-e8db0de48532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< set your paths here >>>\n",
    "train_h5 = \"/home/karlo/train_chunked.h5\"\n",
    "test_h5  = \"../DATA/test.h5\"\n",
    "tile = 128\n",
    "batch_size = 128\n",
    "num_workers = 0  # safer on shared systems\n",
    "\n",
    "# Split train into train/val by index\n",
    "with h5py.File(train_h5, \"r\") as f:\n",
    "    N = f[\"images\"].shape[0]\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.9 * N)  # 90/10 split\n",
    "idx_tr, idx_va = np.sort(idx[:split]), np.sort(idx[split:])\n",
    "\n",
    "# Datasets\n",
    "ds_full = H5TiledDataset(train_h5, tile=tile, k_sigma=5.0)\n",
    "class SubsetDS(Dataset):\n",
    "    def __init__(self, base, panel_indices):\n",
    "        self.base = base; self.panel_indices = panel_indices\n",
    "        # remap base.indices to only those panels\n",
    "        t = base.tile; Hb = math.ceil(base.H / t); Wb = math.ceil(base.W / t)\n",
    "        self.map = []\n",
    "        base_map = {(i,r,c):k for k,(i,r,c) in enumerate(base.indices)}\n",
    "        for i in panel_indices:\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    self.map.append(base_map[(i,r,c)])\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, k): return self.base[self.map[k]]\n",
    "\n",
    "train_ds = SubsetDS(ds_full, idx_tr)\n",
    "val_ds   = SubsetDS(ds_full, idx_va)\n",
    "test_ds  = H5TiledDataset(test_h5, tile=tile, k_sigma=5.0)\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=(device.type=='cuda'))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=(device.type=='cuda'))\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=num_workers, pin_memory=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ad2acf-c834-4b41-9b23-18055652486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=8):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Conv2d(c, c//r, 1)\n",
    "        self.fc2 = nn.Conv2d(c//r, c, 1)\n",
    "    def forward(self, x):\n",
    "        s = F.adaptive_avg_pool2d(x,1)\n",
    "        s = F.silu(self.fc1(s))\n",
    "        s = torch.sigmoid(self.fc2(s))\n",
    "        return x * s\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, se=True):\n",
    "        super().__init__()\n",
    "        p = k//2\n",
    "        self.proj = nn.Identity() if c_in==c_out else nn.Conv2d(c_in, c_out, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(c_in)\n",
    "        self.c1  = nn.Conv2d(c_in, c_out, k, padding=p, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(c_out)\n",
    "        self.c2  = nn.Conv2d(c_out, c_out, k, padding=p, bias=False)\n",
    "        self.se  = SEBlock(c_out) if se else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        h = F.silu(self.bn1(x))\n",
    "        h = self.c1(h)\n",
    "        h = F.silu(self.bn2(h))\n",
    "        h = self.c2(h)\n",
    "        h = self.se(h)\n",
    "        return h + self.proj(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.rb   = ResBlock(c_in, c_out)\n",
    "    def forward(self, x): return self.rb(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, c_in, c_skip, c_out):\n",
    "        super().__init__()\n",
    "        self.up  = nn.ConvTranspose2d(c_in, c_in, 2, stride=2)\n",
    "        self.rb1 = ResBlock(c_in + c_skip, c_out)\n",
    "        self.rb2 = ResBlock(c_out, c_out)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # pad if odd sizes\n",
    "        dh = skip.size(-2) - x.size(-2); dw = skip.size(-1) - x.size(-1)\n",
    "        if dh or dw: x = F.pad(x, (0,max(0,dw),0,max(0,dh)))\n",
    "        x = torch.cat([x, skip], 1)\n",
    "        x = self.rb1(x); x = self.rb2(x)\n",
    "        return x\n",
    "\n",
    "class UNetResSE(nn.Module):\n",
    "    def __init__(self, in_ch=1, out_ch=1, widths=(32,64,128,256,512)):\n",
    "        super().__init__()\n",
    "        w = widths\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, w[0], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(w[0]), nn.SiLU(inplace=True),\n",
    "            ResBlock(w[0], w[0])\n",
    "        )\n",
    "        self.d1 = Down(w[0], w[1])\n",
    "        self.d2 = Down(w[1], w[2])\n",
    "        self.d3 = Down(w[2], w[3])\n",
    "        self.d4 = Down(w[3], w[4])\n",
    "        self.u1 = Up(w[4], w[3], w[3])\n",
    "        self.u2 = Up(w[3], w[2], w[2])\n",
    "        self.u3 = Up(w[2], w[1], w[1])\n",
    "        self.u4 = Up(w[1], w[0], w[0])\n",
    "        self.head = nn.Conv2d(w[0], out_ch, 1)  # logits\n",
    "    def forward(self, x):\n",
    "        s0 = self.stem(x)\n",
    "        s1 = self.d1(s0)\n",
    "        s2 = self.d2(s1)\n",
    "        s3 = self.d3(s2)\n",
    "        b  = self.d4(s3)\n",
    "        x  = self.u1(b, s3)\n",
    "        x  = self.u2(x, s2)\n",
    "        x  = self.u3(x, s1)\n",
    "        x  = self.u4(x, s0)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca911ee9-5008-4b61-bde4-e44746e33a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6): super().__init__(); self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        p = p.view(p.size(0), -1); t = targets.view(targets.size(0), -1)\n",
    "        inter = (p*t).sum(1)\n",
    "        denom = p.sum(1) + t.sum(1)\n",
    "        dice  = (2*inter + self.eps)/(denom + self.eps)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class BCEDice(nn.Module):\n",
    "    def __init__(self, pos_weight=None, dice_weight=1.0, bce_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.dice= DiceLoss()\n",
    "        self.dw, self.bw = dice_weight, bce_weight\n",
    "    def forward(self, logits, targets):\n",
    "        return self.bw*self.bce(logits, targets) + self.dw*self.dice(logits, targets)\n",
    "\n",
    "def make_optimizer(model, lr=1.5e-4, wd=1e-4):\n",
    "    return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "def make_scheduler(opt, steps_per_epoch, epochs, max_lr):\n",
    "    return torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=max_lr,\n",
    "                                               steps_per_epoch=steps_per_epoch,\n",
    "                                               epochs=epochs, pct_start=0.1,\n",
    "                                               anneal_strategy='cos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd4eaa1-6625-4960-9970-ef8d83ea3648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8213/1612307755.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "def _auc_from_hists(pos_hist, neg_hist):\n",
    "    tp = np.cumsum(pos_hist[::-1]); fp = np.cumsum(neg_hist[::-1])\n",
    "    if tp[-1]==0 or fp[-1]==0: return float('nan')\n",
    "    tpr = tp/tp[-1]; fpr = fp/fp[-1]\n",
    "    tpr = np.concatenate(([0.0], tpr)); fpr = np.concatenate(([0.0], fpr))\n",
    "    return np.trapz(tpr, fpr)\n",
    "\n",
    "def run_epoch(loader, model, criterion, optimizer=None, train=True,\n",
    "              tag=\"Train\", print_every=10, n_bins=512, grad_clip=1.0):\n",
    "    model.train(train)\n",
    "    start = time.time()\n",
    "    total = len(loader.dataset)\n",
    "    seen = 0; running_loss=0.0; tp=fp=fn=0.0\n",
    "    pos_hist = np.zeros(n_bins, np.float64)\n",
    "    neg_hist = np.zeros(n_bins, np.float64)\n",
    "\n",
    "    for b, (xb, yb) in enumerate(loader, start=1):\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
    "            logits = model(xb)                     # logits\n",
    "            yb_r   = resize_masks_to(logits, yb)   # nearest for metrics/targets\n",
    "            loss   = criterion(logits, yb_r)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        seen += bs\n",
    "        running_loss += float(loss.item()) * bs\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p = torch.sigmoid(logits).detach().view(-1).cpu()\n",
    "            t = yb_r.detach().view(-1).cpu()\n",
    "            p_bin = (p >= 0.5).float()\n",
    "            tp += float((p_bin*t).sum())\n",
    "            fp += float((p_bin*(1-t)).sum())\n",
    "            fn += float(((1-p_bin)*t).sum())\n",
    "            # streaming AUC\n",
    "            idx = torch.clamp((p*(n_bins-1)).long(), 0, n_bins-1)\n",
    "            pos_hist += np.bincount(idx[t>0.5].numpy(), minlength=n_bins)\n",
    "            neg_hist += np.bincount(idx[t<=0.5].numpy(), minlength=n_bins)\n",
    "\n",
    "        if ((b % print_every == 0) or (seen == total)) and train:\n",
    "            prec = tp / (tp + fp + 1e-8)\n",
    "            rec  = tp / (tp + fn + 1e-8)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            avg_loss = running_loss / seen\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                f\"\\r[{tag}] batch {b}/{len(loader)} | {seen}/{total} ex | \"\n",
    "                f\"loss={avg_loss:.4f} | F1 {f1:.4f} | P {prec:.4f} | R {rec:.4f} | {elapsed:.1f}s\",\n",
    "                end='', flush=True\n",
    "            )\n",
    "    if train: print()\n",
    "    auc = _auc_from_hists(pos_hist, neg_hist)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    rec  = tp / (tp + fn + 1e-8)\n",
    "    f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    epoch_loss = running_loss / total\n",
    "    return epoch_loss, auc, prec, rec, f1\n",
    "\n",
    "def fit(model, train_loader, val_loader, criterion,\n",
    "        epochs=20, max_lr=1.5e-4, early_stop_patience=10,\n",
    "        save_path=\"./best_unet_resse.pt\"):\n",
    "    model = model.to(device)\n",
    "    opt   = make_optimizer(model, lr=max_lr, wd=1e-4)\n",
    "    sched = make_scheduler(opt, steps_per_epoch=len(train_loader), epochs=epochs, max_lr=max_lr)\n",
    "    best_f1, no_improve = -1, 0\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0=time.time()\n",
    "        tr_loss, _, tr_p, tr_r, tr_f1 = run_epoch(train_loader, model, criterion, opt, train=True,  tag=\"Train\", print_every=10)\n",
    "        va_loss, va_auc, va_p, va_r, va_f1 = run_epoch(val_loader,   model, criterion, None,      train=False, tag=\"Val\")\n",
    "        sched.step()\n",
    "\n",
    "        print(f\"Epoch {ep:03d} | \"\n",
    "              f\"Train L {tr_loss:.4f} F1 {tr_f1:.4f} P {tr_p:.4f} R {tr_r:.4f} || \"\n",
    "              f\"Val L {va_loss:.4f} AUC {va_auc:.4f} F1 {va_f1:.4f} P {va_p:.4f} R {va_r:.4f} | \"\n",
    "              f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "        if va_f1 > best_f1 + 1e-4:\n",
    "            best_f1, no_improve = va_f1, 0\n",
    "            torch.save({\"state_dict\": model.state_dict()}, save_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stop_patience:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc37c5b-f0e5-43e9-8416-f9e42d69d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8213/1612307755.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] batch 4520/5760 | 578560/737280 ex | loss=1.0560 | F1 0.0675 | P 0.0594 | R 0.0783 | 912.1s"
     ]
    }
   ],
   "source": [
    "model = UNetResSE(in_ch=1, out_ch=1, widths=(32,64,128,256,512))\n",
    "criterion = BCEDice(pos_weight=None, dice_weight=1.0, bce_weight=1.0)\n",
    "_ = fit(model, train_loader, val_loader, criterion,\n",
    "        epochs=20, max_lr=1.5e-4, early_stop_patience=10,\n",
    "        save_path=\"./best_unet_resse.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490d994-afae-40ba-9429-80ab224a14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_tiles_to_full(h5_path, loader, model, tile=128):\n",
    "    model.eval(); model.to(device)\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        N,H,W = f['images'].shape\n",
    "    Hb, Wb = math.ceil(H/tile), math.ceil(W/tile)\n",
    "    tiles_per_panel = Hb*Wb\n",
    "    full_preds = np.zeros((N,H,W), np.float32)\n",
    "    ptr=0; buf=[]\n",
    "\n",
    "    for xb,_ in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()[:,0]\n",
    "        buf.append(probs)\n",
    "\n",
    "        # flush complete panels\n",
    "        while len(buf)>0:\n",
    "            cur = buf[0]\n",
    "            if cur.shape[0] < tiles_per_panel: break\n",
    "            tile_buf = cur[:tiles_per_panel]\n",
    "            buf[0] = cur[tiles_per_panel:]\n",
    "            if buf[0].shape[0]==0: buf.pop(0)\n",
    "\n",
    "            p = ptr; ptr += 1\n",
    "            panel = np.zeros((Hb*tile, Wb*tile), np.float32)\n",
    "            t=0\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0, c0 = r*tile, c*tile\n",
    "                    panel[r0:r0+tile, c0:c0+tile] = tile_buf[t]; t+=1\n",
    "            full_preds[p] = panel[:H,:W]\n",
    "\n",
    "    return full_preds\n",
    "\n",
    "def postprocess(bin_full, open_iters=1):\n",
    "    if open_iters>0:\n",
    "        bin_full = np.stack([binary_opening(b, iterations=open_iters) for b in bin_full], 0)\n",
    "    return bin_full.astype(np.uint8)\n",
    "\n",
    "def component_PR(bin_full, gt_full, iou_thr=0.1, min_pix=120, min_elong=2.5):\n",
    "    TP=FP=0; total_gt=0\n",
    "    for i in range(bin_full.shape[0]):\n",
    "        pred = bin_full[i]; gt = gt_full[i].astype(np.uint8)\n",
    "        L, n = cc_label(pred, structure=np.ones((3,3), np.uint8))\n",
    "        for k, sl in enumerate(find_objects(L) or [], start=1):\n",
    "            comp = (L[sl]==k)\n",
    "            area = int(comp.sum())\n",
    "            if area < min_pix: continue\n",
    "            h = sl[0].stop - sl[0].start; w = sl[1].stop - sl[1].start\n",
    "            elong = max(h,w)/max(1,min(h,w))\n",
    "            if elong < min_elong: continue\n",
    "            gtl = gt[sl].astype(bool)\n",
    "            inter = (comp & gtl).sum(); union = (comp | gtl).sum()\n",
    "            iou = inter/union if union else 0.0\n",
    "            if iou >= iou_thr: TP += 1\n",
    "            else: FP += 1\n",
    "        _, ng = cc_label(gt, structure=np.ones((3,3), np.uint8))\n",
    "        total_gt += ng\n",
    "    FN = total_gt - TP\n",
    "    P = TP/max(TP+FP,1); R = TP/max(TP+FN,1); F1=2*P*R/max(P+R,1e-8)\n",
    "    return P,R,F1,TP,FP,total_gt\n",
    "\n",
    "# Load best weights\n",
    "ckpt = torch.load(\"./best_unet_resse.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval();\n",
    "\n",
    "# Predict on validation full frames and pick threshold by F1\n",
    "val_panels_h5 = train_h5  # we stitched the same file; extract only val indices\n",
    "p_val_full = predict_tiles_to_full(val_panels_h5, val_loader, model, tile=tile)\n",
    "with h5py.File(val_panels_h5,'r') as f:\n",
    "    gt_all = f['masks'][:].astype(np.uint8)\n",
    "gt_val_full = gt_all[idx_va]\n",
    "\n",
    "ths = np.linspace(0.5, 0.99, 20)\n",
    "best_thr, best_metrics = None, None\n",
    "for t in ths:\n",
    "    bin_v = postprocess((p_val_full>=t).astype(np.uint8), open_iters=1)\n",
    "    P,R,F1,TP,FP,TG = component_PR(bin_v, gt_val_full, iou_thr=0.1, min_pix=120, min_elong=2.5)\n",
    "    if (best_metrics is None) or (F1 > best_metrics[2]):\n",
    "        best_thr, best_metrics = t, (P,R,F1,TP,FP,TG)\n",
    "\n",
    "print(f\"[VAL] best thr={best_thr:.3f}  P={best_metrics[0]:.3f} R={best_metrics[1]:.3f} F1={best_metrics[2]:.3f} \"\n",
    "      f\"| TP={best_metrics[3]} FP={best_metrics[4]} GT={best_metrics[5]}\")\n",
    "\n",
    "# Final TEST evaluation\n",
    "p_test_full = predict_tiles_to_full(test_h5, test_loader, model, tile=tile)\n",
    "with h5py.File(test_h5,'r') as f:\n",
    "    gt_test_full = f['masks'][:].astype(np.uint8)\n",
    "\n",
    "bin_t = postprocess((p_test_full>=best_thr).astype(np.uint8), open_iters=1)\n",
    "P,R,F1,TP,FP,TG = component_PR(bin_t, gt_test_full, iou_thr=0.1, min_pix=120, min_elong=2.5)\n",
    "print(f\"[TEST] thr={best_thr:.3f}  P={P:.3f} R={R:.3f} F1={F1:.3f} | TP={TP} FP={FP} GT={TG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484c9c8-e58c-4eab-a4fb-9c7b9e482c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Asteroid detection",
   "language": "python",
   "name": "asteroid_detection_cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
