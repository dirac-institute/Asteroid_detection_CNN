{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-08T10:16:42.618644Z",
     "start_time": "2025-10-08T10:16:41.087386Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import h5py"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:16:42.709791Z",
     "start_time": "2025-10-08T10:16:42.628263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def activation_parser(activation_str):\n",
    "    \"\"\"\n",
    "    Parse a string to return the corresponding activation function.\n",
    "    Supported strings: 'relu', 'sigmoid', 'tanh', 'leaky_relu'.\n",
    "    \"\"\"\n",
    "    if activation_str.lower() == \"elu\":\n",
    "        return nn.ELU(inplace=True)\n",
    "    elif activation_str.lower() == \"hardshrink\":\n",
    "        return nn.Hardshrink(lambd=0.5)\n",
    "    elif activation_str.lower() == \"hardsigmoid\":\n",
    "        return nn.Hardsigmoid(inplace=True)\n",
    "    elif activation_str.lower() == \"hardtanh\":\n",
    "        return nn.Hardtanh(min_val=-1, max_val=1, inplace=True)\n",
    "    elif activation_str.lower() == \"leakyrelu\":\n",
    "        return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "    elif activation_str.lower() == \"logsigmoid\":\n",
    "        return nn.LogSigmoid()\n",
    "    elif activation_str.lower() == \"prelu\":\n",
    "        return nn.PReLU(num_parameters=1, init=0.25)\n",
    "    elif activation_str.lower() == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif activation_str.lower() == \"relu6\":\n",
    "        return nn.ReLU6(inplace=True)\n",
    "    elif activation_str.lower() == \"selu\":\n",
    "        return nn.SELU(inplace=True)\n",
    "    elif activation_str.lower() == \"celu\":\n",
    "        return nn.CELU(inplace=True)\n",
    "    elif activation_str.lower() == \"gelu\":\n",
    "        return nn.GELU(approximate='none')  # 'tanh' or 'none'\n",
    "    elif activation_str.lower() == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    elif activation_str.lower() == \"silu\":\n",
    "        return nn.SiLU(inplace=True)  # also known as Swish\n",
    "    elif activation_str.lower() == \"mish\":\n",
    "        return nn.Mish(inplace=True)\n",
    "    elif activation_str.lower() == \"softplus\":\n",
    "        return nn.Softplus(beta=1, threshold=20, inplace=True)\n",
    "    elif activation_str.lower() == \"softshrink\":\n",
    "        return nn.Softshrink(lambd=0.5, inplace=True)\n",
    "    elif activation_str.lower() == \"softsign\":\n",
    "        return nn.Softsign()\n",
    "    elif activation_str.lower() == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif activation_str.lower() == \"tanhshrink\":\n",
    "        return nn.Tanhshrink()\n",
    "    elif activation_str.lower() == \"threshold\":\n",
    "        return nn.Threshold(threshold=0.25, value=0.0, inplace=True)\n",
    "    elif activation_str.lower() == \"glu\":\n",
    "        return nn.GLU(dim=1)  # assumes input has shape (B, C, H, W)\n",
    "    elif activation_str.lower() == \"softmax\":\n",
    "        return nn.Softmax(dim=1)  # applies softmax across channels\n",
    "    elif activation_str.lower() == \"logsoftmax\":\n",
    "        return nn.LogSoftmax(dim=1)  # applies log softmax across channels\n",
    "    elif activation_str.lower() == \"none\":\n",
    "        return nn.Identity()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation_str}\")\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // ratio, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(in_channels // ratio, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.BatchNorm1d(in_channels)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.avg_pool(x)        # (B, C, 1, 1)\n",
    "        avg_out = avg_out.view(avg_out.size(0), avg_out.size(1))  # (B, C)\n",
    "        avg_out = self.fc1(avg_out)       # (B, C//ratio)\n",
    "        avg_out = self.relu(avg_out)\n",
    "        avg_out = self.fc2(avg_out)       # (B, C)\n",
    "\n",
    "        max_out = self.max_pool(x)        # (B, C, 1, 1)\n",
    "        max_out = max_out.view(max_out.size(0), max_out.size(1))  # (B, C)\n",
    "        max_out = self.fc1(max_out)       # (B, C//ratio)\n",
    "        max_out = self.relu(max_out)\n",
    "        max_out = self.fc2(max_out)       # (B, C)\n",
    "\n",
    "        out = avg_out + max_out           # (B, C)\n",
    "        out = self.norm(out)              # (B, C)\n",
    "        scale = self.sigmoid(out)         # (B, C)\n",
    "        scale = scale.view(scale.size(0), scale.size(1), 1, 1)  # (B, C, 1, 1)\n",
    "        return x * scale                  # broadcast along H, W\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super().__init__()\n",
    "        assert kernel_size in (3, 5, 7)\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.BatchNorm2d(1)\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)     # (B, 1, H, W)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)   # (B, 1, H, W)\n",
    "        concat = torch.cat([avg_out, max_out], dim=1)    # (B, 2, H, W)\n",
    "        attn = self.conv(concat)                         # (B, 1, H, W)\n",
    "        attn = self.norm(attn)                           # (B, 1, H, W)\n",
    "        attn = self.sigmoid(attn)\n",
    "        return x * attn                                  # broadcast across C\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_channels, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_att(x)\n",
    "        x = self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, kernel_size, padding, dilation=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_ch, in_ch, kernel_size=kernel_size,\n",
    "            padding=padding, dilation=dilation,\n",
    "            groups=in_ch, bias=True\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=True)\n",
    "        self.norm = nn.BatchNorm2d(out_ch)\n",
    "        self.act = activation_parser(activation)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.depthwise.weight, mode='fan_out', nonlinearity=\"relu\")\n",
    "        nn.init.constant_(self.depthwise.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.pointwise.weight, mode='fan_out', nonlinearity=\"relu\")\n",
    "        nn.init.constant_(self.pointwise.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.norm(x))\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        dilations = [1, 2, 3, 4]\n",
    "        kernels   = [1, 3, 5, 7]\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d, k in zip(dilations, kernels):\n",
    "            pad = (k // 2) * d\n",
    "            self.branches.append(\n",
    "                SepConv(in_ch, out_ch, activation, kernel_size=k, padding=pad, dilation=d)\n",
    "            )\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(dilations) * out_ch, out_ch, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation)\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.merge[0].weight, mode='fan_out', nonlinearity=\"relu\")\n",
    "        nn.init.constant_(self.merge[0].bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [branch(x) for branch in self.branches]\n",
    "        x = torch.cat(outs, dim=1)\n",
    "        return self.merge(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            activation_parser(activation)\n",
    "        )\n",
    "        for m in self.block.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity=\"relu\")\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        # W_g projects gating signal\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # W_x projects skip connection\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        # psi computes 1‐channel attention map\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, F_g, kernel_size=1, bias=True),\n",
    "            nn.BatchNorm2d(F_g),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.W_g[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.W_g[0].bias, 0)\n",
    "        nn.init.kaiming_normal_(self.W_x[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(self.W_x[0].bias, 0)\n",
    "        nn.init.xavier_uniform_(self.psi[0].weight)\n",
    "        nn.init.constant_(self.psi[0].bias, 0)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        \"\"\"\n",
    "        g: gating signal from decoder, shape (B, F_g, H, W)\n",
    "        x: skip connection from encoder, shape (B, F_l, H, W)\n",
    "        \"\"\"\n",
    "        g1 = self.W_g(g)   # (B, F_int, H, W)\n",
    "        x1 = self.W_x(x)   # (B, F_int, H, W)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)   # (B, 1, H, W)\n",
    "        return x * psi        # broadcast along channel\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, activation, dropout_prob=0.0, attention=True, pool=True, ASPP_blocks=True):\n",
    "        super().__init__()\n",
    "        if ASPP_blocks:\n",
    "            # Use ASPP instead of DoubleConv\n",
    "            self.conv = ASPP(in_ch, out_ch, activation)\n",
    "        else:\n",
    "            # Use DoubleConv if ASPP_blocks is False\n",
    "            self.conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "        self.pool        = pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        skip = x.clone()\n",
    "        if self.pool:\n",
    "            x = F.max_pool2d(x, kernel_size=2)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, activation, dropout_prob=0.0, attention=True, upsample=True, ASPP_blocks=True):\n",
    "        \"\"\"\n",
    "        in_ch:   channels from previous layer (bottleneck or previous decoder)\n",
    "        skip_ch: channels in the corresponding encoder skip\n",
    "        out_ch:  desired output channels for this decoder block\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.skip_ch = skip_ch\n",
    "\n",
    "        if self.upsample:\n",
    "            # ConvTranspose2d(in_ch → skip_ch) to match spatial & channel dims\n",
    "            self.up = nn.ConvTranspose2d(in_ch, skip_ch, kernel_size=3,\n",
    "                                         stride=2, padding=1, output_padding=1, bias=True)\n",
    "            nn.init.kaiming_normal_(self.up.weight, mode='fan_out', nonlinearity='relu')\n",
    "            self.bn_up = nn.BatchNorm2d(skip_ch)\n",
    "            self.act_up = activation_parser(activation)\n",
    "            self.attention = AttentionGate(F_g=skip_ch, F_l=skip_ch, F_int=skip_ch // 2) if attention else nn.Identity()\n",
    "        else:\n",
    "            self.up = None\n",
    "            self.bn_up = None\n",
    "            self.act_up = None\n",
    "            self.attention = AttentionGate(F_g=in_ch, F_l=in_ch, F_int=in_ch // 2) if attention else nn.Identity()\n",
    "\n",
    "        #self.double_conv = DoubleConv(in_double, out_ch, activation)\n",
    "        if ASPP_blocks:\n",
    "            # Use ASPP instead of DoubleConv\n",
    "            self.conv = ASPP(in_ch, out_ch, activation)\n",
    "        else:\n",
    "            # Use DoubleConv if ASPP_blocks is False\n",
    "            self.conv = DoubleConv(in_ch, out_ch, activation)\n",
    "        self.cbam        = CBAMBlock(out_ch, ratio=8, kernel_size=7) if attention else nn.Identity()\n",
    "        self.dropout     = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        if self.upsample:\n",
    "            x = self.up(x)       # (B, skip_ch, H*2, W*2)\n",
    "            x = self.bn_up(x)\n",
    "            x = self.act_up(x)\n",
    "        if skip is not None:\n",
    "            skip = self.attention(g=x, x=skip)\n",
    "            x = torch.cat([x, skip], dim=1)  # (B, 2*skip_ch, H*2, W*2)\n",
    "        x = self.conv(x)\n",
    "        x = self.cbam(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleneckTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a tensor of shape (B, C, H, W), flattens the H×W patches into tokens,\n",
    "    runs a small TransformerEncoder over them, then reshapes back to (B, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, heads=8, depth=3, mlp_dim=None):\n",
    "        super().__init__()\n",
    "        mlp_dim = mlp_dim or dim * 4\n",
    "        # one TransformerEncoderLayer (or more, if depth>1)\n",
    "        layer_e = nn.TransformerEncoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        layer_d = nn.TransformerDecoderLayer(\n",
    "            d_model=dim,\n",
    "            nhead=heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            activation='relu',\n",
    "            norm_first=True,  # important for TransformerDecoder\n",
    "            #batch_first=True\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(layer_e, num_layers=depth//2 if depth > 1 else depth)\n",
    "        self.norm    = nn.LayerNorm(dim)\n",
    "        if depth > 1:\n",
    "            self.decoder = nn.TransformerDecoder(layer_d, num_layers=depth - depth//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # flatten spatial dims:\n",
    "        # → (B, C, H*W) then permute to (H*W, B, C) for PyTorch’s MHSA\n",
    "        tokens = x.flatten(2).permute(2, 0, 1)   # (H*W, B, C)\n",
    "        # run through TransformerEncoder\n",
    "        out   = self.encoder(tokens)             # (H*W, B, C)\n",
    "        # run through TransformerDecoder (optional, if depth > 1)\n",
    "        if hasattr(self, 'decoder'):\n",
    "            out = self.decoder(out, out)          # (H*W, B, C)\n",
    "        # put back into (B, C, H, W) after a LayerNorm on each token\n",
    "        out   = out.permute(1, 2, 0).view(B, C, H, W)\n",
    "        return self.norm(out.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "        # explanation of the two permutes:\n",
    "        #  - out.permute(1,2,0)→(B, C, H*W) then .view(B, C, H, W)\n",
    "        #  - we want LN over the C‐dimension, so we permute to (B, H, W, C), apply LayerNorm,\n",
    "        #    then back to (B, C, H, W).\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 out_channels=1,\n",
    "                 down_filters=None,\n",
    "                 down_activations=None,\n",
    "                 up_filters=None,\n",
    "                 up_activations=None,\n",
    "                 bottleneck_transformer=True,\n",
    "                 ASPP_blocks=True,\n",
    "                 output_sigmoid=True):\n",
    "        super().__init__()\n",
    "        assert len(down_filters) == len(down_activations)\n",
    "        assert len(up_filters)   == len(up_activations)\n",
    "\n",
    "        # Build Encoder path\n",
    "        self.output_sigmoid = output_sigmoid\n",
    "        self.input_norm = nn.BatchNorm2d(in_channels)\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.bottleneck_transformer = bottleneck_transformer\n",
    "        prev_ch = in_channels\n",
    "        for i, out_ch in enumerate(down_filters):\n",
    "            act_str = down_activations[i].lower()\n",
    "            self.encoders.append(\n",
    "                EncoderBlock(in_ch=prev_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_str,\n",
    "                             dropout_prob=0.1,\n",
    "                             attention=(i != 0),\n",
    "                             pool=True,\n",
    "                             ASPP_blocks=ASPP_blocks)\n",
    "            )\n",
    "            prev_ch = out_ch\n",
    "\n",
    "        # Bottleneck:\n",
    "        if bottleneck_transformer:\n",
    "            self.bottleneck  = BottleneckTransformer(dim=down_filters[-1],\n",
    "                                                           heads=4,\n",
    "                                                           depth=4)\n",
    "        else:\n",
    "            self.bottleneck = nn.Identity()\n",
    "\n",
    "        # Build Decoder path\n",
    "        self.decoders = nn.ModuleList()\n",
    "        N = len(down_filters)\n",
    "        for i in range(len(up_filters)):\n",
    "            act_str = up_activations[i].lower()\n",
    "            # Corresponding skip channels from encoder\n",
    "            skip_ch = down_filters[N - 1 - i]\n",
    "            # Input channels for this decoder block\n",
    "            out_ch = up_filters[i]\n",
    "            in_ch_dec = (down_filters[-1] * 1) if (i == 0) else up_filters[i - 1]\n",
    "\n",
    "            self.decoders.append(\n",
    "                DecoderBlock(in_ch=in_ch_dec,\n",
    "                             skip_ch=skip_ch,\n",
    "                             out_ch=out_ch,\n",
    "                             activation=act_str,\n",
    "                             dropout_prob=0.1,\n",
    "                             attention= True,\n",
    "                             upsample=True,\n",
    "                             ASPP_blocks=ASPP_blocks)\n",
    "            )\n",
    "\n",
    "        if output_sigmoid:\n",
    "            self.final_conv = nn.Sequential(\n",
    "                nn.Conv2d(up_filters[-1], out_channels, kernel_size=5, padding=2, bias=True),\n",
    "                nn.Sigmoid())\n",
    "            nn.init.kaiming_normal_(self.final_conv[0].weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "            nn.init.constant_(self.final_conv[0].bias, 0)\n",
    "        else:\n",
    "            self.final_conv = nn.Conv2d(up_filters[-1], out_channels, kernel_size=5, padding=2, bias=True)\n",
    "            nn.init.kaiming_normal_(self.final_conv.weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.constant_(self.final_conv.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_norm(x)  # Normalize input\n",
    "        # x: (B, 1, 128, 128)\n",
    "        skips = []\n",
    "        for enc in self.encoders[:-1]:  # skip last encoder (bottleneck)\n",
    "            x, skip = enc(x)\n",
    "            skips.append(skip)\n",
    "\n",
    "        # Bottleneck:\n",
    "        x, _ = self.encoders[-1](x) # last encoder does not return a skip\n",
    "        skips.append(None)\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoders[0](x, skips[-1])  # first decoder uses the last encoder skip\n",
    "\n",
    "        skips = skips[::-1]              # reverse order for decoding\n",
    "\n",
    "        for i in range(1, len(self.decoders)):\n",
    "            skip_feat = skips[i]\n",
    "            x = self.decoders[i](x, skip_feat)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ],
   "id": "f1ce347f05a76c76",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:16:42.757354Z",
     "start_time": "2025-10-08T10:16:42.752562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_ds, val_ds, epochs=100, batch_size=32, lr=1e-3, loss=None, alpha=0.99, gamma=3.1, device=None):\n",
    "    \"\"\"\n",
    "    Train the model on train_ds, validate on val_ds, and print losses + F1 each epoch.\n",
    "    Resizes all masks to `output_size` so that preds and targets match in spatial dims.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 1) Figure out the model’s output spatial size by pushing a dummy 128×128 patch.\n",
    "    model.eval()  # ensure BatchNorm uses running‐stats, not “batch” stats\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1, 1, 128, 128).to(device)\n",
    "        out_dummy = model(dummy)\n",
    "        output_size = (out_dummy.shape[-2], out_dummy.shape[-1])  # e.g. (32,32) for your JSON\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "\n",
    "    if loss is None:\n",
    "        criterion = ComboLossTF(bce_weight=0.0, dice_weight=0.0, focal_twersky_weight=1, alpha=alpha, gamma=gamma)\n",
    "    else:\n",
    "        criterion = loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=lr,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=epochs,\n",
    "                                                pct_start=0.1,\n",
    "                                                anneal_strategy='cos')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ——— Training ———\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        tp = fp = fn = 0\n",
    "        for batch_num, (imgs, masks) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)  # (B,1,128,128)\n",
    "\n",
    "            # Resize the ground‐truth masks to output_size (e.g. (32,32))\n",
    "            m_resized = reshape_masks(masks, new_size=output_size).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)              # (B,1, output_H, output_W)\n",
    "            loss = criterion(preds, m_resized)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sched.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            if not model.output_sigmoid:\n",
    "                # If model does not output Sigmoid, apply it here\n",
    "                preds = torch.sigmoid(preds)\n",
    "            with torch.no_grad():\n",
    "                pred_bin = (preds > 0.5).float()\n",
    "                t = m_resized\n",
    "                tp += (pred_bin * t).sum().item()\n",
    "                fp += (pred_bin * (1 - t)).sum().item()\n",
    "                fn += ((1 - pred_bin) * t).sum().item()\n",
    "\n",
    "            prec = tp / (tp + fp + 1e-8)\n",
    "            rec  = tp / (tp + fn + 1e-8)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            print (f\"\\rEpoch {epoch:03d}  \"\n",
    "                   f\"Batch {batch_num+1:03d}/{len(train_loader)}  \"\n",
    "                   f\"Batch Loss: {loss.item():.4f}  \"\n",
    "                   f\"| train F1: {f1:.4f}  | train precision: {prec:.4f}  | train recall: {rec:.4f}\", end='\\r')\n",
    "\n",
    "        train_loss = running_loss / len(train_ds)\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "\n",
    "        # ——— Validation ———\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        tp = fp = fn = 0\n",
    "        val_y = []\n",
    "        pred_val = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                m_resized = reshape_masks(masks, new_size=output_size).to(device)\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, m_resized)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                if not model.output_sigmoid:\n",
    "                    preds = torch.sigmoid(preds)  # apply Sigmoid if model does not output it\n",
    "                pred_bin = (preds > 0.5).float()\n",
    "                tp += (pred_bin * m_resized).sum().item()\n",
    "                fp += (pred_bin * (1 - m_resized)).sum().item()\n",
    "                fn += ((1 - pred_bin) * m_resized).sum().item()\n",
    "                val_y.append(m_resized.cpu().numpy())\n",
    "                pred_val.append(preds.cpu().numpy())\n",
    "        # Collect all validation masks for AUC calculation\n",
    "        val_y = np.concatenate(val_y, axis=0)\n",
    "        preds_val = np.concatenate(pred_val, axis=0)  # (N, 1, Hout, Wout)\n",
    "        val_loss = val_loss / len(val_ds)\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1_val = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "        auc_val = sklearn.metrics.roc_auc_score(val_y.flatten(), preds_val.flatten() )\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}  \"\n",
    "              f\"| Val Loss: {val_loss:.4f}  \"\n",
    "              f\"| Train F1: {f1:.4f}  \"\n",
    "              f\"| Val F1: {f1_val:.4f}  \"\n",
    "              f\"| Val Prec: {prec:.4f}  \"\n",
    "              f\"| Val Rec: {rec:.4f}\"\n",
    "              f\"| Val AUC: {auc_val:.4f}\")\n",
    "\n",
    "    return model\n"
   ],
   "id": "96f5f47c3516a4f8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:16:42.807245Z",
     "start_time": "2025-10-08T10:16:42.801927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds:   Tensor (B,1,H,W) after Sigmoid\n",
    "        targets: Tensor (B,1,H,W) binary {0,1}\n",
    "        \"\"\"\n",
    "        p_flat = preds.view(-1)\n",
    "        t_flat = targets.view(-1)\n",
    "        intersection = (p_flat * t_flat).sum()\n",
    "        dice_coeff = (2. * intersection + self.smooth) / (p_flat.sum() + t_flat.sum() + self.smooth)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma, self.eps = alpha, gamma, eps\n",
    "        self.beta = 1 - alpha  # Ensure alpha + beta = 1\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        TP = (preds * targets).sum()\n",
    "        FP = (preds * (1 - targets)).sum()\n",
    "        FN = ((1 - preds) * targets).sum()\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return torch.pow((1 - tversky), self.gamma)\n",
    "\n",
    "class ComboLossTF(nn.Module):\n",
    "    def __init__(self, bce_weight=0.33, dice_weight=0.33, focal_twersky_weight=0.33, alpha=0.95, gamma=3.1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss(smooth=1e-6)\n",
    "        self.FW = FocalTverskyLoss (alpha = alpha, gamma=gamma)\n",
    "        self.bw, self.dw, self.fw = bce_weight, dice_weight, focal_twersky_weight\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds, targets both (B,1,H,W)\n",
    "        total_loss = 0\n",
    "        if self.bw > 0:\n",
    "            l_bce = self.bce(preds, targets)\n",
    "            total_loss += self.bw * l_bce\n",
    "        if self.dw > 0:\n",
    "            l_dice = self.dice(preds, targets)\n",
    "            total_loss += self.dw * l_dice\n",
    "        if self.fw > 0:\n",
    "            l_focal_tversky = self.FW(preds, targets)\n",
    "            total_loss += self.fw * l_focal_tversky\n",
    "        return total_loss\n",
    "\n",
    "def sigzi(x, axis=None):\n",
    "    \"\"\"\n",
    "Compute the interquartile range (IQR) of x along the specified axis.\n",
    "    Args:\n",
    "        x: array-like, shape (P, H, W) or (H, W) or (N, C, H, W)\n",
    "        axis: axis along which to compute the IQR.\n",
    "              If None, computes over the flattened array.\n",
    "\n",
    "    Returns: float, the IQR of x.\n",
    "\n",
    "    \"\"\"\n",
    "    return 0.741 * (np.percentile(x, 75, axis=axis) - np.percentile(x, 25, axis=axis))\n",
    "\n",
    "def split_stack(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Split a stack of 2D panels into (nrows × ncols) tiles.\n",
    "    arr: ndarray, shape (P, H, W)\n",
    "    Returns: ndarray, shape (P * (H//nrows)*(W//ncols), nrows, ncols)\n",
    "    \"\"\"\n",
    "    P, H, W = arr.shape\n",
    "    pad_h = (-H) % nrows\n",
    "    pad_w = (-W) % ncols\n",
    "    if pad_h or pad_w:\n",
    "        arr = np.pad(arr,\n",
    "                     ((0, 0),\n",
    "                      (0, pad_h),\n",
    "                      (0, pad_w)),\n",
    "                     mode='constant',\n",
    "                     constant_values=0)\n",
    "    H2, W2 = arr.shape[1], arr.shape[2]\n",
    "    blocks = (arr\n",
    "              .reshape(P,\n",
    "                       H2 // nrows, nrows,\n",
    "                       W2 // ncols, ncols)\n",
    "              .swapaxes(2, 3))\n",
    "    P2, Hb, Wb, nr, nc = blocks.shape\n",
    "    out = blocks.reshape(P2 * Hb * Wb, nr, nc)\n",
    "    return out\n",
    "\n",
    "def build_datasets(h5_path, tile_size=128, clip_min=-7.0, clip_max=7.0):\n",
    "    \"\"\"\n",
    "    Load HDF5 training data, normalize, clip, tile, and return PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        h5_path (str): Path to the .h5 file with \"images\" and \"masks\" datasets.\n",
    "        tile_size (int): Size of square tiles to extract from each image.\n",
    "        clip_min (float): Minimum value for clipping.\n",
    "        clip_max (float): Maximum value for clipping.\n",
    "\n",
    "    Returns:\n",
    "        x_tiles (Tensor): (N, 1, tile_size, tile_size) image tiles\n",
    "        y_tiles (Tensor): (N, 1, tile_size, tile_size) mask tiles\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        x = f[\"images\"][:]  # shape (N, H, W), float32\n",
    "        y = f[\"masks\"][:]   # shape (N, H, W), bool or int\n",
    "        print(x.shape)\n",
    "\n",
    "    # Normalize and clip\n",
    "    #x = x / sigzi(x)\n",
    "    #x = np.clip(x, clip_min, clip_max)\n",
    "    #print (x.shape)\n",
    "    x = np.clip(x, -166.43, 169.96)\n",
    "\n",
    "    # Tile images and masks\n",
    "    x_tiles = split_stack(x, tile_size, tile_size)  # (N, tile_size, tile_size)\n",
    "    y_tiles = split_stack(y.astype(\"float32\"), tile_size, tile_size)\n",
    "\n",
    "    # Convert to PyTorch tensors and add channel dimension\n",
    "    x_tiles = torch.from_numpy(x_tiles).float().unsqueeze(1)\n",
    "    y_tiles = torch.from_numpy(y_tiles).float().unsqueeze(1)\n",
    "    print (x_tiles.shape)\n",
    "\n",
    "    return x_tiles, y_tiles\n",
    "\n",
    "\n",
    "def reshape_masks(masks, new_size):\n",
    "    \"\"\"\n",
    "    Resize binary masks (0/1) to `new_size`:\n",
    "      - Uses bilinear interpolation (same as TF’s tf.image.resize with bilinear)\n",
    "      - Applies torch.ceil(...) to recover {0,1} values exactly.\n",
    "    Input:\n",
    "      - masks: either a Tensor of shape (N, 1, H_orig, W_orig)\n",
    "               or a numpy array of shape (N, H_orig, W_orig)\n",
    "      - new_size: tuple (new_H, new_W)\n",
    "    Returns:\n",
    "      - Tensor of shape (N, 1, new_H, new_W), values in {0,1}\n",
    "    \"\"\"\n",
    "    if isinstance(masks, np.ndarray):\n",
    "        m = torch.from_numpy(masks).float().unsqueeze(1)  # → (N,1,H,W)\n",
    "    else:\n",
    "        m = masks  # assume already FloatTensor (N,1,H,W)\n",
    "    m_resized = F.interpolate(m, size=new_size, mode='bilinear', align_corners=False)\n",
    "    m_resized = torch.ceil(m_resized)\n",
    "    return m_resized.clamp(0, 1)\n",
    "\n",
    "def split_train_val(x_tiles, y_tiles, train_frac=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Shuffle and split x_tiles, y_tiles into two TensorDatasets: train (80%) and val (20%).\n",
    "    \"\"\"\n",
    "    n = x_tiles.shape[0]\n",
    "    idx = torch.randperm(n, generator=torch.Generator().manual_seed(seed))\n",
    "    split = int(train_frac * n)\n",
    "    train_idx = idx[:split]\n",
    "    val_idx   = idx[split:]\n",
    "    train_idx, val_idx = train_idx.sort().values, val_idx.sort().values\n",
    "    x_tr, y_tr = x_tiles[train_idx], y_tiles[train_idx]\n",
    "    x_val, y_val = x_tiles[val_idx], y_tiles[val_idx]\n",
    "    return TensorDataset(x_tr, y_tr), TensorDataset(x_val, y_val)"
   ],
   "id": "2e21c1bd7cdc3266",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:42:14.479452Z",
     "start_time": "2025-10-08T16:42:14.471184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "CLIP_MIN, CLIP_MAX = -166.43, 169.96  # match TF\n",
    "\n",
    "def _tiles_for_shape(H, W, tile):\n",
    "    Hb = (H + tile - 1) // tile\n",
    "    Wb = (W + tile - 1) // tile\n",
    "    return Hb, Wb\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Streams tiles directly from an HDF5 file, no full-array load.\n",
    "    Each worker opens its own h5 handle (h5py is not fork/thread safe).\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path, tile_size=128, pos_fraction=1.0, seed=42):\n",
    "        self.h5_path   = h5_path\n",
    "        self.tile      = tile_size\n",
    "        self.rng       = np.random.default_rng(seed)\n",
    "        self.h5        = None  # opened lazily per worker\n",
    "\n",
    "        # Probe shapes without reading the datasets\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            ds_x = f[\"images\"]; ds_y = f[\"masks\"]\n",
    "            self.N, self.H, self.W = ds_x.shape\n",
    "            assert ds_y.shape == (self.N, self.H, self.W)\n",
    "\n",
    "            # Precompute tile index list: (img_idx, r, c)\n",
    "            Hb, Wb = _tiles_for_shape(self.H, self.W, self.tile)\n",
    "            self.all_indices = [(i, r, c)\n",
    "                                for i in range(self.N)\n",
    "                                for r in range(Hb)\n",
    "                                for c in range(Wb)]\n",
    "\n",
    "            # (Optional) positive-tile mining to fight imbalance\n",
    "            # Build a quick bitmap of which tiles have any positives\n",
    "            # (sample a thin mask to avoid full read)\n",
    "            self.pos_tiles = []\n",
    "            step = tile_size  # coarse scan at tile stride\n",
    "            for i in range(self.N):\n",
    "                # read a subsampled view to quickly mark tiles\n",
    "                # (safe on memory: only (H/step)*(W/step) booleans)\n",
    "                m = ds_y[i, ::step, ::step]\n",
    "                Hp, Wp = m.shape\n",
    "                for r in range(Hp):\n",
    "                    for c in range(Wp):\n",
    "                        if m[r, c] > 0:\n",
    "                            self.pos_tiles.append((i, r, c))\n",
    "            # Optionally upsample positive tiles by pos_fraction\n",
    "            if 0 < pos_fraction < 1.0 and len(self.pos_tiles) > 0:\n",
    "                keep = int(len(self.pos_tiles) * pos_fraction)\n",
    "                self.pos_tiles = self.rng.choice(self.pos_tiles, size=keep, replace=False).tolist()\n",
    "\n",
    "        # Mix: all tiles; you can also bias sampling later via a Sampler\n",
    "        self.indices = self.all_indices\n",
    "\n",
    "    def _ensure_open(self):\n",
    "        if self.h5 is None:\n",
    "            self.h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self.ds_x = self.h5[\"images\"]\n",
    "            self.ds_y = self.h5[\"masks\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure_open()\n",
    "        i, r, c = self.indices[idx]\n",
    "        t = self.tile\n",
    "        r0, c0 = r * t, c * t\n",
    "        r1, c1 = min(r0 + t, self.H), min(c0 + t, self.W)\n",
    "\n",
    "        # Slice the exact window (handles borders)\n",
    "        x = self.ds_x[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        y = self.ds_y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "\n",
    "        # Pad to (t,t) if we hit the edges\n",
    "        if x.shape[0] != t or x.shape[1] != t:\n",
    "            xp = np.zeros((t, t), dtype=np.float32)\n",
    "            yp = np.zeros((t, t), dtype=np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x\n",
    "            yp[:y.shape[0], :y.shape[1]] = y\n",
    "            x, y = xp, yp\n",
    "\n",
    "        # Match TF preprocessing\n",
    "        x = np.clip(x, CLIP_MIN, CLIP_MAX)\n",
    "        x = x[None, ...]  # add channel dim\n",
    "        y = y[None, ...]\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "def _worker_init_fn(worker_id):\n",
    "    # each worker will lazily open its own h5 in __getitem__\n",
    "    pass\n",
    "\n",
    "def train_model(model, train_ds, val_ds, epochs=100, batch_size=32, lr=1e-3,\n",
    "                loss=None, alpha=0.99, gamma=3.1, device=None,\n",
    "                num_workers=0, pin_memory=False, prefetch_factor=2, persistent_workers=False):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # infer output size once\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1, 1, 128, 128, device=device)\n",
    "        out_dummy = model(dummy)\n",
    "        output_size = (out_dummy.shape[-2], out_dummy.shape[-1])\n",
    "\n",
    "    # ✅ use caller-provided worker settings\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory,\n",
    "                              prefetch_factor=prefetch_factor if num_workers>0 else None,\n",
    "                              persistent_workers=persistent_workers if num_workers>0 else False)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=pin_memory,\n",
    "                              prefetch_factor=prefetch_factor if num_workers>0 else None,\n",
    "                              persistent_workers=persistent_workers if num_workers>0 else False)\n",
    "\n",
    "    if loss is None:\n",
    "        criterion = ComboLossTF(bce_weight=0.0, dice_weight=0.0, focal_twersky_weight=1, alpha=alpha, gamma=gamma)\n",
    "    else:\n",
    "        criterion = loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=lr,\n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=epochs,\n",
    "                                                pct_start=0.1,\n",
    "                                                anneal_strategy='cos')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ——— Training ———\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        tp = fp = fn = 0\n",
    "        for batch_num, (imgs, masks) in enumerate(train_loader):\n",
    "            imgs = imgs.to(device)  # (B,1,128,128)\n",
    "            #print(imgs.shape)\n",
    "\n",
    "            # Resize the ground‐truth masks to output_size (e.g. (32,32))\n",
    "            m_resized = reshape_masks(masks, new_size=output_size).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(imgs)              # (B,1, output_H, output_W)\n",
    "            loss = criterion(preds, m_resized)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sched.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            if not model.output_sigmoid:\n",
    "                # If model does not output Sigmoid, apply it here\n",
    "                preds = torch.sigmoid(preds)\n",
    "            with torch.no_grad():\n",
    "                pred_bin = (preds > 0.5).float()\n",
    "                t = m_resized\n",
    "                tp += (pred_bin * t).sum().item()\n",
    "                fp += (pred_bin * (1 - t)).sum().item()\n",
    "                fn += ((1 - pred_bin) * t).sum().item()\n",
    "\n",
    "            prec = tp / (tp + fp + 1e-8)\n",
    "            rec  = tp / (tp + fn + 1e-8)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            print (f\"Epoch {epoch:03d}  \"\n",
    "                   f\"Batch {batch_num+1:03d}/{len(train_loader)}  \"\n",
    "                   f\"Batch Loss: {loss.item():.4f}  \"\n",
    "                   f\"| train F1: {f1:.4f}  | train precision: {prec:.4f}  | train recall: {rec:.4f}\", end='\\n', flush=True)\n",
    "\n",
    "        train_loss = running_loss / len(train_ds)\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "\n",
    "        # ——— Validation ———\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        tp = fp = fn = 0\n",
    "        val_y = []\n",
    "        pred_val = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                m_resized = reshape_masks(masks, new_size=output_size).to(device)\n",
    "                preds = model(imgs)\n",
    "                loss = criterion(preds, m_resized)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                if not model.output_sigmoid:\n",
    "                    preds = torch.sigmoid(preds)  # apply Sigmoid if model does not output it\n",
    "                pred_bin = (preds > 0.5).float()\n",
    "                tp += (pred_bin * m_resized).sum().item()\n",
    "                fp += (pred_bin * (1 - m_resized)).sum().item()\n",
    "                fn += ((1 - pred_bin) * m_resized).sum().item()\n",
    "                val_y.append(m_resized.cpu().numpy())\n",
    "                pred_val.append(preds.cpu().numpy())\n",
    "        # Collect all validation masks for AUC calculation\n",
    "        val_y = np.concatenate(val_y, axis=0)\n",
    "        preds_val = np.concatenate(pred_val, axis=0)  # (N, 1, Hout, Wout)\n",
    "        val_loss = val_loss / len(val_ds)\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1_val = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "        auc_val = sklearn.metrics.roc_auc_score(val_y.flatten(), preds_val.flatten() )\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}  \"\n",
    "              f\"Train Loss: {train_loss:.4f}  \"\n",
    "              f\"| Val Loss: {val_loss:.4f}  \"\n",
    "              f\"| Train F1: {f1:.4f}  \"\n",
    "              f\"| Val F1: {f1_val:.4f}  \"\n",
    "              f\"| Val Prec: {prec:.4f}  \"\n",
    "              f\"| Val Rec: {rec:.4f}\"\n",
    "              f\"| Val AUC: {auc_val:.4f}\")\n",
    "\n",
    "    return model\n"
   ],
   "id": "8ab2c097dea407d3",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:28:11.718713Z",
     "start_time": "2025-10-08T16:46:06.388219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "down_filters =  [32, 32, 64, 128, 256, 512, 1024]\n",
    "down_activations = ['relu', 'selu', 'selu', 'selu', 'selu', 'selu', 'selu']\n",
    "\n",
    "up_filters       = [1024, 512, 256, 128, 64]\n",
    "up_activations   = ['selu', 'selu', 'selu', 'selu', 'relu']\n",
    "# BCE loss with logits with\n",
    "#x_tiles, y_tiles = build_datasets(\"../DATA/test.h5\", tile_size=128)\n",
    "\n",
    "#print(\"Positive/negative ratio:\", (y_tiles==1).sum().item() / (y_tiles==0).sum().item())\n",
    "\n",
    "#neg = (y_tiles==0).sum()\n",
    "#pos = (y_tiles==1).sum()\n",
    "#pos_weight = (neg / pos).float()\n",
    "#loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1 / 0.003329051612807811))\n",
    "model = UNet(\n",
    "        down_filters=down_filters,\n",
    "        down_activations=down_activations,\n",
    "        up_filters=up_filters,\n",
    "        up_activations=up_activations,\n",
    "        bottleneck_transformer=False,\n",
    "        ASPP_blocks=False,\n",
    "        output_sigmoid=False)\n",
    "\n",
    "full_ds = H5TiledDataset(\"../DATA/train_chunked.h5\", tile_size=128)\n",
    "\n",
    "# Split without materializing:\n",
    "n = len(full_ds)\n",
    "n_tr = int(0.8 * n)\n",
    "n_va = n - n_tr\n",
    "\n",
    "train_ds, val_ds = random_split(full_ds, [n_tr, n_va],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_ds, val_ds,\n",
    "    epochs=150,\n",
    "    batch_size=64,          # start smaller to reduce I/O pressure\n",
    "    lr=1.5e-4,\n",
    "    loss=loss,              # or use Focal-Tversky combo (recommended)\n",
    "    num_workers=32,          # ✅ critical: no multiprocessing\n",
    "    pin_memory=False\n",
    ")\n"
   ],
   "id": "be859a1c9583bf0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001  Batch 001/10240  Batch Loss: 2.1704  | train F1: 0.0020  | train precision: 0.0010  | train recall: 0.8710\n",
      "Epoch 001  Batch 002/10240  Batch Loss: 2.8776  | train F1: 0.0032  | train precision: 0.0016  | train recall: 0.6098\n",
      "Epoch 001  Batch 003/10240  Batch Loss: 2.3687  | train F1: 0.0056  | train precision: 0.0028  | train recall: 0.7712\n",
      "Epoch 001  Batch 004/10240  Batch Loss: 2.8409  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.7694\n",
      "Epoch 001  Batch 005/10240  Batch Loss: 2.0287  | train F1: 0.0053  | train precision: 0.0027  | train recall: 0.7691\n",
      "Epoch 001  Batch 006/10240  Batch Loss: 2.3819  | train F1: 0.0054  | train precision: 0.0027  | train recall: 0.7802\n",
      "Epoch 001  Batch 007/10240  Batch Loss: 2.9573  | train F1: 0.0063  | train precision: 0.0032  | train recall: 0.7807\n",
      "Epoch 001  Batch 008/10240  Batch Loss: 2.2109  | train F1: 0.0058  | train precision: 0.0029  | train recall: 0.7787\n",
      "Epoch 001  Batch 009/10240  Batch Loss: 2.4495  | train F1: 0.0060  | train precision: 0.0030  | train recall: 0.7945\n",
      "Epoch 001  Batch 010/10240  Batch Loss: 2.6036  | train F1: 0.0064  | train precision: 0.0032  | train recall: 0.7974\n",
      "Epoch 001  Batch 011/10240  Batch Loss: 2.7080  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.8092\n",
      "Epoch 001  Batch 012/10240  Batch Loss: 2.6301  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8055\n",
      "Epoch 001  Batch 013/10240  Batch Loss: 2.6646  | train F1: 0.0078  | train precision: 0.0039  | train recall: 0.8199\n",
      "Epoch 001  Batch 014/10240  Batch Loss: 2.4269  | train F1: 0.0078  | train precision: 0.0039  | train recall: 0.8197\n",
      "Epoch 001  Batch 015/10240  Batch Loss: 2.0170  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.8197\n",
      "Epoch 001  Batch 016/10240  Batch Loss: 2.6628  | train F1: 0.0078  | train precision: 0.0039  | train recall: 0.8313\n",
      "Epoch 001  Batch 017/10240  Batch Loss: 2.2965  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8315\n",
      "Epoch 001  Batch 018/10240  Batch Loss: 2.6572  | train F1: 0.0078  | train precision: 0.0039  | train recall: 0.8316\n",
      "Epoch 001  Batch 019/10240  Batch Loss: 1.9922  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8316\n",
      "Epoch 001  Batch 020/10240  Batch Loss: 3.1547  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8281\n",
      "Epoch 001  Batch 021/10240  Batch Loss: 2.4709  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8273\n",
      "Epoch 001  Batch 022/10240  Batch Loss: 2.4668  | train F1: 0.0078  | train precision: 0.0039  | train recall: 0.8297\n",
      "Epoch 001  Batch 023/10240  Batch Loss: 3.2659  | train F1: 0.0083  | train precision: 0.0042  | train recall: 0.8288\n",
      "Epoch 001  Batch 024/10240  Batch Loss: 2.3550  | train F1: 0.0082  | train precision: 0.0041  | train recall: 0.8286\n",
      "Epoch 001  Batch 025/10240  Batch Loss: 2.4908  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8341\n",
      "Epoch 001  Batch 026/10240  Batch Loss: 2.1674  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8377\n",
      "Epoch 001  Batch 027/10240  Batch Loss: 2.3635  | train F1: 0.0082  | train precision: 0.0041  | train recall: 0.8364\n",
      "Epoch 001  Batch 028/10240  Batch Loss: 2.2067  | train F1: 0.0080  | train precision: 0.0040  | train recall: 0.8376\n",
      "Epoch 001  Batch 029/10240  Batch Loss: 2.7262  | train F1: 0.0081  | train precision: 0.0041  | train recall: 0.8349\n",
      "Epoch 001  Batch 030/10240  Batch Loss: 2.6694  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8378\n",
      "Epoch 001  Batch 031/10240  Batch Loss: 2.3662  | train F1: 0.0083  | train precision: 0.0042  | train recall: 0.8381\n",
      "Epoch 001  Batch 032/10240  Batch Loss: 2.1290  | train F1: 0.0081  | train precision: 0.0041  | train recall: 0.8385\n",
      "Epoch 001  Batch 033/10240  Batch Loss: 2.2195  | train F1: 0.0081  | train precision: 0.0041  | train recall: 0.8401\n",
      "Epoch 001  Batch 034/10240  Batch Loss: 3.0260  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8390\n",
      "Epoch 001  Batch 035/10240  Batch Loss: 2.6366  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8373\n",
      "Epoch 001  Batch 036/10240  Batch Loss: 2.4493  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8386\n",
      "Epoch 001  Batch 037/10240  Batch Loss: 2.3664  | train F1: 0.0083  | train precision: 0.0042  | train recall: 0.8378\n",
      "Epoch 001  Batch 038/10240  Batch Loss: 2.9893  | train F1: 0.0084  | train precision: 0.0042  | train recall: 0.8339\n",
      "Epoch 001  Batch 039/10240  Batch Loss: 2.1389  | train F1: 0.0083  | train precision: 0.0042  | train recall: 0.8361\n",
      "Epoch 001  Batch 040/10240  Batch Loss: 2.0232  | train F1: 0.0081  | train precision: 0.0041  | train recall: 0.8362\n",
      "Epoch 001  Batch 041/10240  Batch Loss: 2.6085  | train F1: 0.0080  | train precision: 0.0040  | train recall: 0.8321\n",
      "Epoch 001  Batch 042/10240  Batch Loss: 2.1155  | train F1: 0.0079  | train precision: 0.0040  | train recall: 0.8326\n",
      "Epoch 001  Batch 043/10240  Batch Loss: 2.7753  | train F1: 0.0081  | train precision: 0.0041  | train recall: 0.8336\n",
      "Epoch 001  Batch 044/10240  Batch Loss: 2.1410  | train F1: 0.0080  | train precision: 0.0040  | train recall: 0.8341\n",
      "Epoch 001  Batch 045/10240  Batch Loss: 2.2540  | train F1: 0.0080  | train precision: 0.0040  | train recall: 0.8350\n",
      "Epoch 001  Batch 046/10240  Batch Loss: 2.1831  | train F1: 0.0079  | train precision: 0.0040  | train recall: 0.8359\n",
      "Epoch 001  Batch 047/10240  Batch Loss: 2.8240  | train F1: 0.0081  | train precision: 0.0041  | train recall: 0.8369\n",
      "Epoch 001  Batch 048/10240  Batch Loss: 2.1858  | train F1: 0.0079  | train precision: 0.0040  | train recall: 0.8353\n",
      "Epoch 001  Batch 049/10240  Batch Loss: 2.6661  | train F1: 0.0079  | train precision: 0.0039  | train recall: 0.8325\n",
      "Epoch 001  Batch 050/10240  Batch Loss: 2.3129  | train F1: 0.0079  | train precision: 0.0040  | train recall: 0.8347\n",
      "Epoch 001  Batch 051/10240  Batch Loss: 2.2434  | train F1: 0.0078  | train precision: 0.0039  | train recall: 0.8345\n",
      "Epoch 001  Batch 052/10240  Batch Loss: 2.1531  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8343\n",
      "Epoch 001  Batch 053/10240  Batch Loss: 2.1064  | train F1: 0.0076  | train precision: 0.0038  | train recall: 0.8345\n",
      "Epoch 001  Batch 054/10240  Batch Loss: 2.3908  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8370\n",
      "Epoch 001  Batch 055/10240  Batch Loss: 2.5384  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8366\n",
      "Epoch 001  Batch 056/10240  Batch Loss: 2.4808  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.8355\n",
      "Epoch 001  Batch 057/10240  Batch Loss: 2.2178  | train F1: 0.0076  | train precision: 0.0038  | train recall: 0.8356\n",
      "Epoch 001  Batch 058/10240  Batch Loss: 2.1165  | train F1: 0.0075  | train precision: 0.0038  | train recall: 0.8355\n",
      "Epoch 001  Batch 059/10240  Batch Loss: 1.9722  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8355\n",
      "Epoch 001  Batch 060/10240  Batch Loss: 2.3067  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8358\n",
      "Epoch 001  Batch 061/10240  Batch Loss: 2.1863  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8365\n",
      "Epoch 001  Batch 062/10240  Batch Loss: 2.5375  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8369\n",
      "Epoch 001  Batch 063/10240  Batch Loss: 3.0868  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8326\n",
      "Epoch 001  Batch 064/10240  Batch Loss: 1.9916  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.8326\n",
      "Epoch 001  Batch 065/10240  Batch Loss: 3.1996  | train F1: 0.0074  | train precision: 0.0037  | train recall: 0.8302\n",
      "Epoch 001  Batch 066/10240  Batch Loss: 1.9625  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.8302\n",
      "Epoch 001  Batch 067/10240  Batch Loss: 2.1681  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.8306\n",
      "Epoch 001  Batch 068/10240  Batch Loss: 2.7756  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.8313\n",
      "Epoch 001  Batch 069/10240  Batch Loss: 2.7013  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.8300\n",
      "Epoch 001  Batch 070/10240  Batch Loss: 2.4545  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.8297\n",
      "Epoch 001  Batch 071/10240  Batch Loss: 2.1848  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.8286\n",
      "Epoch 001  Batch 072/10240  Batch Loss: 2.9180  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.8263\n",
      "Epoch 001  Batch 073/10240  Batch Loss: 2.1660  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.8265\n",
      "Epoch 001  Batch 074/10240  Batch Loss: 2.1833  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8269\n",
      "Epoch 001  Batch 075/10240  Batch Loss: 2.2708  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8264\n",
      "Epoch 001  Batch 076/10240  Batch Loss: 2.2247  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8261\n",
      "Epoch 001  Batch 077/10240  Batch Loss: 2.3316  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.8273\n",
      "Epoch 001  Batch 078/10240  Batch Loss: 2.2216  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8287\n",
      "Epoch 001  Batch 079/10240  Batch Loss: 2.7225  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8273\n",
      "Epoch 001  Batch 080/10240  Batch Loss: 2.7603  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8250\n",
      "Epoch 001  Batch 081/10240  Batch Loss: 2.1009  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8246\n",
      "Epoch 001  Batch 082/10240  Batch Loss: 2.3225  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8243\n",
      "Epoch 001  Batch 083/10240  Batch Loss: 2.3161  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8233\n",
      "Epoch 001  Batch 084/10240  Batch Loss: 2.3790  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8240\n",
      "Epoch 001  Batch 085/10240  Batch Loss: 1.9743  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8242\n",
      "Epoch 001  Batch 086/10240  Batch Loss: 3.1946  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8219\n",
      "Epoch 001  Batch 087/10240  Batch Loss: 2.1981  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8216\n",
      "Epoch 001  Batch 088/10240  Batch Loss: 2.7751  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8228\n",
      "Epoch 001  Batch 089/10240  Batch Loss: 2.1748  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.8235\n",
      "Epoch 001  Batch 090/10240  Batch Loss: 2.2212  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8227\n",
      "Epoch 001  Batch 091/10240  Batch Loss: 2.2656  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8231\n",
      "Epoch 001  Batch 092/10240  Batch Loss: 1.9604  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8231\n",
      "Epoch 001  Batch 093/10240  Batch Loss: 2.7533  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8220\n",
      "Epoch 001  Batch 094/10240  Batch Loss: 2.0363  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8221\n",
      "Epoch 001  Batch 095/10240  Batch Loss: 1.9422  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8221\n",
      "Epoch 001  Batch 096/10240  Batch Loss: 2.1213  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8218\n",
      "Epoch 001  Batch 097/10240  Batch Loss: 2.1811  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8226\n",
      "Epoch 001  Batch 098/10240  Batch Loss: 2.0395  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8234\n",
      "Epoch 001  Batch 099/10240  Batch Loss: 2.0939  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8233\n",
      "Epoch 001  Batch 100/10240  Batch Loss: 2.0238  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8235\n",
      "Epoch 001  Batch 101/10240  Batch Loss: 2.3719  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8245\n",
      "Epoch 001  Batch 102/10240  Batch Loss: 2.1671  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8239\n",
      "Epoch 001  Batch 103/10240  Batch Loss: 2.4391  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8242\n",
      "Epoch 001  Batch 104/10240  Batch Loss: 3.1968  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8229\n",
      "Epoch 001  Batch 105/10240  Batch Loss: 2.1468  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8226\n",
      "Epoch 001  Batch 106/10240  Batch Loss: 2.4478  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8224\n",
      "Epoch 001  Batch 107/10240  Batch Loss: 2.6831  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8224\n",
      "Epoch 001  Batch 108/10240  Batch Loss: 2.1378  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8228\n",
      "Epoch 001  Batch 109/10240  Batch Loss: 2.8590  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8221\n",
      "Epoch 001  Batch 110/10240  Batch Loss: 3.6306  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8220\n",
      "Epoch 001  Batch 111/10240  Batch Loss: 2.6174  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.8225\n",
      "Epoch 001  Batch 112/10240  Batch Loss: 2.6384  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.8222\n",
      "Epoch 001  Batch 113/10240  Batch Loss: 2.2897  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8222\n",
      "Epoch 001  Batch 114/10240  Batch Loss: 1.9143  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8222\n",
      "Epoch 001  Batch 115/10240  Batch Loss: 2.4058  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8220\n",
      "Epoch 001  Batch 116/10240  Batch Loss: 2.0860  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8220\n",
      "Epoch 001  Batch 117/10240  Batch Loss: 2.1222  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8216\n",
      "Epoch 001  Batch 118/10240  Batch Loss: 3.4304  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8196\n",
      "Epoch 001  Batch 119/10240  Batch Loss: 2.4365  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8191\n",
      "Epoch 001  Batch 120/10240  Batch Loss: 3.0533  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8180\n",
      "Epoch 001  Batch 121/10240  Batch Loss: 1.8463  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8180\n",
      "Epoch 001  Batch 122/10240  Batch Loss: 2.3627  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8187\n",
      "Epoch 001  Batch 123/10240  Batch Loss: 2.1496  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8188\n",
      "Epoch 001  Batch 124/10240  Batch Loss: 2.5665  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8184\n",
      "Epoch 001  Batch 125/10240  Batch Loss: 2.0780  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8189\n",
      "Epoch 001  Batch 126/10240  Batch Loss: 2.1624  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8189\n",
      "Epoch 001  Batch 127/10240  Batch Loss: 1.9160  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8189\n",
      "Epoch 001  Batch 128/10240  Batch Loss: 2.7872  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8181\n",
      "Epoch 001  Batch 129/10240  Batch Loss: 2.2745  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8178\n",
      "Epoch 001  Batch 130/10240  Batch Loss: 2.1334  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8176\n",
      "Epoch 001  Batch 131/10240  Batch Loss: 1.9123  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8176\n",
      "Epoch 001  Batch 132/10240  Batch Loss: 1.9848  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8179\n",
      "Epoch 001  Batch 133/10240  Batch Loss: 2.3462  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8185\n",
      "Epoch 001  Batch 134/10240  Batch Loss: 1.8612  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8185\n",
      "Epoch 001  Batch 135/10240  Batch Loss: 3.4043  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8162\n",
      "Epoch 001  Batch 136/10240  Batch Loss: 2.5051  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8156\n",
      "Epoch 001  Batch 137/10240  Batch Loss: 1.8590  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8156\n",
      "Epoch 001  Batch 138/10240  Batch Loss: 2.7058  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8143\n",
      "Epoch 001  Batch 139/10240  Batch Loss: 2.5482  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8152\n",
      "Epoch 001  Batch 140/10240  Batch Loss: 1.9193  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8152\n",
      "Epoch 001  Batch 141/10240  Batch Loss: 1.9781  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8155\n",
      "Epoch 001  Batch 142/10240  Batch Loss: 2.7855  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8147\n",
      "Epoch 001  Batch 143/10240  Batch Loss: 2.0450  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8148\n",
      "Epoch 001  Batch 144/10240  Batch Loss: 2.4230  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8139\n",
      "Epoch 001  Batch 145/10240  Batch Loss: 1.8367  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8139\n",
      "Epoch 001  Batch 146/10240  Batch Loss: 2.3320  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8134\n",
      "Epoch 001  Batch 147/10240  Batch Loss: 2.6003  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8135\n",
      "Epoch 001  Batch 148/10240  Batch Loss: 2.1793  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8137\n",
      "Epoch 001  Batch 149/10240  Batch Loss: 2.5897  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8137\n",
      "Epoch 001  Batch 150/10240  Batch Loss: 2.2253  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8143\n",
      "Epoch 001  Batch 151/10240  Batch Loss: 2.3611  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8152\n",
      "Epoch 001  Batch 152/10240  Batch Loss: 2.1717  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8156\n",
      "Epoch 001  Batch 153/10240  Batch Loss: 1.9826  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8157\n",
      "Epoch 001  Batch 154/10240  Batch Loss: 2.2384  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8155\n",
      "Epoch 001  Batch 155/10240  Batch Loss: 1.9011  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8154\n",
      "Epoch 001  Batch 156/10240  Batch Loss: 2.2704  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8153\n",
      "Epoch 001  Batch 157/10240  Batch Loss: 2.3636  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8155\n",
      "Epoch 001  Batch 158/10240  Batch Loss: 3.3060  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8138\n",
      "Epoch 001  Batch 159/10240  Batch Loss: 2.2673  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8141\n",
      "Epoch 001  Batch 160/10240  Batch Loss: 2.2233  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8143\n",
      "Epoch 001  Batch 161/10240  Batch Loss: 1.8896  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8143\n",
      "Epoch 001  Batch 162/10240  Batch Loss: 2.4230  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8144\n",
      "Epoch 001  Batch 163/10240  Batch Loss: 2.1236  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8141\n",
      "Epoch 001  Batch 164/10240  Batch Loss: 2.0127  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8139\n",
      "Epoch 001  Batch 165/10240  Batch Loss: 2.3890  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8145\n",
      "Epoch 001  Batch 166/10240  Batch Loss: 2.3915  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8144\n",
      "Epoch 001  Batch 167/10240  Batch Loss: 2.3807  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8142\n",
      "Epoch 001  Batch 168/10240  Batch Loss: 3.0504  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8130\n",
      "Epoch 001  Batch 169/10240  Batch Loss: 2.2022  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8130\n",
      "Epoch 001  Batch 170/10240  Batch Loss: 2.1945  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8132\n",
      "Epoch 001  Batch 171/10240  Batch Loss: 3.2097  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8114\n",
      "Epoch 001  Batch 172/10240  Batch Loss: 2.2574  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8123\n",
      "Epoch 001  Batch 173/10240  Batch Loss: 2.0609  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8128\n",
      "Epoch 001  Batch 174/10240  Batch Loss: 2.6727  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8124\n",
      "Epoch 001  Batch 175/10240  Batch Loss: 2.1939  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8129\n",
      "Epoch 001  Batch 176/10240  Batch Loss: 3.2699  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8122\n",
      "Epoch 001  Batch 177/10240  Batch Loss: 2.7920  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8116\n",
      "Epoch 001  Batch 178/10240  Batch Loss: 1.8659  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8116\n",
      "Epoch 001  Batch 179/10240  Batch Loss: 1.8905  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8115\n",
      "Epoch 001  Batch 180/10240  Batch Loss: 2.2254  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8116\n",
      "Epoch 001  Batch 181/10240  Batch Loss: 2.0136  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8115\n",
      "Epoch 001  Batch 182/10240  Batch Loss: 2.4210  | train F1: 0.0065  | train precision: 0.0033  | train recall: 0.8116\n",
      "Epoch 001  Batch 183/10240  Batch Loss: 2.5646  | train F1: 0.0066  | train precision: 0.0033  | train recall: 0.8120\n",
      "Epoch 001  Batch 184/10240  Batch Loss: 3.2089  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8125\n",
      "Epoch 001  Batch 185/10240  Batch Loss: 2.4031  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8131\n",
      "Epoch 001  Batch 186/10240  Batch Loss: 2.1093  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8133\n",
      "Epoch 001  Batch 187/10240  Batch Loss: 3.3281  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8124\n",
      "Epoch 001  Batch 188/10240  Batch Loss: 2.1585  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8124\n",
      "Epoch 001  Batch 189/10240  Batch Loss: 2.2308  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8117\n",
      "Epoch 001  Batch 190/10240  Batch Loss: 2.3476  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8121\n",
      "Epoch 001  Batch 191/10240  Batch Loss: 2.0446  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8126\n",
      "Epoch 001  Batch 192/10240  Batch Loss: 2.7500  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8126\n",
      "Epoch 001  Batch 193/10240  Batch Loss: 2.2747  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8126\n",
      "Epoch 001  Batch 194/10240  Batch Loss: 1.8009  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8126\n",
      "Epoch 001  Batch 195/10240  Batch Loss: 2.0261  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8129\n",
      "Epoch 001  Batch 196/10240  Batch Loss: 2.6868  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8128\n",
      "Epoch 001  Batch 197/10240  Batch Loss: 1.9729  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8128\n",
      "Epoch 001  Batch 198/10240  Batch Loss: 2.2743  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8128\n",
      "Epoch 001  Batch 199/10240  Batch Loss: 2.7150  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8142\n",
      "Epoch 001  Batch 200/10240  Batch Loss: 2.3439  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8142\n",
      "Epoch 001  Batch 201/10240  Batch Loss: 2.3383  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8136\n",
      "Epoch 001  Batch 202/10240  Batch Loss: 2.9192  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8127\n",
      "Epoch 001  Batch 203/10240  Batch Loss: 1.8332  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8127\n",
      "Epoch 001  Batch 204/10240  Batch Loss: 2.2338  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8131\n",
      "Epoch 001  Batch 205/10240  Batch Loss: 1.8927  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8130\n",
      "Epoch 001  Batch 206/10240  Batch Loss: 1.8490  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8130\n",
      "Epoch 001  Batch 207/10240  Batch Loss: 2.1255  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8128\n",
      "Epoch 001  Batch 208/10240  Batch Loss: 2.7376  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8130\n",
      "Epoch 001  Batch 209/10240  Batch Loss: 2.0185  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8130\n",
      "Epoch 001  Batch 210/10240  Batch Loss: 2.3981  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8125\n",
      "Epoch 001  Batch 211/10240  Batch Loss: 1.7909  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8125\n",
      "Epoch 001  Batch 212/10240  Batch Loss: 2.6619  | train F1: 0.0067  | train precision: 0.0033  | train recall: 0.8118\n",
      "Epoch 001  Batch 213/10240  Batch Loss: 2.7015  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8128\n",
      "Epoch 001  Batch 214/10240  Batch Loss: 2.8400  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8127\n",
      "Epoch 001  Batch 215/10240  Batch Loss: 1.9887  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8127\n",
      "Epoch 001  Batch 216/10240  Batch Loss: 2.2902  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8132\n",
      "Epoch 001  Batch 217/10240  Batch Loss: 1.8620  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8131\n",
      "Epoch 001  Batch 218/10240  Batch Loss: 2.5147  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8132\n",
      "Epoch 001  Batch 219/10240  Batch Loss: 2.1692  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8133\n",
      "Epoch 001  Batch 220/10240  Batch Loss: 1.9552  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.8135\n",
      "Epoch 001  Batch 221/10240  Batch Loss: 3.0704  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8139\n",
      "Epoch 001  Batch 222/10240  Batch Loss: 3.1280  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8132\n",
      "Epoch 001  Batch 223/10240  Batch Loss: 2.0854  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8135\n",
      "Epoch 001  Batch 224/10240  Batch Loss: 2.2733  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8143\n",
      "Epoch 001  Batch 225/10240  Batch Loss: 2.0096  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8142\n",
      "Epoch 001  Batch 226/10240  Batch Loss: 2.3464  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8142\n",
      "Epoch 001  Batch 227/10240  Batch Loss: 2.5780  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8138\n",
      "Epoch 001  Batch 228/10240  Batch Loss: 1.8209  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8138\n",
      "Epoch 001  Batch 229/10240  Batch Loss: 2.2484  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8135\n",
      "Epoch 001  Batch 230/10240  Batch Loss: 2.0271  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8133\n",
      "Epoch 001  Batch 231/10240  Batch Loss: 3.1284  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8133\n",
      "Epoch 001  Batch 232/10240  Batch Loss: 1.9812  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8132\n",
      "Epoch 001  Batch 233/10240  Batch Loss: 2.8376  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8126\n",
      "Epoch 001  Batch 234/10240  Batch Loss: 2.2483  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8126\n",
      "Epoch 001  Batch 235/10240  Batch Loss: 1.8698  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8129\n",
      "Epoch 001  Batch 236/10240  Batch Loss: 2.2965  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8133\n",
      "Epoch 001  Batch 237/10240  Batch Loss: 2.9151  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8134\n",
      "Epoch 001  Batch 238/10240  Batch Loss: 2.6829  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8134\n",
      "Epoch 001  Batch 239/10240  Batch Loss: 2.3141  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8134\n",
      "Epoch 001  Batch 240/10240  Batch Loss: 2.5395  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8132\n",
      "Epoch 001  Batch 241/10240  Batch Loss: 3.0218  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8132\n",
      "Epoch 001  Batch 242/10240  Batch Loss: 1.8551  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8133\n",
      "Epoch 001  Batch 243/10240  Batch Loss: 2.5319  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8133\n",
      "Epoch 001  Batch 244/10240  Batch Loss: 2.1220  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8134\n",
      "Epoch 001  Batch 245/10240  Batch Loss: 2.3096  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8137\n",
      "Epoch 001  Batch 246/10240  Batch Loss: 2.3289  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8142\n",
      "Epoch 001  Batch 247/10240  Batch Loss: 1.7765  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8142\n",
      "Epoch 001  Batch 248/10240  Batch Loss: 3.0585  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8142\n",
      "Epoch 001  Batch 249/10240  Batch Loss: 2.2258  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8142\n",
      "Epoch 001  Batch 250/10240  Batch Loss: 1.9896  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8140\n",
      "Epoch 001  Batch 251/10240  Batch Loss: 3.1243  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8135\n",
      "Epoch 001  Batch 252/10240  Batch Loss: 2.0640  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8138\n",
      "Epoch 001  Batch 253/10240  Batch Loss: 2.4001  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8135\n",
      "Epoch 001  Batch 254/10240  Batch Loss: 2.7193  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8131\n",
      "Epoch 001  Batch 255/10240  Batch Loss: 1.9646  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8131\n",
      "Epoch 001  Batch 256/10240  Batch Loss: 2.3029  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8133\n",
      "Epoch 001  Batch 257/10240  Batch Loss: 2.6055  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8129\n",
      "Epoch 001  Batch 258/10240  Batch Loss: 1.8877  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8129\n",
      "Epoch 001  Batch 259/10240  Batch Loss: 2.0039  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8127\n",
      "Epoch 001  Batch 260/10240  Batch Loss: 2.7433  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8126\n",
      "Epoch 001  Batch 261/10240  Batch Loss: 1.8548  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8126\n",
      "Epoch 001  Batch 262/10240  Batch Loss: 2.3370  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8126\n",
      "Epoch 001  Batch 263/10240  Batch Loss: 1.8055  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8126\n",
      "Epoch 001  Batch 264/10240  Batch Loss: 2.6682  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8129\n",
      "Epoch 001  Batch 265/10240  Batch Loss: 2.4442  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8120\n",
      "Epoch 001  Batch 266/10240  Batch Loss: 1.8665  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8121\n",
      "Epoch 001  Batch 267/10240  Batch Loss: 2.4980  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8115\n",
      "Epoch 001  Batch 268/10240  Batch Loss: 1.8119  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8115\n",
      "Epoch 001  Batch 269/10240  Batch Loss: 2.2707  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8114\n",
      "Epoch 001  Batch 270/10240  Batch Loss: 2.1415  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8110\n",
      "Epoch 001  Batch 271/10240  Batch Loss: 2.2001  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8111\n",
      "Epoch 001  Batch 272/10240  Batch Loss: 1.8232  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8111\n",
      "Epoch 001  Batch 273/10240  Batch Loss: 2.3463  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8107\n",
      "Epoch 001  Batch 274/10240  Batch Loss: 1.9109  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8106\n",
      "Epoch 001  Batch 275/10240  Batch Loss: 3.1751  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8094\n",
      "Epoch 001  Batch 276/10240  Batch Loss: 1.9185  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8094\n",
      "Epoch 001  Batch 277/10240  Batch Loss: 1.7367  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8094\n",
      "Epoch 001  Batch 278/10240  Batch Loss: 3.0247  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8090\n",
      "Epoch 001  Batch 279/10240  Batch Loss: 2.0855  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8094\n",
      "Epoch 001  Batch 280/10240  Batch Loss: 2.3041  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8091\n",
      "Epoch 001  Batch 281/10240  Batch Loss: 1.7967  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8091\n",
      "Epoch 001  Batch 282/10240  Batch Loss: 2.4697  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8089\n",
      "Epoch 001  Batch 283/10240  Batch Loss: 1.8560  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8089\n",
      "Epoch 001  Batch 284/10240  Batch Loss: 2.8628  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8093\n",
      "Epoch 001  Batch 285/10240  Batch Loss: 1.8487  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8093\n",
      "Epoch 001  Batch 286/10240  Batch Loss: 2.0933  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8093\n",
      "Epoch 001  Batch 287/10240  Batch Loss: 1.7989  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8093\n",
      "Epoch 001  Batch 288/10240  Batch Loss: 1.9657  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8094\n",
      "Epoch 001  Batch 289/10240  Batch Loss: 2.1630  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8097\n",
      "Epoch 001  Batch 290/10240  Batch Loss: 2.5290  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8100\n",
      "Epoch 001  Batch 291/10240  Batch Loss: 2.5710  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8097\n",
      "Epoch 001  Batch 292/10240  Batch Loss: 2.7525  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8094\n",
      "Epoch 001  Batch 293/10240  Batch Loss: 2.0470  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8095\n",
      "Epoch 001  Batch 294/10240  Batch Loss: 2.4371  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8091\n",
      "Epoch 001  Batch 295/10240  Batch Loss: 3.0131  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8088\n",
      "Epoch 001  Batch 296/10240  Batch Loss: 2.4566  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8091\n",
      "Epoch 001  Batch 297/10240  Batch Loss: 2.0541  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8093\n",
      "Epoch 001  Batch 298/10240  Batch Loss: 3.1179  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8097\n",
      "Epoch 001  Batch 299/10240  Batch Loss: 2.3209  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8097\n",
      "Epoch 001  Batch 300/10240  Batch Loss: 2.3104  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8095\n",
      "Epoch 001  Batch 301/10240  Batch Loss: 1.7514  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8095\n",
      "Epoch 001  Batch 302/10240  Batch Loss: 1.9500  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8095\n",
      "Epoch 001  Batch 303/10240  Batch Loss: 1.8870  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8097\n",
      "Epoch 001  Batch 304/10240  Batch Loss: 3.3163  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8081\n",
      "Epoch 001  Batch 305/10240  Batch Loss: 1.9004  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 306/10240  Batch Loss: 1.8907  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 307/10240  Batch Loss: 2.1692  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 308/10240  Batch Loss: 1.8363  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 309/10240  Batch Loss: 1.8233  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 310/10240  Batch Loss: 1.8293  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 311/10240  Batch Loss: 2.2303  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8079\n",
      "Epoch 001  Batch 312/10240  Batch Loss: 1.9780  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8077\n",
      "Epoch 001  Batch 313/10240  Batch Loss: 2.9004  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8078\n",
      "Epoch 001  Batch 314/10240  Batch Loss: 2.2785  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8079\n",
      "Epoch 001  Batch 315/10240  Batch Loss: 2.4257  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8080\n",
      "Epoch 001  Batch 316/10240  Batch Loss: 2.3776  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8082\n",
      "Epoch 001  Batch 317/10240  Batch Loss: 2.1572  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8083\n",
      "Epoch 001  Batch 318/10240  Batch Loss: 2.7403  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8081\n",
      "Epoch 001  Batch 319/10240  Batch Loss: 1.8241  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8082\n",
      "Epoch 001  Batch 320/10240  Batch Loss: 1.9406  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8083\n",
      "Epoch 001  Batch 321/10240  Batch Loss: 2.1761  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8081\n",
      "Epoch 001  Batch 322/10240  Batch Loss: 2.6416  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8083\n",
      "Epoch 001  Batch 323/10240  Batch Loss: 3.0909  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8074\n",
      "Epoch 001  Batch 324/10240  Batch Loss: 2.4507  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8071\n",
      "Epoch 001  Batch 325/10240  Batch Loss: 2.3001  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8075\n",
      "Epoch 001  Batch 326/10240  Batch Loss: 3.0040  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8072\n",
      "Epoch 001  Batch 327/10240  Batch Loss: 1.9310  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8073\n",
      "Epoch 001  Batch 328/10240  Batch Loss: 2.5934  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8071\n",
      "Epoch 001  Batch 329/10240  Batch Loss: 1.8102  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8071\n",
      "Epoch 001  Batch 330/10240  Batch Loss: 1.8618  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8070\n",
      "Epoch 001  Batch 331/10240  Batch Loss: 2.1976  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8068\n",
      "Epoch 001  Batch 332/10240  Batch Loss: 2.2547  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8068\n",
      "Epoch 001  Batch 333/10240  Batch Loss: 2.6141  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8063\n",
      "Epoch 001  Batch 334/10240  Batch Loss: 2.6115  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8056\n",
      "Epoch 001  Batch 335/10240  Batch Loss: 2.7603  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8055\n",
      "Epoch 001  Batch 336/10240  Batch Loss: 2.1552  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8055\n",
      "Epoch 001  Batch 337/10240  Batch Loss: 1.8721  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8055\n",
      "Epoch 001  Batch 338/10240  Batch Loss: 2.8425  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8052\n",
      "Epoch 001  Batch 339/10240  Batch Loss: 2.2428  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8048\n",
      "Epoch 001  Batch 340/10240  Batch Loss: 2.1257  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8050\n",
      "Epoch 001  Batch 341/10240  Batch Loss: 2.2446  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8048\n",
      "Epoch 001  Batch 342/10240  Batch Loss: 2.1356  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8045\n",
      "Epoch 001  Batch 343/10240  Batch Loss: 2.1325  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8043\n",
      "Epoch 001  Batch 344/10240  Batch Loss: 1.9080  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8041\n",
      "Epoch 001  Batch 345/10240  Batch Loss: 2.1554  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8039\n",
      "Epoch 001  Batch 346/10240  Batch Loss: 2.3842  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.8038\n",
      "Epoch 001  Batch 347/10240  Batch Loss: 1.8315  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8038\n",
      "Epoch 001  Batch 348/10240  Batch Loss: 1.9063  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8038\n",
      "Epoch 001  Batch 349/10240  Batch Loss: 1.7435  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8038\n",
      "Epoch 001  Batch 350/10240  Batch Loss: 2.4119  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8037\n",
      "Epoch 001  Batch 351/10240  Batch Loss: 2.0249  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8035\n",
      "Epoch 001  Batch 352/10240  Batch Loss: 2.0257  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8036\n",
      "Epoch 001  Batch 353/10240  Batch Loss: 2.6772  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8033\n",
      "Epoch 001  Batch 354/10240  Batch Loss: 2.4289  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8033\n",
      "Epoch 001  Batch 355/10240  Batch Loss: 1.7060  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8033\n",
      "Epoch 001  Batch 356/10240  Batch Loss: 2.8029  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8029\n",
      "Epoch 001  Batch 357/10240  Batch Loss: 1.7529  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8029\n",
      "Epoch 001  Batch 358/10240  Batch Loss: 1.7390  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8029\n",
      "Epoch 001  Batch 359/10240  Batch Loss: 1.8339  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8028\n",
      "Epoch 001  Batch 360/10240  Batch Loss: 2.7639  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8019\n",
      "Epoch 001  Batch 361/10240  Batch Loss: 1.6869  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8019\n",
      "Epoch 001  Batch 362/10240  Batch Loss: 2.3186  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8019\n",
      "Epoch 001  Batch 363/10240  Batch Loss: 2.5821  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8013\n",
      "Epoch 001  Batch 364/10240  Batch Loss: 2.3526  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8009\n",
      "Epoch 001  Batch 365/10240  Batch Loss: 1.7001  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8009\n",
      "Epoch 001  Batch 366/10240  Batch Loss: 2.8289  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8006\n",
      "Epoch 001  Batch 367/10240  Batch Loss: 2.1685  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8008\n",
      "Epoch 001  Batch 368/10240  Batch Loss: 1.7763  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8009\n",
      "Epoch 001  Batch 369/10240  Batch Loss: 2.5455  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8014\n",
      "Epoch 001  Batch 370/10240  Batch Loss: 2.4029  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8010\n",
      "Epoch 001  Batch 371/10240  Batch Loss: 2.3240  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8013\n",
      "Epoch 001  Batch 372/10240  Batch Loss: 2.6779  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8017\n",
      "Epoch 001  Batch 373/10240  Batch Loss: 1.9959  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8017\n",
      "Epoch 001  Batch 374/10240  Batch Loss: 2.1910  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8017\n",
      "Epoch 001  Batch 375/10240  Batch Loss: 2.6957  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8012\n",
      "Epoch 001  Batch 376/10240  Batch Loss: 1.7125  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8012\n",
      "Epoch 001  Batch 377/10240  Batch Loss: 1.9865  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8013\n",
      "Epoch 001  Batch 378/10240  Batch Loss: 2.1398  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8016\n",
      "Epoch 001  Batch 379/10240  Batch Loss: 1.8989  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8015\n",
      "Epoch 001  Batch 380/10240  Batch Loss: 1.7443  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8016\n",
      "Epoch 001  Batch 381/10240  Batch Loss: 3.2470  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8011\n",
      "Epoch 001  Batch 382/10240  Batch Loss: 2.1170  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8012\n",
      "Epoch 001  Batch 383/10240  Batch Loss: 2.5648  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8012\n",
      "Epoch 001  Batch 384/10240  Batch Loss: 1.6950  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8012\n",
      "Epoch 001  Batch 385/10240  Batch Loss: 2.8031  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8007\n",
      "Epoch 001  Batch 386/10240  Batch Loss: 2.0154  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8006\n",
      "Epoch 001  Batch 387/10240  Batch Loss: 1.8731  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8007\n",
      "Epoch 001  Batch 388/10240  Batch Loss: 1.9409  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8006\n",
      "Epoch 001  Batch 389/10240  Batch Loss: 2.6533  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7999\n",
      "Epoch 001  Batch 390/10240  Batch Loss: 1.7528  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7999\n",
      "Epoch 001  Batch 391/10240  Batch Loss: 2.1748  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7997\n",
      "Epoch 001  Batch 392/10240  Batch Loss: 2.1117  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8000\n",
      "Epoch 001  Batch 393/10240  Batch Loss: 1.8335  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7999\n",
      "Epoch 001  Batch 394/10240  Batch Loss: 1.9980  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8001\n",
      "Epoch 001  Batch 395/10240  Batch Loss: 1.6667  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8001\n",
      "Epoch 001  Batch 396/10240  Batch Loss: 2.1464  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.8001\n",
      "Epoch 001  Batch 397/10240  Batch Loss: 1.6906  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.8002\n",
      "Epoch 001  Batch 398/10240  Batch Loss: 1.6697  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8002\n",
      "Epoch 001  Batch 399/10240  Batch Loss: 1.7121  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8002\n",
      "Epoch 001  Batch 400/10240  Batch Loss: 2.0646  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8001\n",
      "Epoch 001  Batch 401/10240  Batch Loss: 1.7885  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8002\n",
      "Epoch 001  Batch 402/10240  Batch Loss: 2.1082  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8003\n",
      "Epoch 001  Batch 403/10240  Batch Loss: 1.8983  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8003\n",
      "Epoch 001  Batch 404/10240  Batch Loss: 1.7663  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8003\n",
      "Epoch 001  Batch 405/10240  Batch Loss: 2.1529  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8003\n",
      "Epoch 001  Batch 406/10240  Batch Loss: 1.7413  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8003\n",
      "Epoch 001  Batch 407/10240  Batch Loss: 2.5562  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8004\n",
      "Epoch 001  Batch 408/10240  Batch Loss: 2.1257  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8005\n",
      "Epoch 001  Batch 409/10240  Batch Loss: 2.1931  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8006\n",
      "Epoch 001  Batch 410/10240  Batch Loss: 1.6625  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8006\n",
      "Epoch 001  Batch 411/10240  Batch Loss: 1.6848  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8006\n",
      "Epoch 001  Batch 412/10240  Batch Loss: 2.2024  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8004\n",
      "Epoch 001  Batch 413/10240  Batch Loss: 1.8926  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8004\n",
      "Epoch 001  Batch 414/10240  Batch Loss: 2.7535  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.8000\n",
      "Epoch 001  Batch 415/10240  Batch Loss: 2.8419  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7996\n",
      "Epoch 001  Batch 416/10240  Batch Loss: 2.7528  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7993\n",
      "Epoch 001  Batch 417/10240  Batch Loss: 1.9165  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7992\n",
      "Epoch 001  Batch 418/10240  Batch Loss: 1.9725  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7991\n",
      "Epoch 001  Batch 419/10240  Batch Loss: 2.0176  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7992\n",
      "Epoch 001  Batch 420/10240  Batch Loss: 2.0491  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7993\n",
      "Epoch 001  Batch 421/10240  Batch Loss: 2.2006  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7993\n",
      "Epoch 001  Batch 422/10240  Batch Loss: 2.1345  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7992\n",
      "Epoch 001  Batch 423/10240  Batch Loss: 1.7494  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7992\n",
      "Epoch 001  Batch 424/10240  Batch Loss: 2.9457  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7991\n",
      "Epoch 001  Batch 425/10240  Batch Loss: 1.7794  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7991\n",
      "Epoch 001  Batch 426/10240  Batch Loss: 2.1327  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7994\n",
      "Epoch 001  Batch 427/10240  Batch Loss: 2.0842  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7993\n",
      "Epoch 001  Batch 428/10240  Batch Loss: 1.8452  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7994\n",
      "Epoch 001  Batch 429/10240  Batch Loss: 2.0214  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7992\n",
      "Epoch 001  Batch 430/10240  Batch Loss: 2.6553  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7986\n",
      "Epoch 001  Batch 431/10240  Batch Loss: 2.0462  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7986\n",
      "Epoch 001  Batch 432/10240  Batch Loss: 1.9230  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7987\n",
      "Epoch 001  Batch 433/10240  Batch Loss: 2.3459  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7990\n",
      "Epoch 001  Batch 434/10240  Batch Loss: 1.8178  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7990\n",
      "Epoch 001  Batch 435/10240  Batch Loss: 1.8196  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7990\n",
      "Epoch 001  Batch 436/10240  Batch Loss: 1.9776  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7990\n",
      "Epoch 001  Batch 437/10240  Batch Loss: 2.0623  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7989\n",
      "Epoch 001  Batch 438/10240  Batch Loss: 2.8574  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 439/10240  Batch Loss: 2.5267  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7987\n",
      "Epoch 001  Batch 440/10240  Batch Loss: 1.8598  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 441/10240  Batch Loss: 2.3595  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7989\n",
      "Epoch 001  Batch 442/10240  Batch Loss: 1.7228  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7989\n",
      "Epoch 001  Batch 443/10240  Batch Loss: 2.2653  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 444/10240  Batch Loss: 2.3298  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7985\n",
      "Epoch 001  Batch 445/10240  Batch Loss: 2.0811  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 446/10240  Batch Loss: 2.3336  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 447/10240  Batch Loss: 2.0442  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 448/10240  Batch Loss: 1.7552  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 449/10240  Batch Loss: 1.7434  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7988\n",
      "Epoch 001  Batch 450/10240  Batch Loss: 1.9014  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7987\n",
      "Epoch 001  Batch 451/10240  Batch Loss: 1.6545  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7987\n",
      "Epoch 001  Batch 452/10240  Batch Loss: 2.5911  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7984\n",
      "Epoch 001  Batch 453/10240  Batch Loss: 2.7692  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7981\n",
      "Epoch 001  Batch 454/10240  Batch Loss: 2.5226  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7978\n",
      "Epoch 001  Batch 455/10240  Batch Loss: 1.6143  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7978\n",
      "Epoch 001  Batch 456/10240  Batch Loss: 1.9428  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7979\n",
      "Epoch 001  Batch 457/10240  Batch Loss: 1.7057  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7979\n",
      "Epoch 001  Batch 458/10240  Batch Loss: 2.5022  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7975\n",
      "Epoch 001  Batch 459/10240  Batch Loss: 2.2156  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7972\n",
      "Epoch 001  Batch 460/10240  Batch Loss: 2.5359  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7969\n",
      "Epoch 001  Batch 461/10240  Batch Loss: 2.4377  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7964\n",
      "Epoch 001  Batch 462/10240  Batch Loss: 1.7646  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7965\n",
      "Epoch 001  Batch 463/10240  Batch Loss: 1.8446  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7965\n",
      "Epoch 001  Batch 464/10240  Batch Loss: 2.2877  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7964\n",
      "Epoch 001  Batch 465/10240  Batch Loss: 2.0043  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7961\n",
      "Epoch 001  Batch 466/10240  Batch Loss: 2.0342  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7959\n",
      "Epoch 001  Batch 467/10240  Batch Loss: 1.9636  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7958\n",
      "Epoch 001  Batch 468/10240  Batch Loss: 2.4634  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7955\n",
      "Epoch 001  Batch 469/10240  Batch Loss: 3.2139  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7948\n",
      "Epoch 001  Batch 470/10240  Batch Loss: 2.4691  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7944\n",
      "Epoch 001  Batch 471/10240  Batch Loss: 2.8371  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7944\n",
      "Epoch 001  Batch 472/10240  Batch Loss: 2.0999  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7942\n",
      "Epoch 001  Batch 473/10240  Batch Loss: 1.6160  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7942\n",
      "Epoch 001  Batch 474/10240  Batch Loss: 2.1706  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7943\n",
      "Epoch 001  Batch 475/10240  Batch Loss: 2.2265  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7944\n",
      "Epoch 001  Batch 476/10240  Batch Loss: 1.8928  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7943\n",
      "Epoch 001  Batch 477/10240  Batch Loss: 2.6180  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7939\n",
      "Epoch 001  Batch 478/10240  Batch Loss: 1.6605  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7939\n",
      "Epoch 001  Batch 479/10240  Batch Loss: 2.1872  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7936\n",
      "Epoch 001  Batch 480/10240  Batch Loss: 1.6089  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7936\n",
      "Epoch 001  Batch 481/10240  Batch Loss: 1.6596  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7936\n",
      "Epoch 001  Batch 482/10240  Batch Loss: 1.9259  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7937\n",
      "Epoch 001  Batch 483/10240  Batch Loss: 2.5777  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7937\n",
      "Epoch 001  Batch 484/10240  Batch Loss: 2.0056  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7936\n",
      "Epoch 001  Batch 485/10240  Batch Loss: 1.6764  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7936\n",
      "Epoch 001  Batch 486/10240  Batch Loss: 1.6453  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7936\n",
      "Epoch 001  Batch 487/10240  Batch Loss: 2.0307  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7937\n",
      "Epoch 001  Batch 488/10240  Batch Loss: 1.8359  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7938\n",
      "Epoch 001  Batch 489/10240  Batch Loss: 2.2052  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7938\n",
      "Epoch 001  Batch 490/10240  Batch Loss: 1.9915  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7937\n",
      "Epoch 001  Batch 491/10240  Batch Loss: 2.0778  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7937\n",
      "Epoch 001  Batch 492/10240  Batch Loss: 1.5828  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7937\n",
      "Epoch 001  Batch 493/10240  Batch Loss: 2.7977  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7934\n",
      "Epoch 001  Batch 494/10240  Batch Loss: 2.8903  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7925\n",
      "Epoch 001  Batch 495/10240  Batch Loss: 4.0775  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7915\n",
      "Epoch 001  Batch 496/10240  Batch Loss: 2.2298  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7913\n",
      "Epoch 001  Batch 497/10240  Batch Loss: 2.9654  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7910\n",
      "Epoch 001  Batch 498/10240  Batch Loss: 1.8057  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7909\n",
      "Epoch 001  Batch 499/10240  Batch Loss: 2.6446  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7910\n",
      "Epoch 001  Batch 500/10240  Batch Loss: 1.6104  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7910\n",
      "Epoch 001  Batch 501/10240  Batch Loss: 2.0721  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7909\n",
      "Epoch 001  Batch 502/10240  Batch Loss: 1.7711  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7909\n",
      "Epoch 001  Batch 503/10240  Batch Loss: 2.2462  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7907\n",
      "Epoch 001  Batch 504/10240  Batch Loss: 3.7680  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7900\n",
      "Epoch 001  Batch 505/10240  Batch Loss: 2.1560  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7899\n",
      "Epoch 001  Batch 506/10240  Batch Loss: 2.4525  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7900\n",
      "Epoch 001  Batch 507/10240  Batch Loss: 2.1636  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7900\n",
      "Epoch 001  Batch 508/10240  Batch Loss: 1.5811  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7900\n",
      "Epoch 001  Batch 509/10240  Batch Loss: 1.7961  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7902\n",
      "Epoch 001  Batch 510/10240  Batch Loss: 1.7432  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7903\n",
      "Epoch 001  Batch 511/10240  Batch Loss: 2.0527  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7904\n",
      "Epoch 001  Batch 512/10240  Batch Loss: 1.9296  | train F1: 0.0067  | train precision: 0.0034  | train recall: 0.7905\n",
      "Epoch 001  Batch 513/10240  Batch Loss: 2.0343  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7906\n",
      "Epoch 001  Batch 514/10240  Batch Loss: 2.0483  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7906\n",
      "Epoch 001  Batch 515/10240  Batch Loss: 2.9265  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7911\n",
      "Epoch 001  Batch 516/10240  Batch Loss: 2.4686  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7909\n",
      "Epoch 001  Batch 517/10240  Batch Loss: 1.5841  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7909\n",
      "Epoch 001  Batch 518/10240  Batch Loss: 2.3234  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7908\n",
      "Epoch 001  Batch 519/10240  Batch Loss: 2.8807  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7905\n",
      "Epoch 001  Batch 520/10240  Batch Loss: 1.5806  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7905\n",
      "Epoch 001  Batch 521/10240  Batch Loss: 1.7597  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7906\n",
      "Epoch 001  Batch 522/10240  Batch Loss: 1.8728  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7908\n",
      "Epoch 001  Batch 523/10240  Batch Loss: 1.8310  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7908\n",
      "Epoch 001  Batch 524/10240  Batch Loss: 1.9007  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7907\n",
      "Epoch 001  Batch 525/10240  Batch Loss: 1.6284  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7907\n",
      "Epoch 001  Batch 526/10240  Batch Loss: 2.9257  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7901\n",
      "Epoch 001  Batch 527/10240  Batch Loss: 2.2809  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7900\n",
      "Epoch 001  Batch 528/10240  Batch Loss: 1.8516  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7899\n",
      "Epoch 001  Batch 529/10240  Batch Loss: 2.8156  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7893\n",
      "Epoch 001  Batch 530/10240  Batch Loss: 2.1429  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7893\n",
      "Epoch 001  Batch 531/10240  Batch Loss: 2.8397  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7892\n",
      "Epoch 001  Batch 532/10240  Batch Loss: 1.9315  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7894\n",
      "Epoch 001  Batch 533/10240  Batch Loss: 1.6453  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7894\n",
      "Epoch 001  Batch 534/10240  Batch Loss: 2.0523  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7896\n",
      "Epoch 001  Batch 535/10240  Batch Loss: 2.5763  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7897\n",
      "Epoch 001  Batch 536/10240  Batch Loss: 1.5615  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7897\n",
      "Epoch 001  Batch 537/10240  Batch Loss: 2.3618  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7896\n",
      "Epoch 001  Batch 538/10240  Batch Loss: 1.6706  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7896\n",
      "Epoch 001  Batch 539/10240  Batch Loss: 2.4403  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7893\n",
      "Epoch 001  Batch 540/10240  Batch Loss: 2.6590  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7890\n",
      "Epoch 001  Batch 541/10240  Batch Loss: 2.4769  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7887\n",
      "Epoch 001  Batch 542/10240  Batch Loss: 2.5140  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7884\n",
      "Epoch 001  Batch 543/10240  Batch Loss: 1.8045  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7885\n",
      "Epoch 001  Batch 544/10240  Batch Loss: 1.8231  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7887\n",
      "Epoch 001  Batch 545/10240  Batch Loss: 2.1422  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7887\n",
      "Epoch 001  Batch 546/10240  Batch Loss: 1.5347  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7887\n",
      "Epoch 001  Batch 547/10240  Batch Loss: 2.7813  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7886\n",
      "Epoch 001  Batch 548/10240  Batch Loss: 1.5673  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7886\n",
      "Epoch 001  Batch 549/10240  Batch Loss: 2.4496  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7886\n",
      "Epoch 001  Batch 550/10240  Batch Loss: 2.1421  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7882\n",
      "Epoch 001  Batch 551/10240  Batch Loss: 1.5264  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7882\n",
      "Epoch 001  Batch 552/10240  Batch Loss: 1.7165  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7882\n",
      "Epoch 001  Batch 553/10240  Batch Loss: 2.3733  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7882\n",
      "Epoch 001  Batch 554/10240  Batch Loss: 1.5350  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7882\n",
      "Epoch 001  Batch 555/10240  Batch Loss: 3.3719  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7874\n",
      "Epoch 001  Batch 556/10240  Batch Loss: 1.6561  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7875\n",
      "Epoch 001  Batch 557/10240  Batch Loss: 1.6994  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7876\n",
      "Epoch 001  Batch 558/10240  Batch Loss: 1.6631  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7876\n",
      "Epoch 001  Batch 559/10240  Batch Loss: 1.8191  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7877\n",
      "Epoch 001  Batch 560/10240  Batch Loss: 2.6810  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7875\n",
      "Epoch 001  Batch 561/10240  Batch Loss: 2.8916  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7870\n",
      "Epoch 001  Batch 562/10240  Batch Loss: 1.9660  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7870\n",
      "Epoch 001  Batch 563/10240  Batch Loss: 1.9602  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7870\n",
      "Epoch 001  Batch 564/10240  Batch Loss: 2.0697  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7869\n",
      "Epoch 001  Batch 565/10240  Batch Loss: 2.2380  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7867\n",
      "Epoch 001  Batch 566/10240  Batch Loss: 2.1422  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7865\n",
      "Epoch 001  Batch 567/10240  Batch Loss: 1.6116  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7865\n",
      "Epoch 001  Batch 568/10240  Batch Loss: 2.3265  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7865\n",
      "Epoch 001  Batch 569/10240  Batch Loss: 2.9036  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7862\n",
      "Epoch 001  Batch 570/10240  Batch Loss: 1.6036  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7862\n",
      "Epoch 001  Batch 571/10240  Batch Loss: 2.5818  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7859\n",
      "Epoch 001  Batch 572/10240  Batch Loss: 2.0871  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7858\n",
      "Epoch 001  Batch 573/10240  Batch Loss: 1.6077  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7858\n",
      "Epoch 001  Batch 574/10240  Batch Loss: 2.2235  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7856\n",
      "Epoch 001  Batch 575/10240  Batch Loss: 2.2789  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7856\n",
      "Epoch 001  Batch 576/10240  Batch Loss: 1.5624  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7857\n",
      "Epoch 001  Batch 577/10240  Batch Loss: 1.9697  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7857\n",
      "Epoch 001  Batch 578/10240  Batch Loss: 3.3349  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7853\n",
      "Epoch 001  Batch 579/10240  Batch Loss: 1.8748  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7853\n",
      "Epoch 001  Batch 580/10240  Batch Loss: 1.8479  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7851\n",
      "Epoch 001  Batch 581/10240  Batch Loss: 2.1060  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7852\n",
      "Epoch 001  Batch 582/10240  Batch Loss: 2.8119  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7846\n",
      "Epoch 001  Batch 583/10240  Batch Loss: 2.5374  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7846\n",
      "Epoch 001  Batch 584/10240  Batch Loss: 2.4276  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7843\n",
      "Epoch 001  Batch 585/10240  Batch Loss: 2.1091  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7843\n",
      "Epoch 001  Batch 586/10240  Batch Loss: 1.7867  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7845\n",
      "Epoch 001  Batch 587/10240  Batch Loss: 3.3771  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.7843\n",
      "Epoch 001  Batch 588/10240  Batch Loss: 1.6120  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7843\n",
      "Epoch 001  Batch 589/10240  Batch Loss: 2.1229  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7842\n",
      "Epoch 001  Batch 590/10240  Batch Loss: 2.3779  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.7841\n",
      "Epoch 001  Batch 591/10240  Batch Loss: 1.5572  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7841\n",
      "Epoch 001  Batch 592/10240  Batch Loss: 2.1303  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7841\n",
      "Epoch 001  Batch 593/10240  Batch Loss: 1.6667  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7840\n",
      "Epoch 001  Batch 594/10240  Batch Loss: 1.9699  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7840\n",
      "Epoch 001  Batch 595/10240  Batch Loss: 1.5882  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7839\n",
      "Epoch 001  Batch 596/10240  Batch Loss: 1.5523  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7839\n",
      "Epoch 001  Batch 597/10240  Batch Loss: 1.6681  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7840\n",
      "Epoch 001  Batch 598/10240  Batch Loss: 2.1894  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7842\n",
      "Epoch 001  Batch 599/10240  Batch Loss: 2.1751  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7846\n",
      "Epoch 001  Batch 600/10240  Batch Loss: 1.7634  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7846\n",
      "Epoch 001  Batch 601/10240  Batch Loss: 1.8179  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 602/10240  Batch Loss: 1.8669  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 603/10240  Batch Loss: 1.9783  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 604/10240  Batch Loss: 2.1122  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 605/10240  Batch Loss: 1.6765  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 606/10240  Batch Loss: 1.5742  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 607/10240  Batch Loss: 1.5108  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7848\n",
      "Epoch 001  Batch 608/10240  Batch Loss: 2.7404  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7847\n",
      "Epoch 001  Batch 609/10240  Batch Loss: 2.0115  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7846\n",
      "Epoch 001  Batch 610/10240  Batch Loss: 2.2277  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7847\n",
      "Epoch 001  Batch 611/10240  Batch Loss: 1.9356  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7847\n",
      "Epoch 001  Batch 612/10240  Batch Loss: 2.4141  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7847\n",
      "Epoch 001  Batch 613/10240  Batch Loss: 2.3071  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7846\n",
      "Epoch 001  Batch 614/10240  Batch Loss: 2.7643  | train F1: 0.0068  | train precision: 0.0034  | train recall: 0.7842\n",
      "Epoch 001  Batch 615/10240  Batch Loss: 2.6709  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.7841\n",
      "Epoch 001  Batch 616/10240  Batch Loss: 1.8004  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.7843\n",
      "Epoch 001  Batch 617/10240  Batch Loss: 2.4584  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7842\n",
      "Epoch 001  Batch 618/10240  Batch Loss: 2.3294  | train F1: 0.0069  | train precision: 0.0034  | train recall: 0.7838\n",
      "Epoch 001  Batch 619/10240  Batch Loss: 3.1275  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7834\n",
      "Epoch 001  Batch 620/10240  Batch Loss: 2.2655  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7835\n",
      "Epoch 001  Batch 621/10240  Batch Loss: 3.1487  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7837\n",
      "Epoch 001  Batch 622/10240  Batch Loss: 2.1686  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7838\n",
      "Epoch 001  Batch 623/10240  Batch Loss: 1.5551  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7838\n",
      "Epoch 001  Batch 624/10240  Batch Loss: 2.9490  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7834\n",
      "Epoch 001  Batch 625/10240  Batch Loss: 4.3683  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7827\n",
      "Epoch 001  Batch 626/10240  Batch Loss: 2.0109  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7826\n",
      "Epoch 001  Batch 627/10240  Batch Loss: 1.6637  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7826\n",
      "Epoch 001  Batch 628/10240  Batch Loss: 1.6850  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7824\n",
      "Epoch 001  Batch 629/10240  Batch Loss: 2.2445  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7823\n",
      "Epoch 001  Batch 630/10240  Batch Loss: 1.9397  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7824\n",
      "Epoch 001  Batch 631/10240  Batch Loss: 2.4101  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7819\n",
      "Epoch 001  Batch 632/10240  Batch Loss: 1.5319  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7819\n",
      "Epoch 001  Batch 633/10240  Batch Loss: 1.8457  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7819\n",
      "Epoch 001  Batch 634/10240  Batch Loss: 1.5870  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7820\n",
      "Epoch 001  Batch 635/10240  Batch Loss: 2.5324  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7815\n",
      "Epoch 001  Batch 636/10240  Batch Loss: 2.6028  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7814\n",
      "Epoch 001  Batch 637/10240  Batch Loss: 1.4660  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7814\n",
      "Epoch 001  Batch 638/10240  Batch Loss: 1.9766  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7813\n",
      "Epoch 001  Batch 639/10240  Batch Loss: 1.7542  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7812\n",
      "Epoch 001  Batch 640/10240  Batch Loss: 1.9314  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7813\n",
      "Epoch 001  Batch 641/10240  Batch Loss: 1.8581  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7814\n",
      "Epoch 001  Batch 642/10240  Batch Loss: 2.2900  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7813\n",
      "Epoch 001  Batch 643/10240  Batch Loss: 1.8907  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7813\n",
      "Epoch 001  Batch 644/10240  Batch Loss: 1.6573  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7813\n",
      "Epoch 001  Batch 645/10240  Batch Loss: 2.4589  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7814\n",
      "Epoch 001  Batch 646/10240  Batch Loss: 2.7288  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7812\n",
      "Epoch 001  Batch 647/10240  Batch Loss: 1.6807  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7812\n",
      "Epoch 001  Batch 648/10240  Batch Loss: 3.4248  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7806\n",
      "Epoch 001  Batch 649/10240  Batch Loss: 2.5511  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7804\n",
      "Epoch 001  Batch 650/10240  Batch Loss: 1.5242  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7805\n",
      "Epoch 001  Batch 651/10240  Batch Loss: 1.4662  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7805\n",
      "Epoch 001  Batch 652/10240  Batch Loss: 2.1382  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7805\n",
      "Epoch 001  Batch 653/10240  Batch Loss: 2.6654  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7803\n",
      "Epoch 001  Batch 654/10240  Batch Loss: 2.2193  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7801\n",
      "Epoch 001  Batch 655/10240  Batch Loss: 1.6903  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7800\n",
      "Epoch 001  Batch 656/10240  Batch Loss: 1.7314  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7801\n",
      "Epoch 001  Batch 657/10240  Batch Loss: 2.7088  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7803\n",
      "Epoch 001  Batch 658/10240  Batch Loss: 1.5030  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7803\n",
      "Epoch 001  Batch 659/10240  Batch Loss: 2.4124  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7799\n",
      "Epoch 001  Batch 660/10240  Batch Loss: 1.5532  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7799\n",
      "Epoch 001  Batch 661/10240  Batch Loss: 1.6165  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7798\n",
      "Epoch 001  Batch 662/10240  Batch Loss: 1.4953  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7798\n",
      "Epoch 001  Batch 663/10240  Batch Loss: 1.5063  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7798\n",
      "Epoch 001  Batch 664/10240  Batch Loss: 2.3674  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7797\n",
      "Epoch 001  Batch 665/10240  Batch Loss: 2.6245  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7797\n",
      "Epoch 001  Batch 666/10240  Batch Loss: 1.6717  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7797\n",
      "Epoch 001  Batch 667/10240  Batch Loss: 3.1093  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7793\n",
      "Epoch 001  Batch 668/10240  Batch Loss: 1.8754  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7793\n",
      "Epoch 001  Batch 669/10240  Batch Loss: 2.7104  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7791\n",
      "Epoch 001  Batch 670/10240  Batch Loss: 2.4217  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7792\n",
      "Epoch 001  Batch 671/10240  Batch Loss: 1.5231  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7792\n",
      "Epoch 001  Batch 672/10240  Batch Loss: 1.9351  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7793\n",
      "Epoch 001  Batch 673/10240  Batch Loss: 2.1065  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7794\n",
      "Epoch 001  Batch 674/10240  Batch Loss: 1.8264  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7794\n",
      "Epoch 001  Batch 675/10240  Batch Loss: 1.9235  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7792\n",
      "Epoch 001  Batch 676/10240  Batch Loss: 1.9413  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7791\n",
      "Epoch 001  Batch 677/10240  Batch Loss: 2.7699  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7786\n",
      "Epoch 001  Batch 678/10240  Batch Loss: 1.5792  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7786\n",
      "Epoch 001  Batch 679/10240  Batch Loss: 1.4703  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7786\n",
      "Epoch 001  Batch 680/10240  Batch Loss: 2.3079  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7782\n",
      "Epoch 001  Batch 681/10240  Batch Loss: 2.3276  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7783\n",
      "Epoch 001  Batch 682/10240  Batch Loss: 2.7151  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7777\n",
      "Epoch 001  Batch 683/10240  Batch Loss: 1.6465  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7777\n",
      "Epoch 001  Batch 684/10240  Batch Loss: 1.5161  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7777\n",
      "Epoch 001  Batch 685/10240  Batch Loss: 1.5928  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7777\n",
      "Epoch 001  Batch 686/10240  Batch Loss: 2.3903  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7781\n",
      "Epoch 001  Batch 687/10240  Batch Loss: 1.5304  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7781\n",
      "Epoch 001  Batch 688/10240  Batch Loss: 1.7991  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7783\n",
      "Epoch 001  Batch 689/10240  Batch Loss: 1.7561  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7783\n",
      "Epoch 001  Batch 690/10240  Batch Loss: 2.2405  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7782\n",
      "Epoch 001  Batch 691/10240  Batch Loss: 2.3368  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7781\n",
      "Epoch 001  Batch 692/10240  Batch Loss: 2.1250  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7780\n",
      "Epoch 001  Batch 693/10240  Batch Loss: 1.8314  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7780\n",
      "Epoch 001  Batch 694/10240  Batch Loss: 1.8108  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7780\n",
      "Epoch 001  Batch 695/10240  Batch Loss: 1.9174  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7777\n",
      "Epoch 001  Batch 696/10240  Batch Loss: 2.1509  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7776\n",
      "Epoch 001  Batch 697/10240  Batch Loss: 1.7710  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7776\n",
      "Epoch 001  Batch 698/10240  Batch Loss: 1.8876  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7775\n",
      "Epoch 001  Batch 699/10240  Batch Loss: 1.7692  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7776\n",
      "Epoch 001  Batch 700/10240  Batch Loss: 1.5111  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7776\n",
      "Epoch 001  Batch 701/10240  Batch Loss: 3.0926  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7775\n",
      "Epoch 001  Batch 702/10240  Batch Loss: 1.8298  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7776\n",
      "Epoch 001  Batch 703/10240  Batch Loss: 2.6081  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7773\n",
      "Epoch 001  Batch 704/10240  Batch Loss: 1.4696  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7773\n",
      "Epoch 001  Batch 705/10240  Batch Loss: 1.4869  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7773\n",
      "Epoch 001  Batch 706/10240  Batch Loss: 1.7514  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7774\n",
      "Epoch 001  Batch 707/10240  Batch Loss: 2.1809  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7775\n",
      "Epoch 001  Batch 708/10240  Batch Loss: 2.4411  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7774\n",
      "Epoch 001  Batch 709/10240  Batch Loss: 2.3357  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7773\n",
      "Epoch 001  Batch 710/10240  Batch Loss: 2.1182  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7770\n",
      "Epoch 001  Batch 711/10240  Batch Loss: 1.6454  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7770\n",
      "Epoch 001  Batch 712/10240  Batch Loss: 2.2627  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7768\n",
      "Epoch 001  Batch 713/10240  Batch Loss: 2.1364  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7767\n",
      "Epoch 001  Batch 714/10240  Batch Loss: 1.7895  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7767\n",
      "Epoch 001  Batch 715/10240  Batch Loss: 3.0442  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7762\n",
      "Epoch 001  Batch 716/10240  Batch Loss: 1.4675  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7762\n",
      "Epoch 001  Batch 717/10240  Batch Loss: 1.6045  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7763\n",
      "Epoch 001  Batch 718/10240  Batch Loss: 3.4797  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7760\n",
      "Epoch 001  Batch 719/10240  Batch Loss: 2.0036  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7761\n",
      "Epoch 001  Batch 720/10240  Batch Loss: 1.8962  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7760\n",
      "Epoch 001  Batch 721/10240  Batch Loss: 1.6579  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7760\n",
      "Epoch 001  Batch 722/10240  Batch Loss: 2.0691  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7758\n",
      "Epoch 001  Batch 723/10240  Batch Loss: 3.6722  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7757\n",
      "Epoch 001  Batch 724/10240  Batch Loss: 2.3865  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7754\n",
      "Epoch 001  Batch 725/10240  Batch Loss: 1.7861  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7753\n",
      "Epoch 001  Batch 726/10240  Batch Loss: 2.1292  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7753\n",
      "Epoch 001  Batch 727/10240  Batch Loss: 2.5907  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7751\n",
      "Epoch 001  Batch 728/10240  Batch Loss: 1.4313  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7751\n",
      "Epoch 001  Batch 729/10240  Batch Loss: 2.9449  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7747\n",
      "Epoch 001  Batch 730/10240  Batch Loss: 3.0103  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7745\n",
      "Epoch 001  Batch 731/10240  Batch Loss: 3.0654  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7743\n",
      "Epoch 001  Batch 732/10240  Batch Loss: 1.5184  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7743\n",
      "Epoch 001  Batch 733/10240  Batch Loss: 2.2353  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7741\n",
      "Epoch 001  Batch 734/10240  Batch Loss: 1.7940  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7741\n",
      "Epoch 001  Batch 735/10240  Batch Loss: 1.4930  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7741\n",
      "Epoch 001  Batch 736/10240  Batch Loss: 1.9594  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7742\n",
      "Epoch 001  Batch 737/10240  Batch Loss: 2.0715  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7739\n",
      "Epoch 001  Batch 738/10240  Batch Loss: 1.4230  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7739\n",
      "Epoch 001  Batch 739/10240  Batch Loss: 2.1496  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7739\n",
      "Epoch 001  Batch 740/10240  Batch Loss: 2.1581  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7737\n",
      "Epoch 001  Batch 741/10240  Batch Loss: 2.1616  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7736\n",
      "Epoch 001  Batch 742/10240  Batch Loss: 1.7649  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7736\n",
      "Epoch 001  Batch 743/10240  Batch Loss: 2.2659  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7735\n",
      "Epoch 001  Batch 744/10240  Batch Loss: 1.7876  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7735\n",
      "Epoch 001  Batch 745/10240  Batch Loss: 1.8729  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7737\n",
      "Epoch 001  Batch 746/10240  Batch Loss: 1.9185  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7736\n",
      "Epoch 001  Batch 747/10240  Batch Loss: 1.5976  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7735\n",
      "Epoch 001  Batch 748/10240  Batch Loss: 1.4340  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7735\n",
      "Epoch 001  Batch 749/10240  Batch Loss: 1.4618  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7735\n",
      "Epoch 001  Batch 750/10240  Batch Loss: 1.7830  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7736\n",
      "Epoch 001  Batch 751/10240  Batch Loss: 1.9108  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7733\n",
      "Epoch 001  Batch 752/10240  Batch Loss: 2.1207  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7734\n",
      "Epoch 001  Batch 753/10240  Batch Loss: 2.7460  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7732\n",
      "Epoch 001  Batch 754/10240  Batch Loss: 1.8719  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7733\n",
      "Epoch 001  Batch 755/10240  Batch Loss: 1.5852  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7733\n",
      "Epoch 001  Batch 756/10240  Batch Loss: 1.7808  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7733\n",
      "Epoch 001  Batch 757/10240  Batch Loss: 1.9174  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7732\n",
      "Epoch 001  Batch 758/10240  Batch Loss: 3.3462  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7733\n",
      "Epoch 001  Batch 759/10240  Batch Loss: 1.4267  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7733\n",
      "Epoch 001  Batch 760/10240  Batch Loss: 2.0213  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7732\n",
      "Epoch 001  Batch 761/10240  Batch Loss: 1.4573  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7732\n",
      "Epoch 001  Batch 762/10240  Batch Loss: 2.0548  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7731\n",
      "Epoch 001  Batch 763/10240  Batch Loss: 2.2214  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7732\n",
      "Epoch 001  Batch 764/10240  Batch Loss: 1.5226  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7732\n",
      "Epoch 001  Batch 765/10240  Batch Loss: 3.6370  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7724\n",
      "Epoch 001  Batch 766/10240  Batch Loss: 2.0893  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7726\n",
      "Epoch 001  Batch 767/10240  Batch Loss: 2.2801  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7726\n",
      "Epoch 001  Batch 768/10240  Batch Loss: 2.0966  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7725\n",
      "Epoch 001  Batch 769/10240  Batch Loss: 1.9155  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7724\n",
      "Epoch 001  Batch 770/10240  Batch Loss: 1.6028  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7724\n",
      "Epoch 001  Batch 771/10240  Batch Loss: 1.6517  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7723\n",
      "Epoch 001  Batch 772/10240  Batch Loss: 2.7212  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7721\n",
      "Epoch 001  Batch 773/10240  Batch Loss: 1.9867  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7719\n",
      "Epoch 001  Batch 774/10240  Batch Loss: 1.5573  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7719\n",
      "Epoch 001  Batch 775/10240  Batch Loss: 2.1589  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7719\n",
      "Epoch 001  Batch 776/10240  Batch Loss: 2.2707  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7717\n",
      "Epoch 001  Batch 777/10240  Batch Loss: 2.2247  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7717\n",
      "Epoch 001  Batch 778/10240  Batch Loss: 1.9711  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7720\n",
      "Epoch 001  Batch 779/10240  Batch Loss: 1.4405  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7720\n",
      "Epoch 001  Batch 780/10240  Batch Loss: 2.0489  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7718\n",
      "Epoch 001  Batch 781/10240  Batch Loss: 1.6483  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7717\n",
      "Epoch 001  Batch 782/10240  Batch Loss: 2.0409  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7718\n",
      "Epoch 001  Batch 783/10240  Batch Loss: 2.1267  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7717\n",
      "Epoch 001  Batch 784/10240  Batch Loss: 1.7884  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7716\n",
      "Epoch 001  Batch 785/10240  Batch Loss: 1.7474  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7716\n",
      "Epoch 001  Batch 786/10240  Batch Loss: 1.4667  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7716\n",
      "Epoch 001  Batch 787/10240  Batch Loss: 1.9676  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7714\n",
      "Epoch 001  Batch 788/10240  Batch Loss: 1.9166  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7712\n",
      "Epoch 001  Batch 789/10240  Batch Loss: 2.8557  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7708\n",
      "Epoch 001  Batch 790/10240  Batch Loss: 1.7858  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7708\n",
      "Epoch 001  Batch 791/10240  Batch Loss: 2.2786  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7706\n",
      "Epoch 001  Batch 792/10240  Batch Loss: 1.6283  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7706\n",
      "Epoch 001  Batch 793/10240  Batch Loss: 1.3932  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7706\n",
      "Epoch 001  Batch 794/10240  Batch Loss: 2.2384  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7703\n",
      "Epoch 001  Batch 795/10240  Batch Loss: 2.8284  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7701\n",
      "Epoch 001  Batch 796/10240  Batch Loss: 2.3462  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7701\n",
      "Epoch 001  Batch 797/10240  Batch Loss: 1.6889  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7700\n",
      "Epoch 001  Batch 798/10240  Batch Loss: 2.0511  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7698\n",
      "Epoch 001  Batch 799/10240  Batch Loss: 3.7177  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7694\n",
      "Epoch 001  Batch 800/10240  Batch Loss: 2.4774  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7694\n",
      "Epoch 001  Batch 801/10240  Batch Loss: 1.4023  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7694\n",
      "Epoch 001  Batch 802/10240  Batch Loss: 2.0602  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7693\n",
      "Epoch 001  Batch 803/10240  Batch Loss: 1.8865  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7692\n",
      "Epoch 001  Batch 804/10240  Batch Loss: 3.3682  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7690\n",
      "Epoch 001  Batch 805/10240  Batch Loss: 2.3144  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7688\n",
      "Epoch 001  Batch 806/10240  Batch Loss: 2.4784  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7686\n",
      "Epoch 001  Batch 807/10240  Batch Loss: 1.7898  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7686\n",
      "Epoch 001  Batch 808/10240  Batch Loss: 1.8904  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7685\n",
      "Epoch 001  Batch 809/10240  Batch Loss: 2.3634  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7686\n",
      "Epoch 001  Batch 810/10240  Batch Loss: 2.4051  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7683\n",
      "Epoch 001  Batch 811/10240  Batch Loss: 1.5329  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7683\n",
      "Epoch 001  Batch 812/10240  Batch Loss: 1.5219  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7683\n",
      "Epoch 001  Batch 813/10240  Batch Loss: 1.6793  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7682\n",
      "Epoch 001  Batch 814/10240  Batch Loss: 1.9782  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7681\n",
      "Epoch 001  Batch 815/10240  Batch Loss: 1.8303  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7680\n",
      "Epoch 001  Batch 816/10240  Batch Loss: 1.5726  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7679\n",
      "Epoch 001  Batch 817/10240  Batch Loss: 2.3301  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7677\n",
      "Epoch 001  Batch 818/10240  Batch Loss: 2.3193  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7675\n",
      "Epoch 001  Batch 819/10240  Batch Loss: 2.0751  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7676\n",
      "Epoch 001  Batch 820/10240  Batch Loss: 1.9327  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7675\n",
      "Epoch 001  Batch 821/10240  Batch Loss: 1.9536  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7674\n",
      "Epoch 001  Batch 822/10240  Batch Loss: 1.4697  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7673\n",
      "Epoch 001  Batch 823/10240  Batch Loss: 1.9935  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7673\n",
      "Epoch 001  Batch 824/10240  Batch Loss: 1.5469  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7673\n",
      "Epoch 001  Batch 825/10240  Batch Loss: 2.9826  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 826/10240  Batch Loss: 1.6345  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 827/10240  Batch Loss: 1.6729  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7667\n",
      "Epoch 001  Batch 828/10240  Batch Loss: 1.9887  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7666\n",
      "Epoch 001  Batch 829/10240  Batch Loss: 1.7641  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7667\n",
      "Epoch 001  Batch 830/10240  Batch Loss: 2.7806  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 831/10240  Batch Loss: 1.8838  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7667\n",
      "Epoch 001  Batch 832/10240  Batch Loss: 1.6872  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 833/10240  Batch Loss: 1.4187  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 834/10240  Batch Loss: 1.8350  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 835/10240  Batch Loss: 1.7066  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7669\n",
      "Epoch 001  Batch 836/10240  Batch Loss: 1.6999  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7670\n",
      "Epoch 001  Batch 837/10240  Batch Loss: 2.4674  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7670\n",
      "Epoch 001  Batch 838/10240  Batch Loss: 1.3641  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7670\n",
      "Epoch 001  Batch 839/10240  Batch Loss: 2.3040  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7668\n",
      "Epoch 001  Batch 840/10240  Batch Loss: 2.2448  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7666\n",
      "Epoch 001  Batch 841/10240  Batch Loss: 2.7489  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7663\n",
      "Epoch 001  Batch 842/10240  Batch Loss: 1.5412  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7664\n",
      "Epoch 001  Batch 843/10240  Batch Loss: 3.0329  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7661\n",
      "Epoch 001  Batch 844/10240  Batch Loss: 2.0216  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7660\n",
      "Epoch 001  Batch 845/10240  Batch Loss: 3.0259  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7656\n",
      "Epoch 001  Batch 846/10240  Batch Loss: 1.4183  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7656\n",
      "Epoch 001  Batch 847/10240  Batch Loss: 2.2825  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7656\n",
      "Epoch 001  Batch 848/10240  Batch Loss: 3.4154  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7650\n",
      "Epoch 001  Batch 849/10240  Batch Loss: 1.7345  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7650\n",
      "Epoch 001  Batch 850/10240  Batch Loss: 1.6073  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7650\n",
      "Epoch 001  Batch 851/10240  Batch Loss: 2.2733  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7647\n",
      "Epoch 001  Batch 852/10240  Batch Loss: 2.1906  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7645\n",
      "Epoch 001  Batch 853/10240  Batch Loss: 1.5778  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7645\n",
      "Epoch 001  Batch 854/10240  Batch Loss: 2.7673  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7641\n",
      "Epoch 001  Batch 855/10240  Batch Loss: 1.8579  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7643\n",
      "Epoch 001  Batch 856/10240  Batch Loss: 1.3652  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7643\n",
      "Epoch 001  Batch 857/10240  Batch Loss: 2.7602  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7642\n",
      "Epoch 001  Batch 858/10240  Batch Loss: 1.8689  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7640\n",
      "Epoch 001  Batch 859/10240  Batch Loss: 1.4888  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7640\n",
      "Epoch 001  Batch 860/10240  Batch Loss: 2.8782  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7637\n",
      "Epoch 001  Batch 861/10240  Batch Loss: 1.8928  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7636\n",
      "Epoch 001  Batch 862/10240  Batch Loss: 2.1802  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7636\n",
      "Epoch 001  Batch 863/10240  Batch Loss: 1.8000  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7637\n",
      "Epoch 001  Batch 864/10240  Batch Loss: 2.0732  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7636\n",
      "Epoch 001  Batch 865/10240  Batch Loss: 1.7924  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7637\n",
      "Epoch 001  Batch 866/10240  Batch Loss: 1.9716  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7635\n",
      "Epoch 001  Batch 867/10240  Batch Loss: 2.2264  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7633\n",
      "Epoch 001  Batch 868/10240  Batch Loss: 1.9448  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7633\n",
      "Epoch 001  Batch 869/10240  Batch Loss: 2.2062  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7632\n",
      "Epoch 001  Batch 870/10240  Batch Loss: 2.1954  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7634\n",
      "Epoch 001  Batch 871/10240  Batch Loss: 2.1509  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7633\n",
      "Epoch 001  Batch 872/10240  Batch Loss: 2.6316  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7631\n",
      "Epoch 001  Batch 873/10240  Batch Loss: 1.5506  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7631\n",
      "Epoch 001  Batch 874/10240  Batch Loss: 2.0165  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7631\n",
      "Epoch 001  Batch 875/10240  Batch Loss: 1.6415  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7631\n",
      "Epoch 001  Batch 876/10240  Batch Loss: 2.4719  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7629\n",
      "Epoch 001  Batch 877/10240  Batch Loss: 2.0175  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7627\n",
      "Epoch 001  Batch 878/10240  Batch Loss: 4.0424  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7621\n",
      "Epoch 001  Batch 879/10240  Batch Loss: 1.8389  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7621\n",
      "Epoch 001  Batch 880/10240  Batch Loss: 2.4175  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7620\n",
      "Epoch 001  Batch 881/10240  Batch Loss: 2.8250  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7619\n",
      "Epoch 001  Batch 882/10240  Batch Loss: 1.9662  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7617\n",
      "Epoch 001  Batch 883/10240  Batch Loss: 1.9730  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7616\n",
      "Epoch 001  Batch 884/10240  Batch Loss: 1.7478  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7615\n",
      "Epoch 001  Batch 885/10240  Batch Loss: 2.2136  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7615\n",
      "Epoch 001  Batch 886/10240  Batch Loss: 1.3615  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7615\n",
      "Epoch 001  Batch 887/10240  Batch Loss: 1.7464  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7615\n",
      "Epoch 001  Batch 888/10240  Batch Loss: 3.0958  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7611\n",
      "Epoch 001  Batch 889/10240  Batch Loss: 1.8184  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 890/10240  Batch Loss: 2.2777  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7613\n",
      "Epoch 001  Batch 891/10240  Batch Loss: 1.3201  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7613\n",
      "Epoch 001  Batch 892/10240  Batch Loss: 2.0763  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 893/10240  Batch Loss: 1.7770  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 894/10240  Batch Loss: 1.5491  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 895/10240  Batch Loss: 1.8525  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 896/10240  Batch Loss: 1.3728  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 897/10240  Batch Loss: 1.8291  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 898/10240  Batch Loss: 1.6431  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7613\n",
      "Epoch 001  Batch 899/10240  Batch Loss: 2.3675  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7612\n",
      "Epoch 001  Batch 900/10240  Batch Loss: 1.5689  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7611\n",
      "Epoch 001  Batch 901/10240  Batch Loss: 2.3630  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7610\n",
      "Epoch 001  Batch 902/10240  Batch Loss: 1.5053  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7610\n",
      "Epoch 001  Batch 903/10240  Batch Loss: 1.6795  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7610\n",
      "Epoch 001  Batch 904/10240  Batch Loss: 1.5119  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7610\n",
      "Epoch 001  Batch 905/10240  Batch Loss: 2.0381  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7609\n",
      "Epoch 001  Batch 906/10240  Batch Loss: 1.3437  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7609\n",
      "Epoch 001  Batch 907/10240  Batch Loss: 2.3972  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7608\n",
      "Epoch 001  Batch 908/10240  Batch Loss: 2.0033  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7607\n",
      "Epoch 001  Batch 909/10240  Batch Loss: 1.7157  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7606\n",
      "Epoch 001  Batch 910/10240  Batch Loss: 2.5226  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7605\n",
      "Epoch 001  Batch 911/10240  Batch Loss: 2.8529  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 912/10240  Batch Loss: 2.0344  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7603\n",
      "Epoch 001  Batch 913/10240  Batch Loss: 1.7298  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 914/10240  Batch Loss: 1.8520  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 915/10240  Batch Loss: 1.3948  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 916/10240  Batch Loss: 2.1702  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 917/10240  Batch Loss: 1.6463  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7601\n",
      "Epoch 001  Batch 918/10240  Batch Loss: 1.9895  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 919/10240  Batch Loss: 1.3587  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7602\n",
      "Epoch 001  Batch 920/10240  Batch Loss: 1.6689  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7601\n",
      "Epoch 001  Batch 921/10240  Batch Loss: 2.2751  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7600\n",
      "Epoch 001  Batch 922/10240  Batch Loss: 2.1005  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7598\n",
      "Epoch 001  Batch 923/10240  Batch Loss: 1.8450  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7598\n",
      "Epoch 001  Batch 924/10240  Batch Loss: 1.9578  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7599\n",
      "Epoch 001  Batch 925/10240  Batch Loss: 2.7773  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7596\n",
      "Epoch 001  Batch 926/10240  Batch Loss: 2.8499  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7592\n",
      "Epoch 001  Batch 927/10240  Batch Loss: 1.4431  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7592\n",
      "Epoch 001  Batch 928/10240  Batch Loss: 1.9006  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7592\n",
      "Epoch 001  Batch 929/10240  Batch Loss: 2.6814  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7588\n",
      "Epoch 001  Batch 930/10240  Batch Loss: 1.5416  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7588\n",
      "Epoch 001  Batch 931/10240  Batch Loss: 3.3136  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7584\n",
      "Epoch 001  Batch 932/10240  Batch Loss: 1.5495  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 933/10240  Batch Loss: 1.4702  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 934/10240  Batch Loss: 1.7269  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7584\n",
      "Epoch 001  Batch 935/10240  Batch Loss: 1.3256  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7584\n",
      "Epoch 001  Batch 936/10240  Batch Loss: 2.1497  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 937/10240  Batch Loss: 1.3765  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 938/10240  Batch Loss: 2.5306  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7583\n",
      "Epoch 001  Batch 939/10240  Batch Loss: 1.4505  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7584\n",
      "Epoch 001  Batch 940/10240  Batch Loss: 1.7275  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7583\n",
      "Epoch 001  Batch 941/10240  Batch Loss: 2.2429  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7583\n",
      "Epoch 001  Batch 942/10240  Batch Loss: 1.3243  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7583\n",
      "Epoch 001  Batch 943/10240  Batch Loss: 1.9663  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 944/10240  Batch Loss: 1.9516  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 945/10240  Batch Loss: 2.1406  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7585\n",
      "Epoch 001  Batch 946/10240  Batch Loss: 2.3444  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7583\n",
      "Epoch 001  Batch 947/10240  Batch Loss: 1.4197  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7583\n",
      "Epoch 001  Batch 948/10240  Batch Loss: 2.1090  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7582\n",
      "Epoch 001  Batch 949/10240  Batch Loss: 1.3436  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7582\n",
      "Epoch 001  Batch 950/10240  Batch Loss: 1.3148  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7582\n",
      "Epoch 001  Batch 951/10240  Batch Loss: 2.8133  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7580\n",
      "Epoch 001  Batch 952/10240  Batch Loss: 3.3990  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7576\n",
      "Epoch 001  Batch 953/10240  Batch Loss: 1.3251  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7576\n",
      "Epoch 001  Batch 954/10240  Batch Loss: 2.6203  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7573\n",
      "Epoch 001  Batch 955/10240  Batch Loss: 2.0598  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7572\n",
      "Epoch 001  Batch 956/10240  Batch Loss: 2.0872  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7571\n",
      "Epoch 001  Batch 957/10240  Batch Loss: 3.3433  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7568\n",
      "Epoch 001  Batch 958/10240  Batch Loss: 1.6294  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7568\n",
      "Epoch 001  Batch 959/10240  Batch Loss: 1.3276  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7568\n",
      "Epoch 001  Batch 960/10240  Batch Loss: 2.1441  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 961/10240  Batch Loss: 2.6207  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7566\n",
      "Epoch 001  Batch 962/10240  Batch Loss: 1.4689  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7566\n",
      "Epoch 001  Batch 963/10240  Batch Loss: 2.4559  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 964/10240  Batch Loss: 1.4313  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 965/10240  Batch Loss: 1.3225  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 966/10240  Batch Loss: 1.6791  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 967/10240  Batch Loss: 1.3185  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 968/10240  Batch Loss: 1.7362  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 969/10240  Batch Loss: 1.5131  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7567\n",
      "Epoch 001  Batch 970/10240  Batch Loss: 1.6975  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7568\n",
      "Epoch 001  Batch 971/10240  Batch Loss: 1.7677  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7566\n",
      "Epoch 001  Batch 972/10240  Batch Loss: 1.3992  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7566\n",
      "Epoch 001  Batch 973/10240  Batch Loss: 1.8682  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 974/10240  Batch Loss: 2.0385  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7566\n",
      "Epoch 001  Batch 975/10240  Batch Loss: 2.5433  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 976/10240  Batch Loss: 1.5374  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 977/10240  Batch Loss: 1.6671  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 978/10240  Batch Loss: 1.4585  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 979/10240  Batch Loss: 1.3134  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 980/10240  Batch Loss: 1.5327  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7565\n",
      "Epoch 001  Batch 981/10240  Batch Loss: 2.0752  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7564\n",
      "Epoch 001  Batch 982/10240  Batch Loss: 1.5978  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7563\n",
      "Epoch 001  Batch 983/10240  Batch Loss: 2.9130  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7560\n",
      "Epoch 001  Batch 984/10240  Batch Loss: 1.4153  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7560\n",
      "Epoch 001  Batch 985/10240  Batch Loss: 2.3321  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7561\n",
      "Epoch 001  Batch 986/10240  Batch Loss: 2.5223  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7559\n",
      "Epoch 001  Batch 987/10240  Batch Loss: 1.6784  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7559\n",
      "Epoch 001  Batch 988/10240  Batch Loss: 1.5158  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7559\n",
      "Epoch 001  Batch 989/10240  Batch Loss: 1.5359  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7558\n",
      "Epoch 001  Batch 990/10240  Batch Loss: 1.5115  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7557\n",
      "Epoch 001  Batch 991/10240  Batch Loss: 2.0740  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7557\n",
      "Epoch 001  Batch 992/10240  Batch Loss: 2.5876  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7556\n",
      "Epoch 001  Batch 993/10240  Batch Loss: 1.7137  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7555\n",
      "Epoch 001  Batch 994/10240  Batch Loss: 1.8568  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7555\n",
      "Epoch 001  Batch 995/10240  Batch Loss: 2.3340  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7554\n",
      "Epoch 001  Batch 996/10240  Batch Loss: 1.3491  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7554\n",
      "Epoch 001  Batch 997/10240  Batch Loss: 2.3378  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7553\n",
      "Epoch 001  Batch 998/10240  Batch Loss: 1.7585  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7551\n",
      "Epoch 001  Batch 999/10240  Batch Loss: 1.7827  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7552\n",
      "Epoch 001  Batch 1000/10240  Batch Loss: 2.0969  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7553\n",
      "Epoch 001  Batch 1001/10240  Batch Loss: 1.3458  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7553\n",
      "Epoch 001  Batch 1002/10240  Batch Loss: 1.7562  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7552\n",
      "Epoch 001  Batch 1003/10240  Batch Loss: 1.6763  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7552\n",
      "Epoch 001  Batch 1004/10240  Batch Loss: 2.0358  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7552\n",
      "Epoch 001  Batch 1005/10240  Batch Loss: 2.5414  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7549\n",
      "Epoch 001  Batch 1006/10240  Batch Loss: 1.9360  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7550\n",
      "Epoch 001  Batch 1007/10240  Batch Loss: 3.2624  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7546\n",
      "Epoch 001  Batch 1008/10240  Batch Loss: 2.1035  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7549\n",
      "Epoch 001  Batch 1009/10240  Batch Loss: 1.3527  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7549\n",
      "Epoch 001  Batch 1010/10240  Batch Loss: 1.5248  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7548\n",
      "Epoch 001  Batch 1011/10240  Batch Loss: 1.5913  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7548\n",
      "Epoch 001  Batch 1012/10240  Batch Loss: 1.9369  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7548\n",
      "Epoch 001  Batch 1013/10240  Batch Loss: 3.9147  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7544\n",
      "Epoch 001  Batch 1014/10240  Batch Loss: 2.0580  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7542\n",
      "Epoch 001  Batch 1015/10240  Batch Loss: 1.7075  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7542\n",
      "Epoch 001  Batch 1016/10240  Batch Loss: 1.2938  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7542\n",
      "Epoch 001  Batch 1017/10240  Batch Loss: 1.6588  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7542\n",
      "Epoch 001  Batch 1018/10240  Batch Loss: 1.6301  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7541\n",
      "Epoch 001  Batch 1019/10240  Batch Loss: 1.3186  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7541\n",
      "Epoch 001  Batch 1020/10240  Batch Loss: 2.0042  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7541\n",
      "Epoch 001  Batch 1021/10240  Batch Loss: 2.0716  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7540\n",
      "Epoch 001  Batch 1022/10240  Batch Loss: 2.1834  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7540\n",
      "Epoch 001  Batch 1023/10240  Batch Loss: 1.7094  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7540\n",
      "Epoch 001  Batch 1024/10240  Batch Loss: 2.3019  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7539\n",
      "Epoch 001  Batch 1025/10240  Batch Loss: 1.3776  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7539\n",
      "Epoch 001  Batch 1026/10240  Batch Loss: 3.2339  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7537\n",
      "Epoch 001  Batch 1027/10240  Batch Loss: 3.2591  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7534\n",
      "Epoch 001  Batch 1028/10240  Batch Loss: 2.8345  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7531\n",
      "Epoch 001  Batch 1029/10240  Batch Loss: 2.4553  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7527\n",
      "Epoch 001  Batch 1030/10240  Batch Loss: 1.5990  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7527\n",
      "Epoch 001  Batch 1031/10240  Batch Loss: 2.3776  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7525\n",
      "Epoch 001  Batch 1032/10240  Batch Loss: 1.7943  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7524\n",
      "Epoch 001  Batch 1033/10240  Batch Loss: 3.5227  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7519\n",
      "Epoch 001  Batch 1034/10240  Batch Loss: 1.8210  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7518\n",
      "Epoch 001  Batch 1035/10240  Batch Loss: 3.6614  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7513\n",
      "Epoch 001  Batch 1036/10240  Batch Loss: 2.2790  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7511\n",
      "Epoch 001  Batch 1037/10240  Batch Loss: 1.5190  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7512\n",
      "Epoch 001  Batch 1038/10240  Batch Loss: 2.4788  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7510\n",
      "Epoch 001  Batch 1039/10240  Batch Loss: 1.9356  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7508\n",
      "Epoch 001  Batch 1040/10240  Batch Loss: 2.0137  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7507\n",
      "Epoch 001  Batch 1041/10240  Batch Loss: 2.0474  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7505\n",
      "Epoch 001  Batch 1042/10240  Batch Loss: 1.6409  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7505\n",
      "Epoch 001  Batch 1043/10240  Batch Loss: 1.2913  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7505\n",
      "Epoch 001  Batch 1044/10240  Batch Loss: 1.6770  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7504\n",
      "Epoch 001  Batch 1045/10240  Batch Loss: 1.3022  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7504\n",
      "Epoch 001  Batch 1046/10240  Batch Loss: 2.9015  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7500\n",
      "Epoch 001  Batch 1047/10240  Batch Loss: 1.2688  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7500\n",
      "Epoch 001  Batch 1048/10240  Batch Loss: 1.7765  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7500\n",
      "Epoch 001  Batch 1049/10240  Batch Loss: 2.0575  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7498\n",
      "Epoch 001  Batch 1050/10240  Batch Loss: 2.5491  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7498\n",
      "Epoch 001  Batch 1051/10240  Batch Loss: 1.9316  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7497\n",
      "Epoch 001  Batch 1052/10240  Batch Loss: 2.5057  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7495\n",
      "Epoch 001  Batch 1053/10240  Batch Loss: 2.1494  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7494\n",
      "Epoch 001  Batch 1054/10240  Batch Loss: 1.6154  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7495\n",
      "Epoch 001  Batch 1055/10240  Batch Loss: 1.5538  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7495\n",
      "Epoch 001  Batch 1056/10240  Batch Loss: 2.7649  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7493\n",
      "Epoch 001  Batch 1057/10240  Batch Loss: 2.3335  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7491\n",
      "Epoch 001  Batch 1058/10240  Batch Loss: 1.5835  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7492\n",
      "Epoch 001  Batch 1059/10240  Batch Loss: 1.5058  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7492\n",
      "Epoch 001  Batch 1060/10240  Batch Loss: 1.9373  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7492\n",
      "Epoch 001  Batch 1061/10240  Batch Loss: 2.2488  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7492\n",
      "Epoch 001  Batch 1062/10240  Batch Loss: 2.6031  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7491\n",
      "Epoch 001  Batch 1063/10240  Batch Loss: 2.3321  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7490\n",
      "Epoch 001  Batch 1064/10240  Batch Loss: 1.6918  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7490\n",
      "Epoch 001  Batch 1065/10240  Batch Loss: 2.1528  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7489\n",
      "Epoch 001  Batch 1066/10240  Batch Loss: 1.2786  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7489\n",
      "Epoch 001  Batch 1067/10240  Batch Loss: 2.1579  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7488\n",
      "Epoch 001  Batch 1068/10240  Batch Loss: 3.1175  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7486\n",
      "Epoch 001  Batch 1069/10240  Batch Loss: 1.8296  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7486\n",
      "Epoch 001  Batch 1070/10240  Batch Loss: 2.5952  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7486\n",
      "Epoch 001  Batch 1071/10240  Batch Loss: 2.0631  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7486\n",
      "Epoch 001  Batch 1072/10240  Batch Loss: 1.8090  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7486\n",
      "Epoch 001  Batch 1073/10240  Batch Loss: 1.5809  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7485\n",
      "Epoch 001  Batch 1074/10240  Batch Loss: 2.0064  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7483\n",
      "Epoch 001  Batch 1075/10240  Batch Loss: 2.0444  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7483\n",
      "Epoch 001  Batch 1076/10240  Batch Loss: 1.7398  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7483\n",
      "Epoch 001  Batch 1077/10240  Batch Loss: 2.1618  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7482\n",
      "Epoch 001  Batch 1078/10240  Batch Loss: 1.3155  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7482\n",
      "Epoch 001  Batch 1079/10240  Batch Loss: 1.2741  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7482\n",
      "Epoch 001  Batch 1080/10240  Batch Loss: 1.9688  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7483\n",
      "Epoch 001  Batch 1081/10240  Batch Loss: 2.4317  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1082/10240  Batch Loss: 1.3474  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1083/10240  Batch Loss: 1.8631  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7480\n",
      "Epoch 001  Batch 1084/10240  Batch Loss: 2.3593  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1085/10240  Batch Loss: 1.3355  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1086/10240  Batch Loss: 1.2988  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1087/10240  Batch Loss: 1.7224  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1088/10240  Batch Loss: 2.1511  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1089/10240  Batch Loss: 1.8864  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7481\n",
      "Epoch 001  Batch 1090/10240  Batch Loss: 1.8485  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7480\n",
      "Epoch 001  Batch 1091/10240  Batch Loss: 2.6393  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7479\n",
      "Epoch 001  Batch 1092/10240  Batch Loss: 1.5598  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7479\n",
      "Epoch 001  Batch 1093/10240  Batch Loss: 1.2904  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7479\n",
      "Epoch 001  Batch 1094/10240  Batch Loss: 1.2640  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7479\n",
      "Epoch 001  Batch 1095/10240  Batch Loss: 1.7572  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7478\n",
      "Epoch 001  Batch 1096/10240  Batch Loss: 2.4472  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7477\n",
      "Epoch 001  Batch 1097/10240  Batch Loss: 1.2961  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7477\n",
      "Epoch 001  Batch 1098/10240  Batch Loss: 1.7861  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7478\n",
      "Epoch 001  Batch 1099/10240  Batch Loss: 1.2322  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7478\n",
      "Epoch 001  Batch 1100/10240  Batch Loss: 1.2799  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7478\n",
      "Epoch 001  Batch 1101/10240  Batch Loss: 1.2524  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7478\n",
      "Epoch 001  Batch 1102/10240  Batch Loss: 1.2387  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7478\n",
      "Epoch 001  Batch 1103/10240  Batch Loss: 2.4428  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7475\n",
      "Epoch 001  Batch 1104/10240  Batch Loss: 1.5616  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7475\n",
      "Epoch 001  Batch 1105/10240  Batch Loss: 1.2838  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7475\n",
      "Epoch 001  Batch 1106/10240  Batch Loss: 1.7544  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7475\n",
      "Epoch 001  Batch 1107/10240  Batch Loss: 2.1704  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7475\n",
      "Epoch 001  Batch 1108/10240  Batch Loss: 2.2677  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7473\n",
      "Epoch 001  Batch 1109/10240  Batch Loss: 1.6699  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7473\n",
      "Epoch 001  Batch 1110/10240  Batch Loss: 1.8804  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7473\n",
      "Epoch 001  Batch 1111/10240  Batch Loss: 2.0517  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7473\n",
      "Epoch 001  Batch 1112/10240  Batch Loss: 1.8222  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7473\n",
      "Epoch 001  Batch 1113/10240  Batch Loss: 1.9394  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7472\n",
      "Epoch 001  Batch 1114/10240  Batch Loss: 1.7938  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7472\n",
      "Epoch 001  Batch 1115/10240  Batch Loss: 2.8366  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1116/10240  Batch Loss: 1.3126  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1117/10240  Batch Loss: 2.4398  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7466\n",
      "Epoch 001  Batch 1118/10240  Batch Loss: 1.6198  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1119/10240  Batch Loss: 2.1657  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1120/10240  Batch Loss: 1.4496  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1121/10240  Batch Loss: 1.3062  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1122/10240  Batch Loss: 1.2771  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7467\n",
      "Epoch 001  Batch 1123/10240  Batch Loss: 1.6781  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7466\n",
      "Epoch 001  Batch 1124/10240  Batch Loss: 2.4773  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7464\n",
      "Epoch 001  Batch 1125/10240  Batch Loss: 1.5231  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7465\n",
      "Epoch 001  Batch 1126/10240  Batch Loss: 1.3690  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7464\n",
      "Epoch 001  Batch 1127/10240  Batch Loss: 1.5783  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7464\n",
      "Epoch 001  Batch 1128/10240  Batch Loss: 1.2424  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7464\n",
      "Epoch 001  Batch 1129/10240  Batch Loss: 1.3436  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7464\n",
      "Epoch 001  Batch 1130/10240  Batch Loss: 3.1348  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7460\n",
      "Epoch 001  Batch 1131/10240  Batch Loss: 1.2645  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7460\n",
      "Epoch 001  Batch 1132/10240  Batch Loss: 2.2158  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7459\n",
      "Epoch 001  Batch 1133/10240  Batch Loss: 1.5588  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7458\n",
      "Epoch 001  Batch 1134/10240  Batch Loss: 1.3222  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7458\n",
      "Epoch 001  Batch 1135/10240  Batch Loss: 1.6160  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7458\n",
      "Epoch 001  Batch 1136/10240  Batch Loss: 2.0955  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7458\n",
      "Epoch 001  Batch 1137/10240  Batch Loss: 1.9135  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7457\n",
      "Epoch 001  Batch 1138/10240  Batch Loss: 1.2979  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7457\n",
      "Epoch 001  Batch 1139/10240  Batch Loss: 2.0037  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1140/10240  Batch Loss: 1.8862  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7454\n",
      "Epoch 001  Batch 1141/10240  Batch Loss: 1.4431  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7454\n",
      "Epoch 001  Batch 1142/10240  Batch Loss: 1.4049  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1143/10240  Batch Loss: 1.2686  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1144/10240  Batch Loss: 1.4511  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1145/10240  Batch Loss: 1.8278  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1146/10240  Batch Loss: 1.5711  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1147/10240  Batch Loss: 1.3844  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7454\n",
      "Epoch 001  Batch 1148/10240  Batch Loss: 1.5489  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1149/10240  Batch Loss: 1.7724  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1150/10240  Batch Loss: 1.9436  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1151/10240  Batch Loss: 2.4932  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1152/10240  Batch Loss: 1.5305  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1153/10240  Batch Loss: 1.8716  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1154/10240  Batch Loss: 1.8803  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7455\n",
      "Epoch 001  Batch 1155/10240  Batch Loss: 2.8165  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7453\n",
      "Epoch 001  Batch 1156/10240  Batch Loss: 1.2331  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7453\n",
      "Epoch 001  Batch 1157/10240  Batch Loss: 1.6198  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7453\n",
      "Epoch 001  Batch 1158/10240  Batch Loss: 2.4877  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7451\n",
      "Epoch 001  Batch 1159/10240  Batch Loss: 2.6181  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7448\n",
      "Epoch 001  Batch 1160/10240  Batch Loss: 1.6988  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7448\n",
      "Epoch 001  Batch 1161/10240  Batch Loss: 1.6317  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7449\n",
      "Epoch 001  Batch 1162/10240  Batch Loss: 1.1939  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7449\n",
      "Epoch 001  Batch 1163/10240  Batch Loss: 3.2166  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7445\n",
      "Epoch 001  Batch 1164/10240  Batch Loss: 1.9324  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7445\n",
      "Epoch 001  Batch 1165/10240  Batch Loss: 1.4795  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7445\n",
      "Epoch 001  Batch 1166/10240  Batch Loss: 1.9093  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7445\n",
      "Epoch 001  Batch 1167/10240  Batch Loss: 1.3946  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7445\n",
      "Epoch 001  Batch 1168/10240  Batch Loss: 1.6974  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7445\n",
      "Epoch 001  Batch 1169/10240  Batch Loss: 2.7207  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7443\n",
      "Epoch 001  Batch 1170/10240  Batch Loss: 1.7559  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7443\n",
      "Epoch 001  Batch 1171/10240  Batch Loss: 1.8232  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7442\n",
      "Epoch 001  Batch 1172/10240  Batch Loss: 2.2313  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7440\n",
      "Epoch 001  Batch 1173/10240  Batch Loss: 2.3602  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7439\n",
      "Epoch 001  Batch 1174/10240  Batch Loss: 2.1086  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7439\n",
      "Epoch 001  Batch 1175/10240  Batch Loss: 1.6471  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7439\n",
      "Epoch 001  Batch 1176/10240  Batch Loss: 2.9145  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7437\n",
      "Epoch 001  Batch 1177/10240  Batch Loss: 2.9195  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7436\n",
      "Epoch 001  Batch 1178/10240  Batch Loss: 1.5537  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7436\n",
      "Epoch 001  Batch 1179/10240  Batch Loss: 1.1887  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7436\n",
      "Epoch 001  Batch 1180/10240  Batch Loss: 4.2869  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7434\n",
      "Epoch 001  Batch 1181/10240  Batch Loss: 2.6395  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7432\n",
      "Epoch 001  Batch 1182/10240  Batch Loss: 1.3379  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7432\n",
      "Epoch 001  Batch 1183/10240  Batch Loss: 1.6970  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7431\n",
      "Epoch 001  Batch 1184/10240  Batch Loss: 1.1480  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7431\n",
      "Epoch 001  Batch 1185/10240  Batch Loss: 1.9123  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7431\n",
      "Epoch 001  Batch 1186/10240  Batch Loss: 1.2071  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7431\n",
      "Epoch 001  Batch 1187/10240  Batch Loss: 2.5382  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7428\n",
      "Epoch 001  Batch 1188/10240  Batch Loss: 1.2503  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7428\n",
      "Epoch 001  Batch 1189/10240  Batch Loss: 1.3450  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7428\n",
      "Epoch 001  Batch 1190/10240  Batch Loss: 1.3189  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7428\n",
      "Epoch 001  Batch 1191/10240  Batch Loss: 3.8389  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7426\n",
      "Epoch 001  Batch 1192/10240  Batch Loss: 1.4836  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7425\n",
      "Epoch 001  Batch 1193/10240  Batch Loss: 1.7387  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7426\n",
      "Epoch 001  Batch 1194/10240  Batch Loss: 2.3323  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7425\n",
      "Epoch 001  Batch 1195/10240  Batch Loss: 2.3437  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7424\n",
      "Epoch 001  Batch 1196/10240  Batch Loss: 1.3746  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7424\n",
      "Epoch 001  Batch 1197/10240  Batch Loss: 1.9638  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7422\n",
      "Epoch 001  Batch 1198/10240  Batch Loss: 1.3063  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7422\n",
      "Epoch 001  Batch 1199/10240  Batch Loss: 1.9208  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7423\n",
      "Epoch 001  Batch 1200/10240  Batch Loss: 2.8983  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7424\n",
      "Epoch 001  Batch 1201/10240  Batch Loss: 2.1890  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7422\n",
      "Epoch 001  Batch 1202/10240  Batch Loss: 2.7968  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1203/10240  Batch Loss: 1.5186  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1204/10240  Batch Loss: 1.4668  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1205/10240  Batch Loss: 1.4047  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1206/10240  Batch Loss: 1.6765  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7419\n",
      "Epoch 001  Batch 1207/10240  Batch Loss: 1.9555  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7419\n",
      "Epoch 001  Batch 1208/10240  Batch Loss: 1.4403  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1209/10240  Batch Loss: 2.4031  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1210/10240  Batch Loss: 1.2385  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1211/10240  Batch Loss: 1.5312  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1212/10240  Batch Loss: 1.5964  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1213/10240  Batch Loss: 1.2717  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1214/10240  Batch Loss: 1.3951  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7420\n",
      "Epoch 001  Batch 1215/10240  Batch Loss: 2.2528  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7418\n",
      "Epoch 001  Batch 1216/10240  Batch Loss: 1.3418  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7418\n",
      "Epoch 001  Batch 1217/10240  Batch Loss: 1.6245  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7419\n",
      "Epoch 001  Batch 1218/10240  Batch Loss: 2.5792  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7416\n",
      "Epoch 001  Batch 1219/10240  Batch Loss: 1.5623  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7416\n",
      "Epoch 001  Batch 1220/10240  Batch Loss: 2.8186  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7414\n",
      "Epoch 001  Batch 1221/10240  Batch Loss: 1.4844  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7414\n",
      "Epoch 001  Batch 1222/10240  Batch Loss: 1.5851  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7414\n",
      "Epoch 001  Batch 1223/10240  Batch Loss: 1.9042  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7414\n",
      "Epoch 001  Batch 1224/10240  Batch Loss: 1.7210  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7414\n",
      "Epoch 001  Batch 1225/10240  Batch Loss: 2.2648  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7413\n",
      "Epoch 001  Batch 1226/10240  Batch Loss: 1.4249  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7412\n",
      "Epoch 001  Batch 1227/10240  Batch Loss: 2.7314  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7410\n",
      "Epoch 001  Batch 1228/10240  Batch Loss: 1.9199  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7409\n",
      "Epoch 001  Batch 1229/10240  Batch Loss: 1.2482  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7409\n",
      "Epoch 001  Batch 1230/10240  Batch Loss: 1.6479  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7408\n",
      "Epoch 001  Batch 1231/10240  Batch Loss: 1.5290  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7408\n",
      "Epoch 001  Batch 1232/10240  Batch Loss: 2.0583  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7406\n",
      "Epoch 001  Batch 1233/10240  Batch Loss: 2.0207  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7406\n",
      "Epoch 001  Batch 1234/10240  Batch Loss: 1.4799  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7406\n",
      "Epoch 001  Batch 1235/10240  Batch Loss: 2.7150  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7405\n",
      "Epoch 001  Batch 1236/10240  Batch Loss: 2.2901  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7404\n",
      "Epoch 001  Batch 1237/10240  Batch Loss: 1.9012  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7404\n",
      "Epoch 001  Batch 1238/10240  Batch Loss: 1.5439  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7404\n",
      "Epoch 001  Batch 1239/10240  Batch Loss: 1.5568  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7404\n",
      "Epoch 001  Batch 1240/10240  Batch Loss: 1.7928  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7402\n",
      "Epoch 001  Batch 1241/10240  Batch Loss: 3.1576  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7398\n",
      "Epoch 001  Batch 1242/10240  Batch Loss: 1.4854  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7398\n",
      "Epoch 001  Batch 1243/10240  Batch Loss: 1.1676  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7398\n",
      "Epoch 001  Batch 1244/10240  Batch Loss: 1.6927  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7397\n",
      "Epoch 001  Batch 1245/10240  Batch Loss: 1.1905  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7397\n",
      "Epoch 001  Batch 1246/10240  Batch Loss: 2.3088  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7397\n",
      "Epoch 001  Batch 1247/10240  Batch Loss: 2.8482  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7395\n",
      "Epoch 001  Batch 1248/10240  Batch Loss: 1.9787  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7396\n",
      "Epoch 001  Batch 1249/10240  Batch Loss: 2.5386  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7396\n",
      "Epoch 001  Batch 1250/10240  Batch Loss: 2.2541  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7396\n",
      "Epoch 001  Batch 1251/10240  Batch Loss: 1.9948  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7395\n",
      "Epoch 001  Batch 1252/10240  Batch Loss: 1.1776  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7395\n",
      "Epoch 001  Batch 1253/10240  Batch Loss: 2.0862  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7393\n",
      "Epoch 001  Batch 1254/10240  Batch Loss: 1.8770  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7394\n",
      "Epoch 001  Batch 1255/10240  Batch Loss: 1.9463  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7393\n",
      "Epoch 001  Batch 1256/10240  Batch Loss: 1.8947  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7392\n",
      "Epoch 001  Batch 1257/10240  Batch Loss: 1.2153  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7391\n",
      "Epoch 001  Batch 1258/10240  Batch Loss: 1.5862  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7391\n",
      "Epoch 001  Batch 1259/10240  Batch Loss: 1.4284  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7391\n",
      "Epoch 001  Batch 1260/10240  Batch Loss: 1.1975  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7391\n",
      "Epoch 001  Batch 1261/10240  Batch Loss: 1.6890  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7390\n",
      "Epoch 001  Batch 1262/10240  Batch Loss: 1.1132  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7390\n",
      "Epoch 001  Batch 1263/10240  Batch Loss: 1.7552  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7389\n",
      "Epoch 001  Batch 1264/10240  Batch Loss: 1.4202  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7389\n",
      "Epoch 001  Batch 1265/10240  Batch Loss: 1.1257  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7389\n",
      "Epoch 001  Batch 1266/10240  Batch Loss: 1.6707  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7389\n",
      "Epoch 001  Batch 1267/10240  Batch Loss: 2.2266  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7388\n",
      "Epoch 001  Batch 1268/10240  Batch Loss: 1.5016  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7389\n",
      "Epoch 001  Batch 1269/10240  Batch Loss: 1.3631  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7389\n",
      "Epoch 001  Batch 1270/10240  Batch Loss: 1.9393  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7387\n",
      "Epoch 001  Batch 1271/10240  Batch Loss: 1.7090  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7387\n",
      "Epoch 001  Batch 1272/10240  Batch Loss: 2.4515  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7386\n",
      "Epoch 001  Batch 1273/10240  Batch Loss: 1.9236  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7385\n",
      "Epoch 001  Batch 1274/10240  Batch Loss: 3.4540  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7382\n",
      "Epoch 001  Batch 1275/10240  Batch Loss: 1.2123  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7381\n",
      "Epoch 001  Batch 1276/10240  Batch Loss: 2.1884  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7380\n",
      "Epoch 001  Batch 1277/10240  Batch Loss: 1.6522  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7379\n",
      "Epoch 001  Batch 1278/10240  Batch Loss: 1.4347  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7379\n",
      "Epoch 001  Batch 1279/10240  Batch Loss: 1.2922  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7379\n",
      "Epoch 001  Batch 1280/10240  Batch Loss: 2.1483  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7377\n",
      "Epoch 001  Batch 1281/10240  Batch Loss: 2.3023  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7375\n",
      "Epoch 001  Batch 1282/10240  Batch Loss: 1.3175  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7375\n",
      "Epoch 001  Batch 1283/10240  Batch Loss: 1.1745  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7375\n",
      "Epoch 001  Batch 1284/10240  Batch Loss: 1.3955  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7376\n",
      "Epoch 001  Batch 1285/10240  Batch Loss: 1.5027  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7375\n",
      "Epoch 001  Batch 1286/10240  Batch Loss: 1.3549  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7374\n",
      "Epoch 001  Batch 1287/10240  Batch Loss: 1.6059  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7374\n",
      "Epoch 001  Batch 1288/10240  Batch Loss: 1.3240  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7374\n",
      "Epoch 001  Batch 1289/10240  Batch Loss: 1.6479  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7373\n",
      "Epoch 001  Batch 1290/10240  Batch Loss: 1.7804  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7372\n",
      "Epoch 001  Batch 1291/10240  Batch Loss: 1.9400  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7372\n",
      "Epoch 001  Batch 1292/10240  Batch Loss: 1.7796  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7372\n",
      "Epoch 001  Batch 1293/10240  Batch Loss: 1.3355  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7371\n",
      "Epoch 001  Batch 1294/10240  Batch Loss: 2.2899  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7370\n",
      "Epoch 001  Batch 1295/10240  Batch Loss: 2.4354  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7367\n",
      "Epoch 001  Batch 1296/10240  Batch Loss: 1.5962  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7366\n",
      "Epoch 001  Batch 1297/10240  Batch Loss: 2.1749  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7364\n",
      "Epoch 001  Batch 1298/10240  Batch Loss: 1.9450  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7364\n",
      "Epoch 001  Batch 1299/10240  Batch Loss: 1.6909  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7363\n",
      "Epoch 001  Batch 1300/10240  Batch Loss: 2.6844  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7362\n",
      "Epoch 001  Batch 1301/10240  Batch Loss: 1.2576  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7362\n",
      "Epoch 001  Batch 1302/10240  Batch Loss: 1.6729  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7362\n",
      "Epoch 001  Batch 1303/10240  Batch Loss: 1.3251  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7362\n",
      "Epoch 001  Batch 1304/10240  Batch Loss: 1.2331  | train F1: 0.0069  | train precision: 0.0035  | train recall: 0.7362\n",
      "Epoch 001  Batch 1305/10240  Batch Loss: 2.3759  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7363\n",
      "Epoch 001  Batch 1306/10240  Batch Loss: 1.5243  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7363\n",
      "Epoch 001  Batch 1307/10240  Batch Loss: 2.7750  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7361\n",
      "Epoch 001  Batch 1308/10240  Batch Loss: 1.9447  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7361\n",
      "Epoch 001  Batch 1309/10240  Batch Loss: 1.6364  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7362\n",
      "Epoch 001  Batch 1310/10240  Batch Loss: 1.8486  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7360\n",
      "Epoch 001  Batch 1311/10240  Batch Loss: 1.1355  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7360\n",
      "Epoch 001  Batch 1312/10240  Batch Loss: 1.6808  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7360\n",
      "Epoch 001  Batch 1313/10240  Batch Loss: 3.3772  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7356\n",
      "Epoch 001  Batch 1314/10240  Batch Loss: 1.9126  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7354\n",
      "Epoch 001  Batch 1315/10240  Batch Loss: 1.2397  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7355\n",
      "Epoch 001  Batch 1316/10240  Batch Loss: 1.1867  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7355\n",
      "Epoch 001  Batch 1317/10240  Batch Loss: 2.0148  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7353\n",
      "Epoch 001  Batch 1318/10240  Batch Loss: 1.1433  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7353\n",
      "Epoch 001  Batch 1319/10240  Batch Loss: 1.5609  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7353\n",
      "Epoch 001  Batch 1320/10240  Batch Loss: 2.0793  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7353\n",
      "Epoch 001  Batch 1321/10240  Batch Loss: 1.9227  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7353\n",
      "Epoch 001  Batch 1322/10240  Batch Loss: 1.0773  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7353\n",
      "Epoch 001  Batch 1323/10240  Batch Loss: 1.5221  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7352\n",
      "Epoch 001  Batch 1324/10240  Batch Loss: 3.4932  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7349\n",
      "Epoch 001  Batch 1325/10240  Batch Loss: 1.4541  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7350\n",
      "Epoch 001  Batch 1326/10240  Batch Loss: 1.5631  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7349\n",
      "Epoch 001  Batch 1327/10240  Batch Loss: 1.9087  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7350\n",
      "Epoch 001  Batch 1328/10240  Batch Loss: 1.9757  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7351\n",
      "Epoch 001  Batch 1329/10240  Batch Loss: 1.8688  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7350\n",
      "Epoch 001  Batch 1330/10240  Batch Loss: 3.1846  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7347\n",
      "Epoch 001  Batch 1331/10240  Batch Loss: 1.8005  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1332/10240  Batch Loss: 1.5730  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1333/10240  Batch Loss: 1.3296  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1334/10240  Batch Loss: 1.5466  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7347\n",
      "Epoch 001  Batch 1335/10240  Batch Loss: 1.8045  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1336/10240  Batch Loss: 1.3753  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1337/10240  Batch Loss: 1.7271  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1338/10240  Batch Loss: 2.2407  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7349\n",
      "Epoch 001  Batch 1339/10240  Batch Loss: 1.6872  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7350\n",
      "Epoch 001  Batch 1340/10240  Batch Loss: 2.2897  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7348\n",
      "Epoch 001  Batch 1341/10240  Batch Loss: 1.7861  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7349\n",
      "Epoch 001  Batch 1342/10240  Batch Loss: 2.0620  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7347\n",
      "Epoch 001  Batch 1343/10240  Batch Loss: 2.9405  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7344\n",
      "Epoch 001  Batch 1344/10240  Batch Loss: 1.3475  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7344\n",
      "Epoch 001  Batch 1345/10240  Batch Loss: 2.0576  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7343\n",
      "Epoch 001  Batch 1346/10240  Batch Loss: 1.5366  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7343\n",
      "Epoch 001  Batch 1347/10240  Batch Loss: 1.1828  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7343\n",
      "Epoch 001  Batch 1348/10240  Batch Loss: 1.8134  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7344\n",
      "Epoch 001  Batch 1349/10240  Batch Loss: 1.6394  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7343\n",
      "Epoch 001  Batch 1350/10240  Batch Loss: 1.9511  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7342\n",
      "Epoch 001  Batch 1351/10240  Batch Loss: 1.4623  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7342\n",
      "Epoch 001  Batch 1352/10240  Batch Loss: 2.7629  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7340\n",
      "Epoch 001  Batch 1353/10240  Batch Loss: 2.8688  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7337\n",
      "Epoch 001  Batch 1354/10240  Batch Loss: 1.4087  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7337\n",
      "Epoch 001  Batch 1355/10240  Batch Loss: 2.6758  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7335\n",
      "Epoch 001  Batch 1356/10240  Batch Loss: 1.8261  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7334\n",
      "Epoch 001  Batch 1357/10240  Batch Loss: 1.3113  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7334\n",
      "Epoch 001  Batch 1358/10240  Batch Loss: 1.1617  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7334\n",
      "Epoch 001  Batch 1359/10240  Batch Loss: 1.1058  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7334\n",
      "Epoch 001  Batch 1360/10240  Batch Loss: 1.4759  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7334\n",
      "Epoch 001  Batch 1361/10240  Batch Loss: 2.5229  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7333\n",
      "Epoch 001  Batch 1362/10240  Batch Loss: 1.1008  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7333\n",
      "Epoch 001  Batch 1363/10240  Batch Loss: 2.0995  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7333\n",
      "Epoch 001  Batch 1364/10240  Batch Loss: 1.1503  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7333\n",
      "Epoch 001  Batch 1365/10240  Batch Loss: 1.2509  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7332\n",
      "Epoch 001  Batch 1366/10240  Batch Loss: 1.0911  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7332\n",
      "Epoch 001  Batch 1367/10240  Batch Loss: 2.8381  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7331\n",
      "Epoch 001  Batch 1368/10240  Batch Loss: 2.2594  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7331\n",
      "Epoch 001  Batch 1369/10240  Batch Loss: 1.1374  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7331\n",
      "Epoch 001  Batch 1370/10240  Batch Loss: 1.6874  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7331\n",
      "Epoch 001  Batch 1371/10240  Batch Loss: 2.7901  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7329\n",
      "Epoch 001  Batch 1372/10240  Batch Loss: 1.3487  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7329\n",
      "Epoch 001  Batch 1373/10240  Batch Loss: 1.7576  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7329\n",
      "Epoch 001  Batch 1374/10240  Batch Loss: 2.3176  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7328\n",
      "Epoch 001  Batch 1375/10240  Batch Loss: 1.7641  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7327\n",
      "Epoch 001  Batch 1376/10240  Batch Loss: 1.9812  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7325\n",
      "Epoch 001  Batch 1377/10240  Batch Loss: 1.3069  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7325\n",
      "Epoch 001  Batch 1378/10240  Batch Loss: 1.3151  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7325\n",
      "Epoch 001  Batch 1379/10240  Batch Loss: 2.2819  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7323\n",
      "Epoch 001  Batch 1380/10240  Batch Loss: 1.6351  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7324\n",
      "Epoch 001  Batch 1381/10240  Batch Loss: 2.0521  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7322\n",
      "Epoch 001  Batch 1382/10240  Batch Loss: 1.5519  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7323\n",
      "Epoch 001  Batch 1383/10240  Batch Loss: 1.7385  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7322\n",
      "Epoch 001  Batch 1384/10240  Batch Loss: 1.1544  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7322\n",
      "Epoch 001  Batch 1385/10240  Batch Loss: 1.9405  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7322\n",
      "Epoch 001  Batch 1386/10240  Batch Loss: 2.1203  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7322\n",
      "Epoch 001  Batch 1387/10240  Batch Loss: 1.9395  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7322\n",
      "Epoch 001  Batch 1388/10240  Batch Loss: 2.0134  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7321\n",
      "Epoch 001  Batch 1389/10240  Batch Loss: 1.7947  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7319\n",
      "Epoch 001  Batch 1390/10240  Batch Loss: 1.1810  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7319\n",
      "Epoch 001  Batch 1391/10240  Batch Loss: 1.1063  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7319\n",
      "Epoch 001  Batch 1392/10240  Batch Loss: 1.2148  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7319\n",
      "Epoch 001  Batch 1393/10240  Batch Loss: 2.1804  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7317\n",
      "Epoch 001  Batch 1394/10240  Batch Loss: 2.2672  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7316\n",
      "Epoch 001  Batch 1395/10240  Batch Loss: 1.7988  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7315\n",
      "Epoch 001  Batch 1396/10240  Batch Loss: 2.4040  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7314\n",
      "Epoch 001  Batch 1397/10240  Batch Loss: 1.4460  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7315\n",
      "Epoch 001  Batch 1398/10240  Batch Loss: 1.3077  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7315\n",
      "Epoch 001  Batch 1399/10240  Batch Loss: 1.1208  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7315\n",
      "Epoch 001  Batch 1400/10240  Batch Loss: 1.8700  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7314\n",
      "Epoch 001  Batch 1401/10240  Batch Loss: 2.3364  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7312\n",
      "Epoch 001  Batch 1402/10240  Batch Loss: 1.8525  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7311\n",
      "Epoch 001  Batch 1403/10240  Batch Loss: 1.3971  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7311\n",
      "Epoch 001  Batch 1404/10240  Batch Loss: 1.2385  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7311\n",
      "Epoch 001  Batch 1405/10240  Batch Loss: 1.6719  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7311\n",
      "Epoch 001  Batch 1406/10240  Batch Loss: 1.4800  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7311\n",
      "Epoch 001  Batch 1407/10240  Batch Loss: 1.2140  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7311\n",
      "Epoch 001  Batch 1408/10240  Batch Loss: 1.8484  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7310\n",
      "Epoch 001  Batch 1409/10240  Batch Loss: 1.1961  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7310\n",
      "Epoch 001  Batch 1410/10240  Batch Loss: 2.4296  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7310\n",
      "Epoch 001  Batch 1411/10240  Batch Loss: 1.3866  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7310\n",
      "Epoch 001  Batch 1412/10240  Batch Loss: 1.5709  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7310\n",
      "Epoch 001  Batch 1413/10240  Batch Loss: 1.6221  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7308\n",
      "Epoch 001  Batch 1414/10240  Batch Loss: 1.5780  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7309\n",
      "Epoch 001  Batch 1415/10240  Batch Loss: 1.8674  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7307\n",
      "Epoch 001  Batch 1416/10240  Batch Loss: 1.0928  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7307\n",
      "Epoch 001  Batch 1417/10240  Batch Loss: 1.5436  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7308\n",
      "Epoch 001  Batch 1418/10240  Batch Loss: 1.4749  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7308\n",
      "Epoch 001  Batch 1419/10240  Batch Loss: 1.2177  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7308\n",
      "Epoch 001  Batch 1420/10240  Batch Loss: 2.1044  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7307\n",
      "Epoch 001  Batch 1421/10240  Batch Loss: 2.0962  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7307\n",
      "Epoch 001  Batch 1422/10240  Batch Loss: 1.7177  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7306\n",
      "Epoch 001  Batch 1423/10240  Batch Loss: 2.2531  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7303\n",
      "Epoch 001  Batch 1424/10240  Batch Loss: 2.3713  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7302\n",
      "Epoch 001  Batch 1425/10240  Batch Loss: 2.2690  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7302\n",
      "Epoch 001  Batch 1426/10240  Batch Loss: 2.1727  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7301\n",
      "Epoch 001  Batch 1427/10240  Batch Loss: 1.6250  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7301\n",
      "Epoch 001  Batch 1428/10240  Batch Loss: 1.2450  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7301\n",
      "Epoch 001  Batch 1429/10240  Batch Loss: 1.5726  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7300\n",
      "Epoch 001  Batch 1430/10240  Batch Loss: 1.4007  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7301\n",
      "Epoch 001  Batch 1431/10240  Batch Loss: 1.2313  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7301\n",
      "Epoch 001  Batch 1432/10240  Batch Loss: 2.5093  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7300\n",
      "Epoch 001  Batch 1433/10240  Batch Loss: 1.0778  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7300\n",
      "Epoch 001  Batch 1434/10240  Batch Loss: 2.4409  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7297\n",
      "Epoch 001  Batch 1435/10240  Batch Loss: 2.0210  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7296\n",
      "Epoch 001  Batch 1436/10240  Batch Loss: 2.4300  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7295\n",
      "Epoch 001  Batch 1437/10240  Batch Loss: 2.3502  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7293\n",
      "Epoch 001  Batch 1438/10240  Batch Loss: 1.5014  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7293\n",
      "Epoch 001  Batch 1439/10240  Batch Loss: 1.4493  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7293\n",
      "Epoch 001  Batch 1440/10240  Batch Loss: 1.1861  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7293\n",
      "Epoch 001  Batch 1441/10240  Batch Loss: 2.3885  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7290\n",
      "Epoch 001  Batch 1442/10240  Batch Loss: 1.1053  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7290\n",
      "Epoch 001  Batch 1443/10240  Batch Loss: 1.5905  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7289\n",
      "Epoch 001  Batch 1444/10240  Batch Loss: 1.3058  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7289\n",
      "Epoch 001  Batch 1445/10240  Batch Loss: 1.7775  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7288\n",
      "Epoch 001  Batch 1446/10240  Batch Loss: 1.9175  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7288\n",
      "Epoch 001  Batch 1447/10240  Batch Loss: 1.7644  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7288\n",
      "Epoch 001  Batch 1448/10240  Batch Loss: 2.5790  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7286\n",
      "Epoch 001  Batch 1449/10240  Batch Loss: 2.2702  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7285\n",
      "Epoch 001  Batch 1450/10240  Batch Loss: 2.0109  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7283\n",
      "Epoch 001  Batch 1451/10240  Batch Loss: 1.5684  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7282\n",
      "Epoch 001  Batch 1452/10240  Batch Loss: 2.0613  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7282\n",
      "Epoch 001  Batch 1453/10240  Batch Loss: 4.3362  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7277\n",
      "Epoch 001  Batch 1454/10240  Batch Loss: 1.6420  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7277\n",
      "Epoch 001  Batch 1455/10240  Batch Loss: 1.9402  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7276\n",
      "Epoch 001  Batch 1456/10240  Batch Loss: 1.6056  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7275\n",
      "Epoch 001  Batch 1457/10240  Batch Loss: 2.2654  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7274\n",
      "Epoch 001  Batch 1458/10240  Batch Loss: 1.1065  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7274\n",
      "Epoch 001  Batch 1459/10240  Batch Loss: 1.6207  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7274\n",
      "Epoch 001  Batch 1460/10240  Batch Loss: 1.7012  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7273\n",
      "Epoch 001  Batch 1461/10240  Batch Loss: 2.3671  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7273\n",
      "Epoch 001  Batch 1462/10240  Batch Loss: 1.6693  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7273\n",
      "Epoch 001  Batch 1463/10240  Batch Loss: 1.1184  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7273\n",
      "Epoch 001  Batch 1464/10240  Batch Loss: 2.8280  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7270\n",
      "Epoch 001  Batch 1465/10240  Batch Loss: 1.8638  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7270\n",
      "Epoch 001  Batch 1466/10240  Batch Loss: 1.3478  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7270\n",
      "Epoch 001  Batch 1467/10240  Batch Loss: 2.5482  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7268\n",
      "Epoch 001  Batch 1468/10240  Batch Loss: 2.5558  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7268\n",
      "Epoch 001  Batch 1469/10240  Batch Loss: 1.5123  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7268\n",
      "Epoch 001  Batch 1470/10240  Batch Loss: 1.2326  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7267\n",
      "Epoch 001  Batch 1471/10240  Batch Loss: 1.3717  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7268\n",
      "Epoch 001  Batch 1472/10240  Batch Loss: 1.1138  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7268\n",
      "Epoch 001  Batch 1473/10240  Batch Loss: 1.2718  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7268\n",
      "Epoch 001  Batch 1474/10240  Batch Loss: 2.1808  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7267\n",
      "Epoch 001  Batch 1475/10240  Batch Loss: 1.2915  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7267\n",
      "Epoch 001  Batch 1476/10240  Batch Loss: 1.7609  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7267\n",
      "Epoch 001  Batch 1477/10240  Batch Loss: 1.2283  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7267\n",
      "Epoch 001  Batch 1478/10240  Batch Loss: 1.4970  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7266\n",
      "Epoch 001  Batch 1479/10240  Batch Loss: 1.9394  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7265\n",
      "Epoch 001  Batch 1480/10240  Batch Loss: 2.2122  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7265\n",
      "Epoch 001  Batch 1481/10240  Batch Loss: 1.0620  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7265\n",
      "Epoch 001  Batch 1482/10240  Batch Loss: 2.0965  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7264\n",
      "Epoch 001  Batch 1483/10240  Batch Loss: 1.4880  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7264\n",
      "Epoch 001  Batch 1484/10240  Batch Loss: 1.3418  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7264\n",
      "Epoch 001  Batch 1485/10240  Batch Loss: 3.0225  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7262\n",
      "Epoch 001  Batch 1486/10240  Batch Loss: 1.0926  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7262\n",
      "Epoch 001  Batch 1487/10240  Batch Loss: 1.8855  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7261\n",
      "Epoch 001  Batch 1488/10240  Batch Loss: 2.9348  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7260\n",
      "Epoch 001  Batch 1489/10240  Batch Loss: 3.6532  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7257\n",
      "Epoch 001  Batch 1490/10240  Batch Loss: 1.8991  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7256\n",
      "Epoch 001  Batch 1491/10240  Batch Loss: 1.2720  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7256\n",
      "Epoch 001  Batch 1492/10240  Batch Loss: 1.7059  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7255\n",
      "Epoch 001  Batch 1493/10240  Batch Loss: 2.4868  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7254\n",
      "Epoch 001  Batch 1494/10240  Batch Loss: 2.3779  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7252\n",
      "Epoch 001  Batch 1495/10240  Batch Loss: 2.0255  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7252\n",
      "Epoch 001  Batch 1496/10240  Batch Loss: 1.4267  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7251\n",
      "Epoch 001  Batch 1497/10240  Batch Loss: 2.0590  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7250\n",
      "Epoch 001  Batch 1498/10240  Batch Loss: 2.6946  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7248\n",
      "Epoch 001  Batch 1499/10240  Batch Loss: 3.6904  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7244\n",
      "Epoch 001  Batch 1500/10240  Batch Loss: 2.4015  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7243\n",
      "Epoch 001  Batch 1501/10240  Batch Loss: 1.8214  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7242\n",
      "Epoch 001  Batch 1502/10240  Batch Loss: 1.6338  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7242\n",
      "Epoch 001  Batch 1503/10240  Batch Loss: 2.8239  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7240\n",
      "Epoch 001  Batch 1504/10240  Batch Loss: 1.5798  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7240\n",
      "Epoch 001  Batch 1505/10240  Batch Loss: 1.4783  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7240\n",
      "Epoch 001  Batch 1506/10240  Batch Loss: 2.0805  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7240\n",
      "Epoch 001  Batch 1507/10240  Batch Loss: 1.3487  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7239\n",
      "Epoch 001  Batch 1508/10240  Batch Loss: 1.6305  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7239\n",
      "Epoch 001  Batch 1509/10240  Batch Loss: 1.0584  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7239\n",
      "Epoch 001  Batch 1510/10240  Batch Loss: 2.0146  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7238\n",
      "Epoch 001  Batch 1511/10240  Batch Loss: 1.8135  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7237\n",
      "Epoch 001  Batch 1512/10240  Batch Loss: 1.5433  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7238\n",
      "Epoch 001  Batch 1513/10240  Batch Loss: 1.5125  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7237\n",
      "Epoch 001  Batch 1514/10240  Batch Loss: 1.5789  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7237\n",
      "Epoch 001  Batch 1515/10240  Batch Loss: 1.4434  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7237\n",
      "Epoch 001  Batch 1516/10240  Batch Loss: 3.0550  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7236\n",
      "Epoch 001  Batch 1517/10240  Batch Loss: 2.4417  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7235\n",
      "Epoch 001  Batch 1518/10240  Batch Loss: 3.1972  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7232\n",
      "Epoch 001  Batch 1519/10240  Batch Loss: 1.2758  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7232\n",
      "Epoch 001  Batch 1520/10240  Batch Loss: 1.2921  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7232\n",
      "Epoch 001  Batch 1521/10240  Batch Loss: 1.7628  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7232\n",
      "Epoch 001  Batch 1522/10240  Batch Loss: 2.4442  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7231\n",
      "Epoch 001  Batch 1523/10240  Batch Loss: 1.3719  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7231\n",
      "Epoch 001  Batch 1524/10240  Batch Loss: 2.2205  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7230\n",
      "Epoch 001  Batch 1525/10240  Batch Loss: 2.4042  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7229\n",
      "Epoch 001  Batch 1526/10240  Batch Loss: 2.6145  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7226\n",
      "Epoch 001  Batch 1527/10240  Batch Loss: 2.3380  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7225\n",
      "Epoch 001  Batch 1528/10240  Batch Loss: 2.0533  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7225\n",
      "Epoch 001  Batch 1529/10240  Batch Loss: 1.5632  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7225\n",
      "Epoch 001  Batch 1530/10240  Batch Loss: 2.0027  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7225\n",
      "Epoch 001  Batch 1531/10240  Batch Loss: 1.2340  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7225\n",
      "Epoch 001  Batch 1532/10240  Batch Loss: 1.0794  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7225\n",
      "Epoch 001  Batch 1533/10240  Batch Loss: 3.0114  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7223\n",
      "Epoch 001  Batch 1534/10240  Batch Loss: 1.5392  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7222\n",
      "Epoch 001  Batch 1535/10240  Batch Loss: 1.8461  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7222\n",
      "Epoch 001  Batch 1536/10240  Batch Loss: 1.8796  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7221\n",
      "Epoch 001  Batch 1537/10240  Batch Loss: 1.4844  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7220\n",
      "Epoch 001  Batch 1538/10240  Batch Loss: 1.9861  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7219\n",
      "Epoch 001  Batch 1539/10240  Batch Loss: 1.1829  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7219\n",
      "Epoch 001  Batch 1540/10240  Batch Loss: 1.2265  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7219\n",
      "Epoch 001  Batch 1541/10240  Batch Loss: 2.9080  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7218\n",
      "Epoch 001  Batch 1542/10240  Batch Loss: 2.3771  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7217\n",
      "Epoch 001  Batch 1543/10240  Batch Loss: 1.7584  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7217\n",
      "Epoch 001  Batch 1544/10240  Batch Loss: 2.6238  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7216\n",
      "Epoch 001  Batch 1545/10240  Batch Loss: 1.0627  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7215\n",
      "Epoch 001  Batch 1546/10240  Batch Loss: 1.0556  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7215\n",
      "Epoch 001  Batch 1547/10240  Batch Loss: 1.9136  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7215\n",
      "Epoch 001  Batch 1548/10240  Batch Loss: 1.7702  | train F1: 0.0070  | train precision: 0.0035  | train recall: 0.7215\n",
      "Epoch 001  Batch 1549/10240  Batch Loss: 3.1490  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7214\n",
      "Epoch 001  Batch 1550/10240  Batch Loss: 3.0141  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7212\n",
      "Epoch 001  Batch 1551/10240  Batch Loss: 1.4187  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7212\n",
      "Epoch 001  Batch 1552/10240  Batch Loss: 1.1135  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7212\n",
      "Epoch 001  Batch 1553/10240  Batch Loss: 3.4072  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7210\n",
      "Epoch 001  Batch 1554/10240  Batch Loss: 1.7898  | train F1: 0.0071  | train precision: 0.0035  | train recall: 0.7210\n",
      "Epoch 001  Batch 1555/10240  Batch Loss: 1.7343  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7211\n",
      "Epoch 001  Batch 1556/10240  Batch Loss: 1.9071  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7211\n",
      "Epoch 001  Batch 1557/10240  Batch Loss: 1.4329  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7211\n",
      "Epoch 001  Batch 1558/10240  Batch Loss: 1.6321  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7212\n",
      "Epoch 001  Batch 1559/10240  Batch Loss: 1.6371  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7211\n",
      "Epoch 001  Batch 1560/10240  Batch Loss: 3.0929  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7210\n",
      "Epoch 001  Batch 1561/10240  Batch Loss: 1.8460  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7209\n",
      "Epoch 001  Batch 1562/10240  Batch Loss: 2.2726  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7207\n",
      "Epoch 001  Batch 1563/10240  Batch Loss: 1.5594  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7206\n",
      "Epoch 001  Batch 1564/10240  Batch Loss: 1.0597  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7206\n",
      "Epoch 001  Batch 1565/10240  Batch Loss: 3.4312  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7205\n",
      "Epoch 001  Batch 1566/10240  Batch Loss: 1.5598  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7205\n",
      "Epoch 001  Batch 1567/10240  Batch Loss: 3.8461  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7201\n",
      "Epoch 001  Batch 1568/10240  Batch Loss: 2.9191  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7200\n",
      "Epoch 001  Batch 1569/10240  Batch Loss: 2.0011  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7200\n",
      "Epoch 001  Batch 1570/10240  Batch Loss: 1.1666  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7200\n",
      "Epoch 001  Batch 1571/10240  Batch Loss: 1.5437  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7200\n",
      "Epoch 001  Batch 1572/10240  Batch Loss: 1.8736  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7200\n",
      "Epoch 001  Batch 1573/10240  Batch Loss: 1.1599  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7200\n",
      "Epoch 001  Batch 1574/10240  Batch Loss: 2.2794  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7198\n",
      "Epoch 001  Batch 1575/10240  Batch Loss: 1.1326  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7198\n",
      "Epoch 001  Batch 1576/10240  Batch Loss: 2.1151  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7197\n",
      "Epoch 001  Batch 1577/10240  Batch Loss: 1.5104  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7197\n",
      "Epoch 001  Batch 1578/10240  Batch Loss: 1.2869  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7197\n",
      "Epoch 001  Batch 1579/10240  Batch Loss: 3.6531  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7193\n",
      "Epoch 001  Batch 1580/10240  Batch Loss: 1.8017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7193\n",
      "Epoch 001  Batch 1581/10240  Batch Loss: 1.6907  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7192\n",
      "Epoch 001  Batch 1582/10240  Batch Loss: 1.9694  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7191\n",
      "Epoch 001  Batch 1583/10240  Batch Loss: 1.8887  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7191\n",
      "Epoch 001  Batch 1584/10240  Batch Loss: 2.0367  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7189\n",
      "Epoch 001  Batch 1585/10240  Batch Loss: 1.8543  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7189\n",
      "Epoch 001  Batch 1586/10240  Batch Loss: 1.3023  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7189\n",
      "Epoch 001  Batch 1587/10240  Batch Loss: 1.4421  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7188\n",
      "Epoch 001  Batch 1588/10240  Batch Loss: 1.1838  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7188\n",
      "Epoch 001  Batch 1589/10240  Batch Loss: 1.2273  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7188\n",
      "Epoch 001  Batch 1590/10240  Batch Loss: 1.4501  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7187\n",
      "Epoch 001  Batch 1591/10240  Batch Loss: 1.4297  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7186\n",
      "Epoch 001  Batch 1592/10240  Batch Loss: 1.6091  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7186\n",
      "Epoch 001  Batch 1593/10240  Batch Loss: 1.2528  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7186\n",
      "Epoch 001  Batch 1594/10240  Batch Loss: 2.7401  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7185\n",
      "Epoch 001  Batch 1595/10240  Batch Loss: 1.3507  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7185\n",
      "Epoch 001  Batch 1596/10240  Batch Loss: 1.0393  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7185\n",
      "Epoch 001  Batch 1597/10240  Batch Loss: 2.6152  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7184\n",
      "Epoch 001  Batch 1598/10240  Batch Loss: 2.0933  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7184\n",
      "Epoch 001  Batch 1599/10240  Batch Loss: 2.8458  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7184\n",
      "Epoch 001  Batch 1600/10240  Batch Loss: 1.8158  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7184\n",
      "Epoch 001  Batch 1601/10240  Batch Loss: 2.3970  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7183\n",
      "Epoch 001  Batch 1602/10240  Batch Loss: 2.4996  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7182\n",
      "Epoch 001  Batch 1603/10240  Batch Loss: 1.0643  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7182\n",
      "Epoch 001  Batch 1604/10240  Batch Loss: 1.8949  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7181\n",
      "Epoch 001  Batch 1605/10240  Batch Loss: 1.0097  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7181\n",
      "Epoch 001  Batch 1606/10240  Batch Loss: 1.6804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7181\n",
      "Epoch 001  Batch 1607/10240  Batch Loss: 2.1779  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7179\n",
      "Epoch 001  Batch 1608/10240  Batch Loss: 2.4140  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7178\n",
      "Epoch 001  Batch 1609/10240  Batch Loss: 1.5118  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7178\n",
      "Epoch 001  Batch 1610/10240  Batch Loss: 1.3946  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7177\n",
      "Epoch 001  Batch 1611/10240  Batch Loss: 1.8807  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7176\n",
      "Epoch 001  Batch 1612/10240  Batch Loss: 4.8833  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7173\n",
      "Epoch 001  Batch 1613/10240  Batch Loss: 2.2625  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7172\n",
      "Epoch 001  Batch 1614/10240  Batch Loss: 1.0783  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7172\n",
      "Epoch 001  Batch 1615/10240  Batch Loss: 1.6598  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7172\n",
      "Epoch 001  Batch 1616/10240  Batch Loss: 1.5966  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7173\n",
      "Epoch 001  Batch 1617/10240  Batch Loss: 1.3806  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7173\n",
      "Epoch 001  Batch 1618/10240  Batch Loss: 2.3369  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7171\n",
      "Epoch 001  Batch 1619/10240  Batch Loss: 1.6132  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7170\n",
      "Epoch 001  Batch 1620/10240  Batch Loss: 1.9415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7168\n",
      "Epoch 001  Batch 1621/10240  Batch Loss: 1.3311  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7168\n",
      "Epoch 001  Batch 1622/10240  Batch Loss: 1.6421  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7167\n",
      "Epoch 001  Batch 1623/10240  Batch Loss: 2.1963  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7165\n",
      "Epoch 001  Batch 1624/10240  Batch Loss: 1.1969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7165\n",
      "Epoch 001  Batch 1625/10240  Batch Loss: 1.9714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7164\n",
      "Epoch 001  Batch 1626/10240  Batch Loss: 1.2764  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7164\n",
      "Epoch 001  Batch 1627/10240  Batch Loss: 1.8882  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7164\n",
      "Epoch 001  Batch 1628/10240  Batch Loss: 2.0422  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7162\n",
      "Epoch 001  Batch 1629/10240  Batch Loss: 2.0426  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7161\n",
      "Epoch 001  Batch 1630/10240  Batch Loss: 1.0846  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7161\n",
      "Epoch 001  Batch 1631/10240  Batch Loss: 2.7950  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1632/10240  Batch Loss: 1.4376  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1633/10240  Batch Loss: 2.3302  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1634/10240  Batch Loss: 1.0742  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1635/10240  Batch Loss: 1.1108  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1636/10240  Batch Loss: 1.8031  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1637/10240  Batch Loss: 2.0105  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1638/10240  Batch Loss: 1.0623  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7160\n",
      "Epoch 001  Batch 1639/10240  Batch Loss: 1.9004  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7159\n",
      "Epoch 001  Batch 1640/10240  Batch Loss: 1.0716  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7159\n",
      "Epoch 001  Batch 1641/10240  Batch Loss: 1.9470  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7158\n",
      "Epoch 001  Batch 1642/10240  Batch Loss: 1.7881  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7158\n",
      "Epoch 001  Batch 1643/10240  Batch Loss: 1.0467  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7158\n",
      "Epoch 001  Batch 1644/10240  Batch Loss: 1.6205  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7158\n",
      "Epoch 001  Batch 1645/10240  Batch Loss: 2.2682  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7156\n",
      "Epoch 001  Batch 1646/10240  Batch Loss: 1.3432  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7156\n",
      "Epoch 001  Batch 1647/10240  Batch Loss: 1.3105  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7156\n",
      "Epoch 001  Batch 1648/10240  Batch Loss: 3.3132  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7154\n",
      "Epoch 001  Batch 1649/10240  Batch Loss: 2.8848  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7151\n",
      "Epoch 001  Batch 1650/10240  Batch Loss: 2.1102  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7150\n",
      "Epoch 001  Batch 1651/10240  Batch Loss: 1.1806  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7150\n",
      "Epoch 001  Batch 1652/10240  Batch Loss: 0.9922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7150\n",
      "Epoch 001  Batch 1653/10240  Batch Loss: 1.2768  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7149\n",
      "Epoch 001  Batch 1654/10240  Batch Loss: 1.4077  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7149\n",
      "Epoch 001  Batch 1655/10240  Batch Loss: 1.0114  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7149\n",
      "Epoch 001  Batch 1656/10240  Batch Loss: 1.9562  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7148\n",
      "Epoch 001  Batch 1657/10240  Batch Loss: 2.3039  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7146\n",
      "Epoch 001  Batch 1658/10240  Batch Loss: 1.6630  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7145\n",
      "Epoch 001  Batch 1659/10240  Batch Loss: 1.2479  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7145\n",
      "Epoch 001  Batch 1660/10240  Batch Loss: 1.2699  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7145\n",
      "Epoch 001  Batch 1661/10240  Batch Loss: 2.4385  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7144\n",
      "Epoch 001  Batch 1662/10240  Batch Loss: 1.2113  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7144\n",
      "Epoch 001  Batch 1663/10240  Batch Loss: 1.8049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7143\n",
      "Epoch 001  Batch 1664/10240  Batch Loss: 2.5497  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7142\n",
      "Epoch 001  Batch 1665/10240  Batch Loss: 1.6411  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7142\n",
      "Epoch 001  Batch 1666/10240  Batch Loss: 2.6057  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7140\n",
      "Epoch 001  Batch 1667/10240  Batch Loss: 1.7350  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7139\n",
      "Epoch 001  Batch 1668/10240  Batch Loss: 2.6731  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7138\n",
      "Epoch 001  Batch 1669/10240  Batch Loss: 1.2464  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7137\n",
      "Epoch 001  Batch 1670/10240  Batch Loss: 1.5875  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7136\n",
      "Epoch 001  Batch 1671/10240  Batch Loss: 1.4202  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7136\n",
      "Epoch 001  Batch 1672/10240  Batch Loss: 2.8730  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7134\n",
      "Epoch 001  Batch 1673/10240  Batch Loss: 1.4613  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7134\n",
      "Epoch 001  Batch 1674/10240  Batch Loss: 3.2042  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7133\n",
      "Epoch 001  Batch 1675/10240  Batch Loss: 0.9738  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7133\n",
      "Epoch 001  Batch 1676/10240  Batch Loss: 1.1839  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7133\n",
      "Epoch 001  Batch 1677/10240  Batch Loss: 1.2885  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7132\n",
      "Epoch 001  Batch 1678/10240  Batch Loss: 2.0472  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7131\n",
      "Epoch 001  Batch 1679/10240  Batch Loss: 1.7581  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7131\n",
      "Epoch 001  Batch 1680/10240  Batch Loss: 1.0541  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7131\n",
      "Epoch 001  Batch 1681/10240  Batch Loss: 1.6199  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7131\n",
      "Epoch 001  Batch 1682/10240  Batch Loss: 1.7321  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7130\n",
      "Epoch 001  Batch 1683/10240  Batch Loss: 2.0296  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7130\n",
      "Epoch 001  Batch 1684/10240  Batch Loss: 3.1680  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7128\n",
      "Epoch 001  Batch 1685/10240  Batch Loss: 1.3642  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7129\n",
      "Epoch 001  Batch 1686/10240  Batch Loss: 1.4619  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7129\n",
      "Epoch 001  Batch 1687/10240  Batch Loss: 1.8718  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7128\n",
      "Epoch 001  Batch 1688/10240  Batch Loss: 2.3078  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7126\n",
      "Epoch 001  Batch 1689/10240  Batch Loss: 0.9972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7126\n",
      "Epoch 001  Batch 1690/10240  Batch Loss: 1.5197  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7126\n",
      "Epoch 001  Batch 1691/10240  Batch Loss: 1.5596  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7126\n",
      "Epoch 001  Batch 1692/10240  Batch Loss: 2.5041  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1693/10240  Batch Loss: 1.3642  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1694/10240  Batch Loss: 1.9103  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7124\n",
      "Epoch 001  Batch 1695/10240  Batch Loss: 1.7051  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1696/10240  Batch Loss: 1.6006  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1697/10240  Batch Loss: 1.0372  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1698/10240  Batch Loss: 1.0046  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1699/10240  Batch Loss: 1.0256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1700/10240  Batch Loss: 1.1316  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7123\n",
      "Epoch 001  Batch 1701/10240  Batch Loss: 2.6521  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7122\n",
      "Epoch 001  Batch 1702/10240  Batch Loss: 2.6400  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7120\n",
      "Epoch 001  Batch 1703/10240  Batch Loss: 1.5423  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7119\n",
      "Epoch 001  Batch 1704/10240  Batch Loss: 2.7143  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7119\n",
      "Epoch 001  Batch 1705/10240  Batch Loss: 1.7100  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7119\n",
      "Epoch 001  Batch 1706/10240  Batch Loss: 1.6024  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7119\n",
      "Epoch 001  Batch 1707/10240  Batch Loss: 1.1521  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7118\n",
      "Epoch 001  Batch 1708/10240  Batch Loss: 1.4740  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7118\n",
      "Epoch 001  Batch 1709/10240  Batch Loss: 1.0554  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7118\n",
      "Epoch 001  Batch 1710/10240  Batch Loss: 1.9799  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7117\n",
      "Epoch 001  Batch 1711/10240  Batch Loss: 1.9177  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7118\n",
      "Epoch 001  Batch 1712/10240  Batch Loss: 1.2772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7117\n",
      "Epoch 001  Batch 1713/10240  Batch Loss: 1.1042  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7118\n",
      "Epoch 001  Batch 1714/10240  Batch Loss: 2.1983  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7116\n",
      "Epoch 001  Batch 1715/10240  Batch Loss: 1.0178  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7116\n",
      "Epoch 001  Batch 1716/10240  Batch Loss: 2.6433  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7115\n",
      "Epoch 001  Batch 1717/10240  Batch Loss: 1.3602  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7115\n",
      "Epoch 001  Batch 1718/10240  Batch Loss: 1.3848  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7115\n",
      "Epoch 001  Batch 1719/10240  Batch Loss: 1.1045  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7115\n",
      "Epoch 001  Batch 1720/10240  Batch Loss: 1.6436  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7115\n",
      "Epoch 001  Batch 1721/10240  Batch Loss: 1.6901  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7114\n",
      "Epoch 001  Batch 1722/10240  Batch Loss: 1.8805  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7112\n",
      "Epoch 001  Batch 1723/10240  Batch Loss: 1.3734  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7112\n",
      "Epoch 001  Batch 1724/10240  Batch Loss: 1.6544  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7111\n",
      "Epoch 001  Batch 1725/10240  Batch Loss: 1.4114  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7111\n",
      "Epoch 001  Batch 1726/10240  Batch Loss: 2.2262  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7111\n",
      "Epoch 001  Batch 1727/10240  Batch Loss: 1.2869  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7110\n",
      "Epoch 001  Batch 1728/10240  Batch Loss: 1.6283  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7110\n",
      "Epoch 001  Batch 1729/10240  Batch Loss: 2.1634  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7109\n",
      "Epoch 001  Batch 1730/10240  Batch Loss: 1.7384  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7108\n",
      "Epoch 001  Batch 1731/10240  Batch Loss: 1.7588  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7108\n",
      "Epoch 001  Batch 1732/10240  Batch Loss: 1.7429  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7107\n",
      "Epoch 001  Batch 1733/10240  Batch Loss: 1.7442  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7107\n",
      "Epoch 001  Batch 1734/10240  Batch Loss: 1.9188  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7106\n",
      "Epoch 001  Batch 1735/10240  Batch Loss: 1.2296  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7106\n",
      "Epoch 001  Batch 1736/10240  Batch Loss: 1.7700  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7105\n",
      "Epoch 001  Batch 1737/10240  Batch Loss: 1.3785  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7105\n",
      "Epoch 001  Batch 1738/10240  Batch Loss: 2.9915  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7103\n",
      "Epoch 001  Batch 1739/10240  Batch Loss: 1.5244  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7103\n",
      "Epoch 001  Batch 1740/10240  Batch Loss: 2.0178  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7102\n",
      "Epoch 001  Batch 1741/10240  Batch Loss: 0.9918  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7102\n",
      "Epoch 001  Batch 1742/10240  Batch Loss: 3.2077  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7100\n",
      "Epoch 001  Batch 1743/10240  Batch Loss: 1.6119  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7100\n",
      "Epoch 001  Batch 1744/10240  Batch Loss: 1.3731  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7100\n",
      "Epoch 001  Batch 1745/10240  Batch Loss: 1.8660  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1746/10240  Batch Loss: 1.3627  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1747/10240  Batch Loss: 1.5473  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1748/10240  Batch Loss: 1.1539  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1749/10240  Batch Loss: 1.2274  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1750/10240  Batch Loss: 1.4610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1751/10240  Batch Loss: 1.4171  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1752/10240  Batch Loss: 0.9268  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1753/10240  Batch Loss: 1.1693  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7098\n",
      "Epoch 001  Batch 1754/10240  Batch Loss: 1.9488  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7097\n",
      "Epoch 001  Batch 1755/10240  Batch Loss: 2.5564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7096\n",
      "Epoch 001  Batch 1756/10240  Batch Loss: 1.6338  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7096\n",
      "Epoch 001  Batch 1757/10240  Batch Loss: 1.0808  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7096\n",
      "Epoch 001  Batch 1758/10240  Batch Loss: 1.3548  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7096\n",
      "Epoch 001  Batch 1759/10240  Batch Loss: 1.1625  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7096\n",
      "Epoch 001  Batch 1760/10240  Batch Loss: 2.0599  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7096\n",
      "Epoch 001  Batch 1761/10240  Batch Loss: 1.5417  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7095\n",
      "Epoch 001  Batch 1762/10240  Batch Loss: 2.5037  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7094\n",
      "Epoch 001  Batch 1763/10240  Batch Loss: 1.3736  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7094\n",
      "Epoch 001  Batch 1764/10240  Batch Loss: 1.0525  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7094\n",
      "Epoch 001  Batch 1765/10240  Batch Loss: 1.6553  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7093\n",
      "Epoch 001  Batch 1766/10240  Batch Loss: 1.9113  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7093\n",
      "Epoch 001  Batch 1767/10240  Batch Loss: 1.8581  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7092\n",
      "Epoch 001  Batch 1768/10240  Batch Loss: 1.0755  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7092\n",
      "Epoch 001  Batch 1769/10240  Batch Loss: 1.1644  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7092\n",
      "Epoch 001  Batch 1770/10240  Batch Loss: 0.9456  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7092\n",
      "Epoch 001  Batch 1771/10240  Batch Loss: 2.9115  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7090\n",
      "Epoch 001  Batch 1772/10240  Batch Loss: 1.2316  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7089\n",
      "Epoch 001  Batch 1773/10240  Batch Loss: 1.4624  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7089\n",
      "Epoch 001  Batch 1774/10240  Batch Loss: 2.0178  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7088\n",
      "Epoch 001  Batch 1775/10240  Batch Loss: 0.9517  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7088\n",
      "Epoch 001  Batch 1776/10240  Batch Loss: 1.0749  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7088\n",
      "Epoch 001  Batch 1777/10240  Batch Loss: 2.0276  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7088\n",
      "Epoch 001  Batch 1778/10240  Batch Loss: 1.9706  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7086\n",
      "Epoch 001  Batch 1779/10240  Batch Loss: 2.0180  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7085\n",
      "Epoch 001  Batch 1780/10240  Batch Loss: 1.4860  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7085\n",
      "Epoch 001  Batch 1781/10240  Batch Loss: 1.7159  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7084\n",
      "Epoch 001  Batch 1782/10240  Batch Loss: 2.3407  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7082\n",
      "Epoch 001  Batch 1783/10240  Batch Loss: 1.3361  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7081\n",
      "Epoch 001  Batch 1784/10240  Batch Loss: 2.7802  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7079\n",
      "Epoch 001  Batch 1785/10240  Batch Loss: 1.6316  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7078\n",
      "Epoch 001  Batch 1786/10240  Batch Loss: 1.2434  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7078\n",
      "Epoch 001  Batch 1787/10240  Batch Loss: 2.7510  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7076\n",
      "Epoch 001  Batch 1788/10240  Batch Loss: 0.9863  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7076\n",
      "Epoch 001  Batch 1789/10240  Batch Loss: 1.4728  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7076\n",
      "Epoch 001  Batch 1790/10240  Batch Loss: 1.0208  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7076\n",
      "Epoch 001  Batch 1791/10240  Batch Loss: 1.2002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7076\n",
      "Epoch 001  Batch 1792/10240  Batch Loss: 1.3124  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7075\n",
      "Epoch 001  Batch 1793/10240  Batch Loss: 1.6474  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7075\n",
      "Epoch 001  Batch 1794/10240  Batch Loss: 2.3135  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7073\n",
      "Epoch 001  Batch 1795/10240  Batch Loss: 2.5556  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7071\n",
      "Epoch 001  Batch 1796/10240  Batch Loss: 1.3780  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7071\n",
      "Epoch 001  Batch 1797/10240  Batch Loss: 1.8450  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7070\n",
      "Epoch 001  Batch 1798/10240  Batch Loss: 1.6278  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7070\n",
      "Epoch 001  Batch 1799/10240  Batch Loss: 1.6503  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7069\n",
      "Epoch 001  Batch 1800/10240  Batch Loss: 3.6390  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7068\n",
      "Epoch 001  Batch 1801/10240  Batch Loss: 1.5164  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7067\n",
      "Epoch 001  Batch 1802/10240  Batch Loss: 1.9034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7066\n",
      "Epoch 001  Batch 1803/10240  Batch Loss: 0.9700  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7066\n",
      "Epoch 001  Batch 1804/10240  Batch Loss: 1.0559  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7066\n",
      "Epoch 001  Batch 1805/10240  Batch Loss: 2.3582  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7066\n",
      "Epoch 001  Batch 1806/10240  Batch Loss: 1.8441  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7066\n",
      "Epoch 001  Batch 1807/10240  Batch Loss: 2.6252  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7065\n",
      "Epoch 001  Batch 1808/10240  Batch Loss: 1.6463  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7064\n",
      "Epoch 001  Batch 1809/10240  Batch Loss: 1.6092  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7064\n",
      "Epoch 001  Batch 1810/10240  Batch Loss: 1.4781  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7064\n",
      "Epoch 001  Batch 1811/10240  Batch Loss: 2.3497  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7062\n",
      "Epoch 001  Batch 1812/10240  Batch Loss: 1.7610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7061\n",
      "Epoch 001  Batch 1813/10240  Batch Loss: 1.2904  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7060\n",
      "Epoch 001  Batch 1814/10240  Batch Loss: 1.7221  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7059\n",
      "Epoch 001  Batch 1815/10240  Batch Loss: 2.7863  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7058\n",
      "Epoch 001  Batch 1816/10240  Batch Loss: 1.6079  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7057\n",
      "Epoch 001  Batch 1817/10240  Batch Loss: 1.5375  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7058\n",
      "Epoch 001  Batch 1818/10240  Batch Loss: 1.5749  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7058\n",
      "Epoch 001  Batch 1819/10240  Batch Loss: 3.3130  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1820/10240  Batch Loss: 1.5984  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7055\n",
      "Epoch 001  Batch 1821/10240  Batch Loss: 2.5688  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7055\n",
      "Epoch 001  Batch 1822/10240  Batch Loss: 0.9355  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7055\n",
      "Epoch 001  Batch 1823/10240  Batch Loss: 1.5908  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1824/10240  Batch Loss: 0.9452  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1825/10240  Batch Loss: 1.5119  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1826/10240  Batch Loss: 1.5203  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1827/10240  Batch Loss: 1.0308  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1828/10240  Batch Loss: 1.7040  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1829/10240  Batch Loss: 2.2627  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7056\n",
      "Epoch 001  Batch 1830/10240  Batch Loss: 1.8055  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7055\n",
      "Epoch 001  Batch 1831/10240  Batch Loss: 2.7431  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7053\n",
      "Epoch 001  Batch 1832/10240  Batch Loss: 1.0857  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7053\n",
      "Epoch 001  Batch 1833/10240  Batch Loss: 1.5895  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7052\n",
      "Epoch 001  Batch 1834/10240  Batch Loss: 2.0675  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7052\n",
      "Epoch 001  Batch 1835/10240  Batch Loss: 1.2596  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7052\n",
      "Epoch 001  Batch 1836/10240  Batch Loss: 2.0881  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7051\n",
      "Epoch 001  Batch 1837/10240  Batch Loss: 1.0018  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7051\n",
      "Epoch 001  Batch 1838/10240  Batch Loss: 1.9386  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7050\n",
      "Epoch 001  Batch 1839/10240  Batch Loss: 1.8546  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7049\n",
      "Epoch 001  Batch 1840/10240  Batch Loss: 4.0651  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7048\n",
      "Epoch 001  Batch 1841/10240  Batch Loss: 2.6745  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7046\n",
      "Epoch 001  Batch 1842/10240  Batch Loss: 1.7505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7045\n",
      "Epoch 001  Batch 1843/10240  Batch Loss: 1.5453  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7044\n",
      "Epoch 001  Batch 1844/10240  Batch Loss: 1.5788  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7044\n",
      "Epoch 001  Batch 1845/10240  Batch Loss: 3.1594  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7041\n",
      "Epoch 001  Batch 1846/10240  Batch Loss: 1.1894  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7041\n",
      "Epoch 001  Batch 1847/10240  Batch Loss: 1.1299  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7041\n",
      "Epoch 001  Batch 1848/10240  Batch Loss: 1.3129  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7041\n",
      "Epoch 001  Batch 1849/10240  Batch Loss: 2.4635  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7039\n",
      "Epoch 001  Batch 1850/10240  Batch Loss: 1.3080  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7039\n",
      "Epoch 001  Batch 1851/10240  Batch Loss: 1.0081  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7039\n",
      "Epoch 001  Batch 1852/10240  Batch Loss: 2.4713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7038\n",
      "Epoch 001  Batch 1853/10240  Batch Loss: 1.7234  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7038\n",
      "Epoch 001  Batch 1854/10240  Batch Loss: 1.3190  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7037\n",
      "Epoch 001  Batch 1855/10240  Batch Loss: 3.0707  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7036\n",
      "Epoch 001  Batch 1856/10240  Batch Loss: 2.8569  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7033\n",
      "Epoch 001  Batch 1857/10240  Batch Loss: 2.7305  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7031\n",
      "Epoch 001  Batch 1858/10240  Batch Loss: 1.3613  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7032\n",
      "Epoch 001  Batch 1859/10240  Batch Loss: 2.3216  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7031\n",
      "Epoch 001  Batch 1860/10240  Batch Loss: 1.9012  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7030\n",
      "Epoch 001  Batch 1861/10240  Batch Loss: 1.9540  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7030\n",
      "Epoch 001  Batch 1862/10240  Batch Loss: 1.3786  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7030\n",
      "Epoch 001  Batch 1863/10240  Batch Loss: 1.3859  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7030\n",
      "Epoch 001  Batch 1864/10240  Batch Loss: 1.1021  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7030\n",
      "Epoch 001  Batch 1865/10240  Batch Loss: 1.4620  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7030\n",
      "Epoch 001  Batch 1866/10240  Batch Loss: 1.2451  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7029\n",
      "Epoch 001  Batch 1867/10240  Batch Loss: 1.1943  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7029\n",
      "Epoch 001  Batch 1868/10240  Batch Loss: 1.2514  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7029\n",
      "Epoch 001  Batch 1869/10240  Batch Loss: 1.5245  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7029\n",
      "Epoch 001  Batch 1870/10240  Batch Loss: 1.5074  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7028\n",
      "Epoch 001  Batch 1871/10240  Batch Loss: 1.6798  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7028\n",
      "Epoch 001  Batch 1872/10240  Batch Loss: 2.1867  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7028\n",
      "Epoch 001  Batch 1873/10240  Batch Loss: 2.2258  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7028\n",
      "Epoch 001  Batch 1874/10240  Batch Loss: 1.6595  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7028\n",
      "Epoch 001  Batch 1875/10240  Batch Loss: 1.6836  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7027\n",
      "Epoch 001  Batch 1876/10240  Batch Loss: 1.6962  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7027\n",
      "Epoch 001  Batch 1877/10240  Batch Loss: 1.5904  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7026\n",
      "Epoch 001  Batch 1878/10240  Batch Loss: 2.6395  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7024\n",
      "Epoch 001  Batch 1879/10240  Batch Loss: 2.4557  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7021\n",
      "Epoch 001  Batch 1880/10240  Batch Loss: 1.6794  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7021\n",
      "Epoch 001  Batch 1881/10240  Batch Loss: 2.0546  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7021\n",
      "Epoch 001  Batch 1882/10240  Batch Loss: 1.4074  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7020\n",
      "Epoch 001  Batch 1883/10240  Batch Loss: 1.7830  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7020\n",
      "Epoch 001  Batch 1884/10240  Batch Loss: 2.3882  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7018\n",
      "Epoch 001  Batch 1885/10240  Batch Loss: 1.9483  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7018\n",
      "Epoch 001  Batch 1886/10240  Batch Loss: 1.5602  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7017\n",
      "Epoch 001  Batch 1887/10240  Batch Loss: 2.0702  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7016\n",
      "Epoch 001  Batch 1888/10240  Batch Loss: 1.2028  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7016\n",
      "Epoch 001  Batch 1889/10240  Batch Loss: 2.3153  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7015\n",
      "Epoch 001  Batch 1890/10240  Batch Loss: 1.4302  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7014\n",
      "Epoch 001  Batch 1891/10240  Batch Loss: 2.8772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7012\n",
      "Epoch 001  Batch 1892/10240  Batch Loss: 2.1795  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7011\n",
      "Epoch 001  Batch 1893/10240  Batch Loss: 0.9454  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7011\n",
      "Epoch 001  Batch 1894/10240  Batch Loss: 1.8624  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7011\n",
      "Epoch 001  Batch 1895/10240  Batch Loss: 1.5772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7010\n",
      "Epoch 001  Batch 1896/10240  Batch Loss: 1.3056  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7010\n",
      "Epoch 001  Batch 1897/10240  Batch Loss: 1.9829  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7010\n",
      "Epoch 001  Batch 1898/10240  Batch Loss: 1.9830  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7008\n",
      "Epoch 001  Batch 1899/10240  Batch Loss: 2.0326  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7008\n",
      "Epoch 001  Batch 1900/10240  Batch Loss: 3.0432  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7006\n",
      "Epoch 001  Batch 1901/10240  Batch Loss: 0.9323  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7006\n",
      "Epoch 001  Batch 1902/10240  Batch Loss: 4.3537  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7003\n",
      "Epoch 001  Batch 1903/10240  Batch Loss: 1.6292  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7003\n",
      "Epoch 001  Batch 1904/10240  Batch Loss: 1.2340  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7002\n",
      "Epoch 001  Batch 1905/10240  Batch Loss: 1.2832  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7002\n",
      "Epoch 001  Batch 1906/10240  Batch Loss: 2.9592  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7001\n",
      "Epoch 001  Batch 1907/10240  Batch Loss: 1.4925  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.7001\n",
      "Epoch 001  Batch 1908/10240  Batch Loss: 2.1098  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6999\n",
      "Epoch 001  Batch 1909/10240  Batch Loss: 1.0523  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6999\n",
      "Epoch 001  Batch 1910/10240  Batch Loss: 2.3106  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6998\n",
      "Epoch 001  Batch 1911/10240  Batch Loss: 1.7371  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6997\n",
      "Epoch 001  Batch 1912/10240  Batch Loss: 1.2588  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6997\n",
      "Epoch 001  Batch 1913/10240  Batch Loss: 2.1332  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6997\n",
      "Epoch 001  Batch 1914/10240  Batch Loss: 2.5446  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6997\n",
      "Epoch 001  Batch 1915/10240  Batch Loss: 1.2809  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6997\n",
      "Epoch 001  Batch 1916/10240  Batch Loss: 1.8067  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6996\n",
      "Epoch 001  Batch 1917/10240  Batch Loss: 2.3493  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6995\n",
      "Epoch 001  Batch 1918/10240  Batch Loss: 2.4653  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6995\n",
      "Epoch 001  Batch 1919/10240  Batch Loss: 0.9621  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6995\n",
      "Epoch 001  Batch 1920/10240  Batch Loss: 1.1369  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6994\n",
      "Epoch 001  Batch 1921/10240  Batch Loss: 1.9512  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6993\n",
      "Epoch 001  Batch 1922/10240  Batch Loss: 2.2376  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6992\n",
      "Epoch 001  Batch 1923/10240  Batch Loss: 1.1408  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6992\n",
      "Epoch 001  Batch 1924/10240  Batch Loss: 1.8767  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6992\n",
      "Epoch 001  Batch 1925/10240  Batch Loss: 1.4398  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6991\n",
      "Epoch 001  Batch 1926/10240  Batch Loss: 1.6311  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6991\n",
      "Epoch 001  Batch 1927/10240  Batch Loss: 1.4346  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6991\n",
      "Epoch 001  Batch 1928/10240  Batch Loss: 1.3143  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6991\n",
      "Epoch 001  Batch 1929/10240  Batch Loss: 1.2944  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6991\n",
      "Epoch 001  Batch 1930/10240  Batch Loss: 2.8157  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6990\n",
      "Epoch 001  Batch 1931/10240  Batch Loss: 2.1308  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6988\n",
      "Epoch 001  Batch 1932/10240  Batch Loss: 1.6850  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6988\n",
      "Epoch 001  Batch 1933/10240  Batch Loss: 0.9271  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6988\n",
      "Epoch 001  Batch 1934/10240  Batch Loss: 2.9913  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6987\n",
      "Epoch 001  Batch 1935/10240  Batch Loss: 1.1511  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6987\n",
      "Epoch 001  Batch 1936/10240  Batch Loss: 1.8780  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6986\n",
      "Epoch 001  Batch 1937/10240  Batch Loss: 1.0804  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6986\n",
      "Epoch 001  Batch 1938/10240  Batch Loss: 3.0357  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6985\n",
      "Epoch 001  Batch 1939/10240  Batch Loss: 0.9504  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6985\n",
      "Epoch 001  Batch 1940/10240  Batch Loss: 1.5022  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6984\n",
      "Epoch 001  Batch 1941/10240  Batch Loss: 2.1050  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6983\n",
      "Epoch 001  Batch 1942/10240  Batch Loss: 1.0040  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6983\n",
      "Epoch 001  Batch 1943/10240  Batch Loss: 2.3795  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6981\n",
      "Epoch 001  Batch 1944/10240  Batch Loss: 0.9376  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6981\n",
      "Epoch 001  Batch 1945/10240  Batch Loss: 2.3091  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6981\n",
      "Epoch 001  Batch 1946/10240  Batch Loss: 2.0029  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6980\n",
      "Epoch 001  Batch 1947/10240  Batch Loss: 1.0466  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6980\n",
      "Epoch 001  Batch 1948/10240  Batch Loss: 1.2488  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6980\n",
      "Epoch 001  Batch 1949/10240  Batch Loss: 2.2252  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6980\n",
      "Epoch 001  Batch 1950/10240  Batch Loss: 2.3753  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6978\n",
      "Epoch 001  Batch 1951/10240  Batch Loss: 2.6480  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6977\n",
      "Epoch 001  Batch 1952/10240  Batch Loss: 2.0761  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6975\n",
      "Epoch 001  Batch 1953/10240  Batch Loss: 0.9419  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6975\n",
      "Epoch 001  Batch 1954/10240  Batch Loss: 1.7654  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6974\n",
      "Epoch 001  Batch 1955/10240  Batch Loss: 1.8400  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6974\n",
      "Epoch 001  Batch 1956/10240  Batch Loss: 1.1577  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6974\n",
      "Epoch 001  Batch 1957/10240  Batch Loss: 1.7582  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6974\n",
      "Epoch 001  Batch 1958/10240  Batch Loss: 2.6058  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6972\n",
      "Epoch 001  Batch 1959/10240  Batch Loss: 1.9803  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6971\n",
      "Epoch 001  Batch 1960/10240  Batch Loss: 2.5510  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6970\n",
      "Epoch 001  Batch 1961/10240  Batch Loss: 1.3469  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6969\n",
      "Epoch 001  Batch 1962/10240  Batch Loss: 0.9779  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6969\n",
      "Epoch 001  Batch 1963/10240  Batch Loss: 1.3982  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6970\n",
      "Epoch 001  Batch 1964/10240  Batch Loss: 1.9791  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6968\n",
      "Epoch 001  Batch 1965/10240  Batch Loss: 1.2976  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6968\n",
      "Epoch 001  Batch 1966/10240  Batch Loss: 1.5733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6968\n",
      "Epoch 001  Batch 1967/10240  Batch Loss: 1.2304  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6968\n",
      "Epoch 001  Batch 1968/10240  Batch Loss: 2.6611  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6966\n",
      "Epoch 001  Batch 1969/10240  Batch Loss: 2.2334  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6966\n",
      "Epoch 001  Batch 1970/10240  Batch Loss: 2.2539  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6964\n",
      "Epoch 001  Batch 1971/10240  Batch Loss: 0.9365  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6964\n",
      "Epoch 001  Batch 1972/10240  Batch Loss: 2.6087  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6963\n",
      "Epoch 001  Batch 1973/10240  Batch Loss: 1.6899  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6963\n",
      "Epoch 001  Batch 1974/10240  Batch Loss: 1.9702  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6962\n",
      "Epoch 001  Batch 1975/10240  Batch Loss: 2.3715  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6960\n",
      "Epoch 001  Batch 1976/10240  Batch Loss: 0.9403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6960\n",
      "Epoch 001  Batch 1977/10240  Batch Loss: 2.0108  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6959\n",
      "Epoch 001  Batch 1978/10240  Batch Loss: 2.4064  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6958\n",
      "Epoch 001  Batch 1979/10240  Batch Loss: 2.8921  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6957\n",
      "Epoch 001  Batch 1980/10240  Batch Loss: 1.9450  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6957\n",
      "Epoch 001  Batch 1981/10240  Batch Loss: 2.1919  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6956\n",
      "Epoch 001  Batch 1982/10240  Batch Loss: 1.9657  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6955\n",
      "Epoch 001  Batch 1983/10240  Batch Loss: 1.7690  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6954\n",
      "Epoch 001  Batch 1984/10240  Batch Loss: 1.1089  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6954\n",
      "Epoch 001  Batch 1985/10240  Batch Loss: 1.9388  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6952\n",
      "Epoch 001  Batch 1986/10240  Batch Loss: 2.1792  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6952\n",
      "Epoch 001  Batch 1987/10240  Batch Loss: 1.6498  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6951\n",
      "Epoch 001  Batch 1988/10240  Batch Loss: 3.0810  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6949\n",
      "Epoch 001  Batch 1989/10240  Batch Loss: 1.7269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6949\n",
      "Epoch 001  Batch 1990/10240  Batch Loss: 1.9020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6949\n",
      "Epoch 001  Batch 1991/10240  Batch Loss: 2.9442  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6947\n",
      "Epoch 001  Batch 1992/10240  Batch Loss: 2.0093  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6947\n",
      "Epoch 001  Batch 1993/10240  Batch Loss: 1.6060  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6947\n",
      "Epoch 001  Batch 1994/10240  Batch Loss: 1.1026  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6947\n",
      "Epoch 001  Batch 1995/10240  Batch Loss: 1.1329  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6946\n",
      "Epoch 001  Batch 1996/10240  Batch Loss: 1.8830  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6946\n",
      "Epoch 001  Batch 1997/10240  Batch Loss: 1.4807  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6946\n",
      "Epoch 001  Batch 1998/10240  Batch Loss: 0.9011  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6946\n",
      "Epoch 001  Batch 1999/10240  Batch Loss: 0.9761  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6946\n",
      "Epoch 001  Batch 2000/10240  Batch Loss: 1.0312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6946\n",
      "Epoch 001  Batch 2001/10240  Batch Loss: 1.4037  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2002/10240  Batch Loss: 1.1322  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2003/10240  Batch Loss: 1.6840  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2004/10240  Batch Loss: 1.6607  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2005/10240  Batch Loss: 1.0628  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2006/10240  Batch Loss: 0.8991  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2007/10240  Batch Loss: 2.5243  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2008/10240  Batch Loss: 1.4541  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6944\n",
      "Epoch 001  Batch 2009/10240  Batch Loss: 1.5850  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6944\n",
      "Epoch 001  Batch 2010/10240  Batch Loss: 1.3485  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2011/10240  Batch Loss: 1.6114  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2012/10240  Batch Loss: 0.9392  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2013/10240  Batch Loss: 1.5144  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6945\n",
      "Epoch 001  Batch 2014/10240  Batch Loss: 1.8615  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6944\n",
      "Epoch 001  Batch 2015/10240  Batch Loss: 1.7390  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6943\n",
      "Epoch 001  Batch 2016/10240  Batch Loss: 1.6776  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6942\n",
      "Epoch 001  Batch 2017/10240  Batch Loss: 2.2183  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6941\n",
      "Epoch 001  Batch 2018/10240  Batch Loss: 1.9423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6941\n",
      "Epoch 001  Batch 2019/10240  Batch Loss: 1.9653  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6940\n",
      "Epoch 001  Batch 2020/10240  Batch Loss: 0.9321  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6940\n",
      "Epoch 001  Batch 2021/10240  Batch Loss: 2.9882  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6939\n",
      "Epoch 001  Batch 2022/10240  Batch Loss: 1.9586  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6940\n",
      "Epoch 001  Batch 2023/10240  Batch Loss: 0.9489  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6940\n",
      "Epoch 001  Batch 2024/10240  Batch Loss: 2.5652  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6939\n",
      "Epoch 001  Batch 2025/10240  Batch Loss: 1.6230  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6939\n",
      "Epoch 001  Batch 2026/10240  Batch Loss: 0.9688  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6939\n",
      "Epoch 001  Batch 2027/10240  Batch Loss: 3.1965  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6937\n",
      "Epoch 001  Batch 2028/10240  Batch Loss: 2.1473  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6936\n",
      "Epoch 001  Batch 2029/10240  Batch Loss: 1.2110  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6936\n",
      "Epoch 001  Batch 2030/10240  Batch Loss: 1.0816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6936\n",
      "Epoch 001  Batch 2031/10240  Batch Loss: 1.1778  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6936\n",
      "Epoch 001  Batch 2032/10240  Batch Loss: 2.0159  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6936\n",
      "Epoch 001  Batch 2033/10240  Batch Loss: 0.9322  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6936\n",
      "Epoch 001  Batch 2034/10240  Batch Loss: 1.5023  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6935\n",
      "Epoch 001  Batch 2035/10240  Batch Loss: 2.1674  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6935\n",
      "Epoch 001  Batch 2036/10240  Batch Loss: 1.7542  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6934\n",
      "Epoch 001  Batch 2037/10240  Batch Loss: 2.9842  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6933\n",
      "Epoch 001  Batch 2038/10240  Batch Loss: 2.7068  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6932\n",
      "Epoch 001  Batch 2039/10240  Batch Loss: 3.6441  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6929\n",
      "Epoch 001  Batch 2040/10240  Batch Loss: 1.3348  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6929\n",
      "Epoch 001  Batch 2041/10240  Batch Loss: 1.2550  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6929\n",
      "Epoch 001  Batch 2042/10240  Batch Loss: 1.7199  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6928\n",
      "Epoch 001  Batch 2043/10240  Batch Loss: 1.6073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6928\n",
      "Epoch 001  Batch 2044/10240  Batch Loss: 1.1710  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6928\n",
      "Epoch 001  Batch 2045/10240  Batch Loss: 2.2831  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6928\n",
      "Epoch 001  Batch 2046/10240  Batch Loss: 1.5069  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6927\n",
      "Epoch 001  Batch 2047/10240  Batch Loss: 2.6572  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6926\n",
      "Epoch 001  Batch 2048/10240  Batch Loss: 1.4963  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2049/10240  Batch Loss: 1.7591  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2050/10240  Batch Loss: 1.5734  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2051/10240  Batch Loss: 0.9805  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2052/10240  Batch Loss: 1.0923  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2053/10240  Batch Loss: 0.9042  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2054/10240  Batch Loss: 1.1546  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2055/10240  Batch Loss: 0.9767  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6925\n",
      "Epoch 001  Batch 2056/10240  Batch Loss: 1.6914  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6924\n",
      "Epoch 001  Batch 2057/10240  Batch Loss: 2.7198  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6922\n",
      "Epoch 001  Batch 2058/10240  Batch Loss: 1.8072  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6922\n",
      "Epoch 001  Batch 2059/10240  Batch Loss: 2.3767  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6923\n",
      "Epoch 001  Batch 2060/10240  Batch Loss: 0.8900  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6923\n",
      "Epoch 001  Batch 2061/10240  Batch Loss: 0.9376  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6923\n",
      "Epoch 001  Batch 2062/10240  Batch Loss: 2.4223  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6922\n",
      "Epoch 001  Batch 2063/10240  Batch Loss: 1.0864  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6922\n",
      "Epoch 001  Batch 2064/10240  Batch Loss: 2.2915  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6920\n",
      "Epoch 001  Batch 2065/10240  Batch Loss: 1.5766  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6920\n",
      "Epoch 001  Batch 2066/10240  Batch Loss: 1.0249  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6920\n",
      "Epoch 001  Batch 2067/10240  Batch Loss: 0.9361  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6920\n",
      "Epoch 001  Batch 2068/10240  Batch Loss: 2.3597  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6918\n",
      "Epoch 001  Batch 2069/10240  Batch Loss: 1.0489  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6918\n",
      "Epoch 001  Batch 2070/10240  Batch Loss: 2.1475  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6918\n",
      "Epoch 001  Batch 2071/10240  Batch Loss: 1.9314  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6917\n",
      "Epoch 001  Batch 2072/10240  Batch Loss: 1.4983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6917\n",
      "Epoch 001  Batch 2073/10240  Batch Loss: 0.9709  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6917\n",
      "Epoch 001  Batch 2074/10240  Batch Loss: 2.0261  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6915\n",
      "Epoch 001  Batch 2075/10240  Batch Loss: 0.9020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6915\n",
      "Epoch 001  Batch 2076/10240  Batch Loss: 1.3169  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6915\n",
      "Epoch 001  Batch 2077/10240  Batch Loss: 1.2378  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6914\n",
      "Epoch 001  Batch 2078/10240  Batch Loss: 2.3642  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6913\n",
      "Epoch 001  Batch 2079/10240  Batch Loss: 1.1265  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6912\n",
      "Epoch 001  Batch 2080/10240  Batch Loss: 1.4136  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6912\n",
      "Epoch 001  Batch 2081/10240  Batch Loss: 1.3747  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6912\n",
      "Epoch 001  Batch 2082/10240  Batch Loss: 0.9629  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6912\n",
      "Epoch 001  Batch 2083/10240  Batch Loss: 2.5534  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6911\n",
      "Epoch 001  Batch 2084/10240  Batch Loss: 0.9291  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6911\n",
      "Epoch 001  Batch 2085/10240  Batch Loss: 1.0108  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6911\n",
      "Epoch 001  Batch 2086/10240  Batch Loss: 2.4118  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6910\n",
      "Epoch 001  Batch 2087/10240  Batch Loss: 1.4814  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6909\n",
      "Epoch 001  Batch 2088/10240  Batch Loss: 1.1432  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6909\n",
      "Epoch 001  Batch 2089/10240  Batch Loss: 1.7528  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6909\n",
      "Epoch 001  Batch 2090/10240  Batch Loss: 2.5558  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6908\n",
      "Epoch 001  Batch 2091/10240  Batch Loss: 1.3154  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6907\n",
      "Epoch 001  Batch 2092/10240  Batch Loss: 2.1532  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6907\n",
      "Epoch 001  Batch 2093/10240  Batch Loss: 1.3507  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6907\n",
      "Epoch 001  Batch 2094/10240  Batch Loss: 1.6176  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6906\n",
      "Epoch 001  Batch 2095/10240  Batch Loss: 1.6917  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6906\n",
      "Epoch 001  Batch 2096/10240  Batch Loss: 1.1733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6906\n",
      "Epoch 001  Batch 2097/10240  Batch Loss: 1.0169  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6906\n",
      "Epoch 001  Batch 2098/10240  Batch Loss: 1.2981  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6906\n",
      "Epoch 001  Batch 2099/10240  Batch Loss: 1.8590  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6905\n",
      "Epoch 001  Batch 2100/10240  Batch Loss: 1.2177  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6905\n",
      "Epoch 001  Batch 2101/10240  Batch Loss: 1.5607  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6904\n",
      "Epoch 001  Batch 2102/10240  Batch Loss: 1.0793  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6904\n",
      "Epoch 001  Batch 2103/10240  Batch Loss: 1.7456  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6903\n",
      "Epoch 001  Batch 2104/10240  Batch Loss: 1.2518  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6903\n",
      "Epoch 001  Batch 2105/10240  Batch Loss: 1.2877  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6903\n",
      "Epoch 001  Batch 2106/10240  Batch Loss: 1.3481  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6903\n",
      "Epoch 001  Batch 2107/10240  Batch Loss: 2.3246  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6901\n",
      "Epoch 001  Batch 2108/10240  Batch Loss: 1.0093  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6901\n",
      "Epoch 001  Batch 2109/10240  Batch Loss: 2.9822  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6899\n",
      "Epoch 001  Batch 2110/10240  Batch Loss: 1.3177  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6900\n",
      "Epoch 001  Batch 2111/10240  Batch Loss: 1.5092  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6900\n",
      "Epoch 001  Batch 2112/10240  Batch Loss: 1.3310  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6899\n",
      "Epoch 001  Batch 2113/10240  Batch Loss: 1.4173  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6899\n",
      "Epoch 001  Batch 2114/10240  Batch Loss: 1.4913  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6898\n",
      "Epoch 001  Batch 2115/10240  Batch Loss: 1.5855  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6898\n",
      "Epoch 001  Batch 2116/10240  Batch Loss: 1.7323  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6897\n",
      "Epoch 001  Batch 2117/10240  Batch Loss: 1.5153  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6897\n",
      "Epoch 001  Batch 2118/10240  Batch Loss: 1.4644  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6896\n",
      "Epoch 001  Batch 2119/10240  Batch Loss: 3.2563  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6894\n",
      "Epoch 001  Batch 2120/10240  Batch Loss: 2.5917  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6894\n",
      "Epoch 001  Batch 2121/10240  Batch Loss: 1.1973  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6894\n",
      "Epoch 001  Batch 2122/10240  Batch Loss: 2.6598  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6893\n",
      "Epoch 001  Batch 2123/10240  Batch Loss: 1.4837  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6892\n",
      "Epoch 001  Batch 2124/10240  Batch Loss: 2.2185  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6891\n",
      "Epoch 001  Batch 2125/10240  Batch Loss: 2.0503  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6890\n",
      "Epoch 001  Batch 2126/10240  Batch Loss: 1.8094  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6890\n",
      "Epoch 001  Batch 2127/10240  Batch Loss: 1.0553  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6890\n",
      "Epoch 001  Batch 2128/10240  Batch Loss: 1.2926  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6890\n",
      "Epoch 001  Batch 2129/10240  Batch Loss: 1.6331  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6890\n",
      "Epoch 001  Batch 2130/10240  Batch Loss: 2.3312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6889\n",
      "Epoch 001  Batch 2131/10240  Batch Loss: 1.6520  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6888\n",
      "Epoch 001  Batch 2132/10240  Batch Loss: 1.2663  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6888\n",
      "Epoch 001  Batch 2133/10240  Batch Loss: 1.6014  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6887\n",
      "Epoch 001  Batch 2134/10240  Batch Loss: 1.2426  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6887\n",
      "Epoch 001  Batch 2135/10240  Batch Loss: 0.8665  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6887\n",
      "Epoch 001  Batch 2136/10240  Batch Loss: 2.0357  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6886\n",
      "Epoch 001  Batch 2137/10240  Batch Loss: 0.8487  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6886\n",
      "Epoch 001  Batch 2138/10240  Batch Loss: 1.7073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6885\n",
      "Epoch 001  Batch 2139/10240  Batch Loss: 1.7631  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6884\n",
      "Epoch 001  Batch 2140/10240  Batch Loss: 1.7453  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6884\n",
      "Epoch 001  Batch 2141/10240  Batch Loss: 1.4445  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6883\n",
      "Epoch 001  Batch 2142/10240  Batch Loss: 1.4403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6883\n",
      "Epoch 001  Batch 2143/10240  Batch Loss: 1.0717  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6883\n",
      "Epoch 001  Batch 2144/10240  Batch Loss: 1.4359  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6882\n",
      "Epoch 001  Batch 2145/10240  Batch Loss: 1.3921  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6882\n",
      "Epoch 001  Batch 2146/10240  Batch Loss: 2.4285  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6880\n",
      "Epoch 001  Batch 2147/10240  Batch Loss: 1.9160  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6880\n",
      "Epoch 001  Batch 2148/10240  Batch Loss: 0.9037  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6880\n",
      "Epoch 001  Batch 2149/10240  Batch Loss: 1.5760  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6879\n",
      "Epoch 001  Batch 2150/10240  Batch Loss: 1.4020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6879\n",
      "Epoch 001  Batch 2151/10240  Batch Loss: 1.6261  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6878\n",
      "Epoch 001  Batch 2152/10240  Batch Loss: 1.7570  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2153/10240  Batch Loss: 0.8924  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2154/10240  Batch Loss: 1.0173  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2155/10240  Batch Loss: 1.0214  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2156/10240  Batch Loss: 0.9999  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2157/10240  Batch Loss: 0.8342  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2158/10240  Batch Loss: 1.5507  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6877\n",
      "Epoch 001  Batch 2159/10240  Batch Loss: 1.3192  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6876\n",
      "Epoch 001  Batch 2160/10240  Batch Loss: 1.2427  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6876\n",
      "Epoch 001  Batch 2161/10240  Batch Loss: 1.5795  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6875\n",
      "Epoch 001  Batch 2162/10240  Batch Loss: 1.3166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6874\n",
      "Epoch 001  Batch 2163/10240  Batch Loss: 1.2815  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6874\n",
      "Epoch 001  Batch 2164/10240  Batch Loss: 1.4710  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6874\n",
      "Epoch 001  Batch 2165/10240  Batch Loss: 2.5610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6873\n",
      "Epoch 001  Batch 2166/10240  Batch Loss: 2.2804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6871\n",
      "Epoch 001  Batch 2167/10240  Batch Loss: 1.0452  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6871\n",
      "Epoch 001  Batch 2168/10240  Batch Loss: 0.8790  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6871\n",
      "Epoch 001  Batch 2169/10240  Batch Loss: 1.6013  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6871\n",
      "Epoch 001  Batch 2170/10240  Batch Loss: 1.5002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6870\n",
      "Epoch 001  Batch 2171/10240  Batch Loss: 0.9809  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6870\n",
      "Epoch 001  Batch 2172/10240  Batch Loss: 1.6885  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6869\n",
      "Epoch 001  Batch 2173/10240  Batch Loss: 0.8893  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6869\n",
      "Epoch 001  Batch 2174/10240  Batch Loss: 0.9092  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6869\n",
      "Epoch 001  Batch 2175/10240  Batch Loss: 1.7481  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6869\n",
      "Epoch 001  Batch 2176/10240  Batch Loss: 0.9896  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6869\n",
      "Epoch 001  Batch 2177/10240  Batch Loss: 0.9111  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6869\n",
      "Epoch 001  Batch 2178/10240  Batch Loss: 1.5559  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6868\n",
      "Epoch 001  Batch 2179/10240  Batch Loss: 2.5246  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6867\n",
      "Epoch 001  Batch 2180/10240  Batch Loss: 1.6555  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6866\n",
      "Epoch 001  Batch 2181/10240  Batch Loss: 1.2060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6866\n",
      "Epoch 001  Batch 2182/10240  Batch Loss: 1.6131  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6866\n",
      "Epoch 001  Batch 2183/10240  Batch Loss: 1.0378  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6865\n",
      "Epoch 001  Batch 2184/10240  Batch Loss: 0.8341  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6865\n",
      "Epoch 001  Batch 2185/10240  Batch Loss: 2.4394  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6865\n",
      "Epoch 001  Batch 2186/10240  Batch Loss: 1.4662  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6865\n",
      "Epoch 001  Batch 2187/10240  Batch Loss: 2.6544  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6864\n",
      "Epoch 001  Batch 2188/10240  Batch Loss: 1.0180  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6864\n",
      "Epoch 001  Batch 2189/10240  Batch Loss: 2.6086  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6863\n",
      "Epoch 001  Batch 2190/10240  Batch Loss: 0.8515  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6863\n",
      "Epoch 001  Batch 2191/10240  Batch Loss: 2.2042  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6861\n",
      "Epoch 001  Batch 2192/10240  Batch Loss: 1.8713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6861\n",
      "Epoch 001  Batch 2193/10240  Batch Loss: 1.1886  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6861\n",
      "Epoch 001  Batch 2194/10240  Batch Loss: 0.9382  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6861\n",
      "Epoch 001  Batch 2195/10240  Batch Loss: 0.8612  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6861\n",
      "Epoch 001  Batch 2196/10240  Batch Loss: 1.7490  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6859\n",
      "Epoch 001  Batch 2197/10240  Batch Loss: 1.1739  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6859\n",
      "Epoch 001  Batch 2198/10240  Batch Loss: 0.8714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6859\n",
      "Epoch 001  Batch 2199/10240  Batch Loss: 3.3505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6857\n",
      "Epoch 001  Batch 2200/10240  Batch Loss: 1.8285  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6856\n",
      "Epoch 001  Batch 2201/10240  Batch Loss: 2.3745  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2202/10240  Batch Loss: 1.2325  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2203/10240  Batch Loss: 2.0060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6854\n",
      "Epoch 001  Batch 2204/10240  Batch Loss: 1.3881  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2205/10240  Batch Loss: 0.8463  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2206/10240  Batch Loss: 0.9923  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2207/10240  Batch Loss: 1.2447  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2208/10240  Batch Loss: 0.9086  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6855\n",
      "Epoch 001  Batch 2209/10240  Batch Loss: 3.2335  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6853\n",
      "Epoch 001  Batch 2210/10240  Batch Loss: 3.0238  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6850\n",
      "Epoch 001  Batch 2211/10240  Batch Loss: 1.5959  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6850\n",
      "Epoch 001  Batch 2212/10240  Batch Loss: 1.1350  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6849\n",
      "Epoch 001  Batch 2213/10240  Batch Loss: 1.0710  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6849\n",
      "Epoch 001  Batch 2214/10240  Batch Loss: 2.3952  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6848\n",
      "Epoch 001  Batch 2215/10240  Batch Loss: 1.0381  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6848\n",
      "Epoch 001  Batch 2216/10240  Batch Loss: 2.3292  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6846\n",
      "Epoch 001  Batch 2217/10240  Batch Loss: 3.3962  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6844\n",
      "Epoch 001  Batch 2218/10240  Batch Loss: 0.9001  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6844\n",
      "Epoch 001  Batch 2219/10240  Batch Loss: 0.9250  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6844\n",
      "Epoch 001  Batch 2220/10240  Batch Loss: 1.4250  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6843\n",
      "Epoch 001  Batch 2221/10240  Batch Loss: 1.9162  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6842\n",
      "Epoch 001  Batch 2222/10240  Batch Loss: 0.9300  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6842\n",
      "Epoch 001  Batch 2223/10240  Batch Loss: 1.5131  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6841\n",
      "Epoch 001  Batch 2224/10240  Batch Loss: 0.8358  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6841\n",
      "Epoch 001  Batch 2225/10240  Batch Loss: 1.0208  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6840\n",
      "Epoch 001  Batch 2226/10240  Batch Loss: 1.0393  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6840\n",
      "Epoch 001  Batch 2227/10240  Batch Loss: 1.1201  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6840\n",
      "Epoch 001  Batch 2228/10240  Batch Loss: 2.0317  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6839\n",
      "Epoch 001  Batch 2229/10240  Batch Loss: 1.1740  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6839\n",
      "Epoch 001  Batch 2230/10240  Batch Loss: 1.2291  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6839\n",
      "Epoch 001  Batch 2231/10240  Batch Loss: 0.8798  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6839\n",
      "Epoch 001  Batch 2232/10240  Batch Loss: 1.8984  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6839\n",
      "Epoch 001  Batch 2233/10240  Batch Loss: 1.6043  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6838\n",
      "Epoch 001  Batch 2234/10240  Batch Loss: 2.4397  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6838\n",
      "Epoch 001  Batch 2235/10240  Batch Loss: 0.8977  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6838\n",
      "Epoch 001  Batch 2236/10240  Batch Loss: 1.0923  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6838\n",
      "Epoch 001  Batch 2237/10240  Batch Loss: 2.2323  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6836\n",
      "Epoch 001  Batch 2238/10240  Batch Loss: 1.3986  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6836\n",
      "Epoch 001  Batch 2239/10240  Batch Loss: 1.3648  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6835\n",
      "Epoch 001  Batch 2240/10240  Batch Loss: 1.6321  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6835\n",
      "Epoch 001  Batch 2241/10240  Batch Loss: 0.9629  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6835\n",
      "Epoch 001  Batch 2242/10240  Batch Loss: 1.3278  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6834\n",
      "Epoch 001  Batch 2243/10240  Batch Loss: 1.6722  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6833\n",
      "Epoch 001  Batch 2244/10240  Batch Loss: 2.1931  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6833\n",
      "Epoch 001  Batch 2245/10240  Batch Loss: 2.5836  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6831\n",
      "Epoch 001  Batch 2246/10240  Batch Loss: 1.8942  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6829\n",
      "Epoch 001  Batch 2247/10240  Batch Loss: 0.8321  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6829\n",
      "Epoch 001  Batch 2248/10240  Batch Loss: 1.9279  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6829\n",
      "Epoch 001  Batch 2249/10240  Batch Loss: 1.5079  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6828\n",
      "Epoch 001  Batch 2250/10240  Batch Loss: 1.7024  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6829\n",
      "Epoch 001  Batch 2251/10240  Batch Loss: 0.9047  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6829\n",
      "Epoch 001  Batch 2252/10240  Batch Loss: 2.0504  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6828\n",
      "Epoch 001  Batch 2253/10240  Batch Loss: 2.0542  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6828\n",
      "Epoch 001  Batch 2254/10240  Batch Loss: 1.0901  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6828\n",
      "Epoch 001  Batch 2255/10240  Batch Loss: 2.5551  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6826\n",
      "Epoch 001  Batch 2256/10240  Batch Loss: 1.0253  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6826\n",
      "Epoch 001  Batch 2257/10240  Batch Loss: 1.1421  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6826\n",
      "Epoch 001  Batch 2258/10240  Batch Loss: 0.8714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6826\n",
      "Epoch 001  Batch 2259/10240  Batch Loss: 2.6096  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6824\n",
      "Epoch 001  Batch 2260/10240  Batch Loss: 1.0828  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6824\n",
      "Epoch 001  Batch 2261/10240  Batch Loss: 3.1221  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6823\n",
      "Epoch 001  Batch 2262/10240  Batch Loss: 2.2130  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6823\n",
      "Epoch 001  Batch 2263/10240  Batch Loss: 0.8635  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6823\n",
      "Epoch 001  Batch 2264/10240  Batch Loss: 1.0483  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6823\n",
      "Epoch 001  Batch 2265/10240  Batch Loss: 2.3322  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6822\n",
      "Epoch 001  Batch 2266/10240  Batch Loss: 1.4061  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6822\n",
      "Epoch 001  Batch 2267/10240  Batch Loss: 1.8555  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6821\n",
      "Epoch 001  Batch 2268/10240  Batch Loss: 1.0040  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6821\n",
      "Epoch 001  Batch 2269/10240  Batch Loss: 2.3683  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6820\n",
      "Epoch 001  Batch 2270/10240  Batch Loss: 1.2247  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6819\n",
      "Epoch 001  Batch 2271/10240  Batch Loss: 1.1049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6819\n",
      "Epoch 001  Batch 2272/10240  Batch Loss: 1.4029  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6820\n",
      "Epoch 001  Batch 2273/10240  Batch Loss: 1.1522  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6819\n",
      "Epoch 001  Batch 2274/10240  Batch Loss: 1.2579  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6819\n",
      "Epoch 001  Batch 2275/10240  Batch Loss: 1.3413  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6820\n",
      "Epoch 001  Batch 2276/10240  Batch Loss: 1.3064  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6819\n",
      "Epoch 001  Batch 2277/10240  Batch Loss: 2.8123  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6818\n",
      "Epoch 001  Batch 2278/10240  Batch Loss: 1.5908  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6817\n",
      "Epoch 001  Batch 2279/10240  Batch Loss: 0.8362  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6817\n",
      "Epoch 001  Batch 2280/10240  Batch Loss: 0.9835  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6817\n",
      "Epoch 001  Batch 2281/10240  Batch Loss: 1.1127  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6817\n",
      "Epoch 001  Batch 2282/10240  Batch Loss: 2.3861  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6816\n",
      "Epoch 001  Batch 2283/10240  Batch Loss: 2.0034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6815\n",
      "Epoch 001  Batch 2284/10240  Batch Loss: 2.0387  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6815\n",
      "Epoch 001  Batch 2285/10240  Batch Loss: 1.2090  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6815\n",
      "Epoch 001  Batch 2286/10240  Batch Loss: 2.1052  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6814\n",
      "Epoch 001  Batch 2287/10240  Batch Loss: 1.7584  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6814\n",
      "Epoch 001  Batch 2288/10240  Batch Loss: 1.0856  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6813\n",
      "Epoch 001  Batch 2289/10240  Batch Loss: 1.3280  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6813\n",
      "Epoch 001  Batch 2290/10240  Batch Loss: 2.0948  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6812\n",
      "Epoch 001  Batch 2291/10240  Batch Loss: 0.9764  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6812\n",
      "Epoch 001  Batch 2292/10240  Batch Loss: 0.8427  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6812\n",
      "Epoch 001  Batch 2293/10240  Batch Loss: 1.6348  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6811\n",
      "Epoch 001  Batch 2294/10240  Batch Loss: 1.4154  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6810\n",
      "Epoch 001  Batch 2295/10240  Batch Loss: 1.5095  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6810\n",
      "Epoch 001  Batch 2296/10240  Batch Loss: 2.6682  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6808\n",
      "Epoch 001  Batch 2297/10240  Batch Loss: 1.7782  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6808\n",
      "Epoch 001  Batch 2298/10240  Batch Loss: 2.4700  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6806\n",
      "Epoch 001  Batch 2299/10240  Batch Loss: 1.8017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6805\n",
      "Epoch 001  Batch 2300/10240  Batch Loss: 0.9595  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6805\n",
      "Epoch 001  Batch 2301/10240  Batch Loss: 2.1230  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6804\n",
      "Epoch 001  Batch 2302/10240  Batch Loss: 1.4493  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6803\n",
      "Epoch 001  Batch 2303/10240  Batch Loss: 2.4920  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6801\n",
      "Epoch 001  Batch 2304/10240  Batch Loss: 1.1137  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6801\n",
      "Epoch 001  Batch 2305/10240  Batch Loss: 3.0523  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6799\n",
      "Epoch 001  Batch 2306/10240  Batch Loss: 3.7969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6797\n",
      "Epoch 001  Batch 2307/10240  Batch Loss: 2.6032  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6797\n",
      "Epoch 001  Batch 2308/10240  Batch Loss: 1.2129  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6797\n",
      "Epoch 001  Batch 2309/10240  Batch Loss: 1.4726  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6798\n",
      "Epoch 001  Batch 2310/10240  Batch Loss: 1.4042  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6797\n",
      "Epoch 001  Batch 2311/10240  Batch Loss: 1.3249  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6797\n",
      "Epoch 001  Batch 2312/10240  Batch Loss: 0.9160  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6797\n",
      "Epoch 001  Batch 2313/10240  Batch Loss: 3.5154  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6795\n",
      "Epoch 001  Batch 2314/10240  Batch Loss: 1.7338  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6794\n",
      "Epoch 001  Batch 2315/10240  Batch Loss: 1.7531  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2316/10240  Batch Loss: 0.8486  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2317/10240  Batch Loss: 0.8889  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2318/10240  Batch Loss: 1.0256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2319/10240  Batch Loss: 0.8969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2320/10240  Batch Loss: 1.1677  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2321/10240  Batch Loss: 1.0843  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2322/10240  Batch Loss: 1.2683  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2323/10240  Batch Loss: 0.8488  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2324/10240  Batch Loss: 1.5637  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2325/10240  Batch Loss: 0.8670  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2326/10240  Batch Loss: 1.2675  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2327/10240  Batch Loss: 1.1461  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2328/10240  Batch Loss: 1.4448  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2329/10240  Batch Loss: 1.0495  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2330/10240  Batch Loss: 0.8478  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6793\n",
      "Epoch 001  Batch 2331/10240  Batch Loss: 2.2695  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6791\n",
      "Epoch 001  Batch 2332/10240  Batch Loss: 2.0032  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6790\n",
      "Epoch 001  Batch 2333/10240  Batch Loss: 0.8487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6790\n",
      "Epoch 001  Batch 2334/10240  Batch Loss: 2.5386  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6788\n",
      "Epoch 001  Batch 2335/10240  Batch Loss: 1.1761  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6788\n",
      "Epoch 001  Batch 2336/10240  Batch Loss: 1.9409  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6787\n",
      "Epoch 001  Batch 2337/10240  Batch Loss: 2.5274  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6785\n",
      "Epoch 001  Batch 2338/10240  Batch Loss: 1.5545  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6785\n",
      "Epoch 001  Batch 2339/10240  Batch Loss: 1.2401  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6785\n",
      "Epoch 001  Batch 2340/10240  Batch Loss: 2.0590  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6784\n",
      "Epoch 001  Batch 2341/10240  Batch Loss: 1.6062  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6783\n",
      "Epoch 001  Batch 2342/10240  Batch Loss: 1.3463  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6782\n",
      "Epoch 001  Batch 2343/10240  Batch Loss: 1.5606  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6782\n",
      "Epoch 001  Batch 2344/10240  Batch Loss: 1.0851  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6782\n",
      "Epoch 001  Batch 2345/10240  Batch Loss: 2.4347  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6781\n",
      "Epoch 001  Batch 2346/10240  Batch Loss: 2.3726  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6780\n",
      "Epoch 001  Batch 2347/10240  Batch Loss: 1.3142  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6780\n",
      "Epoch 001  Batch 2348/10240  Batch Loss: 2.4374  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6778\n",
      "Epoch 001  Batch 2349/10240  Batch Loss: 1.4131  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6778\n",
      "Epoch 001  Batch 2350/10240  Batch Loss: 0.8251  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6778\n",
      "Epoch 001  Batch 2351/10240  Batch Loss: 1.7148  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2352/10240  Batch Loss: 0.9603  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2353/10240  Batch Loss: 1.2368  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2354/10240  Batch Loss: 0.9741  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2355/10240  Batch Loss: 0.8768  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2356/10240  Batch Loss: 1.4778  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2357/10240  Batch Loss: 1.9636  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6777\n",
      "Epoch 001  Batch 2358/10240  Batch Loss: 1.8699  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6775\n",
      "Epoch 001  Batch 2359/10240  Batch Loss: 1.2913  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6775\n",
      "Epoch 001  Batch 2360/10240  Batch Loss: 1.2903  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6774\n",
      "Epoch 001  Batch 2361/10240  Batch Loss: 1.3477  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6774\n",
      "Epoch 001  Batch 2362/10240  Batch Loss: 2.4830  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6772\n",
      "Epoch 001  Batch 2363/10240  Batch Loss: 1.5936  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6771\n",
      "Epoch 001  Batch 2364/10240  Batch Loss: 1.4256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6770\n",
      "Epoch 001  Batch 2365/10240  Batch Loss: 0.8728  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6771\n",
      "Epoch 001  Batch 2366/10240  Batch Loss: 1.8957  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6770\n",
      "Epoch 001  Batch 2367/10240  Batch Loss: 0.9348  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6770\n",
      "Epoch 001  Batch 2368/10240  Batch Loss: 1.9603  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6770\n",
      "Epoch 001  Batch 2369/10240  Batch Loss: 3.8930  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6767\n",
      "Epoch 001  Batch 2370/10240  Batch Loss: 2.1383  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6767\n",
      "Epoch 001  Batch 2371/10240  Batch Loss: 2.0259  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6766\n",
      "Epoch 001  Batch 2372/10240  Batch Loss: 1.5168  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6766\n",
      "Epoch 001  Batch 2373/10240  Batch Loss: 1.4731  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6766\n",
      "Epoch 001  Batch 2374/10240  Batch Loss: 3.3880  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6763\n",
      "Epoch 001  Batch 2375/10240  Batch Loss: 0.9487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6763\n",
      "Epoch 001  Batch 2376/10240  Batch Loss: 3.0026  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6762\n",
      "Epoch 001  Batch 2377/10240  Batch Loss: 1.5546  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6762\n",
      "Epoch 001  Batch 2378/10240  Batch Loss: 1.8563  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6761\n",
      "Epoch 001  Batch 2379/10240  Batch Loss: 0.9515  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6761\n",
      "Epoch 001  Batch 2380/10240  Batch Loss: 2.0194  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6760\n",
      "Epoch 001  Batch 2381/10240  Batch Loss: 0.8034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6760\n",
      "Epoch 001  Batch 2382/10240  Batch Loss: 1.8535  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6759\n",
      "Epoch 001  Batch 2383/10240  Batch Loss: 1.7415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6759\n",
      "Epoch 001  Batch 2384/10240  Batch Loss: 1.8400  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6758\n",
      "Epoch 001  Batch 2385/10240  Batch Loss: 1.0942  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6758\n",
      "Epoch 001  Batch 2386/10240  Batch Loss: 2.7815  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6757\n",
      "Epoch 001  Batch 2387/10240  Batch Loss: 1.5552  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6756\n",
      "Epoch 001  Batch 2388/10240  Batch Loss: 1.1766  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6756\n",
      "Epoch 001  Batch 2389/10240  Batch Loss: 2.2436  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6755\n",
      "Epoch 001  Batch 2390/10240  Batch Loss: 2.6476  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6754\n",
      "Epoch 001  Batch 2391/10240  Batch Loss: 0.8811  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6754\n",
      "Epoch 001  Batch 2392/10240  Batch Loss: 1.5351  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6753\n",
      "Epoch 001  Batch 2393/10240  Batch Loss: 1.8936  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6752\n",
      "Epoch 001  Batch 2394/10240  Batch Loss: 2.5911  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6751\n",
      "Epoch 001  Batch 2395/10240  Batch Loss: 1.1742  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6751\n",
      "Epoch 001  Batch 2396/10240  Batch Loss: 1.1252  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6751\n",
      "Epoch 001  Batch 2397/10240  Batch Loss: 0.7894  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6751\n",
      "Epoch 001  Batch 2398/10240  Batch Loss: 2.9493  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6749\n",
      "Epoch 001  Batch 2399/10240  Batch Loss: 2.1876  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6748\n",
      "Epoch 001  Batch 2400/10240  Batch Loss: 1.5301  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6747\n",
      "Epoch 001  Batch 2401/10240  Batch Loss: 1.6392  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6746\n",
      "Epoch 001  Batch 2402/10240  Batch Loss: 2.1049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6746\n",
      "Epoch 001  Batch 2403/10240  Batch Loss: 1.2396  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6747\n",
      "Epoch 001  Batch 2404/10240  Batch Loss: 1.2725  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6746\n",
      "Epoch 001  Batch 2405/10240  Batch Loss: 1.7860  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6746\n",
      "Epoch 001  Batch 2406/10240  Batch Loss: 0.8979  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6746\n",
      "Epoch 001  Batch 2407/10240  Batch Loss: 1.5479  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6746\n",
      "Epoch 001  Batch 2408/10240  Batch Loss: 1.2246  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6745\n",
      "Epoch 001  Batch 2409/10240  Batch Loss: 1.6108  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6744\n",
      "Epoch 001  Batch 2410/10240  Batch Loss: 4.6990  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6742\n",
      "Epoch 001  Batch 2411/10240  Batch Loss: 2.1626  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6740\n",
      "Epoch 001  Batch 2412/10240  Batch Loss: 3.1830  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6739\n",
      "Epoch 001  Batch 2413/10240  Batch Loss: 1.4878  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6738\n",
      "Epoch 001  Batch 2414/10240  Batch Loss: 1.1135  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6738\n",
      "Epoch 001  Batch 2415/10240  Batch Loss: 0.8660  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6738\n",
      "Epoch 001  Batch 2416/10240  Batch Loss: 0.8352  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6738\n",
      "Epoch 001  Batch 2417/10240  Batch Loss: 0.8248  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6738\n",
      "Epoch 001  Batch 2418/10240  Batch Loss: 1.0705  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6739\n",
      "Epoch 001  Batch 2419/10240  Batch Loss: 1.7407  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6738\n",
      "Epoch 001  Batch 2420/10240  Batch Loss: 3.9287  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6736\n",
      "Epoch 001  Batch 2421/10240  Batch Loss: 3.7799  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6734\n",
      "Epoch 001  Batch 2422/10240  Batch Loss: 0.9664  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6733\n",
      "Epoch 001  Batch 2423/10240  Batch Loss: 1.6423  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6732\n",
      "Epoch 001  Batch 2424/10240  Batch Loss: 1.0302  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6732\n",
      "Epoch 001  Batch 2425/10240  Batch Loss: 2.2449  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6731\n",
      "Epoch 001  Batch 2426/10240  Batch Loss: 1.1972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6731\n",
      "Epoch 001  Batch 2427/10240  Batch Loss: 1.5180  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6731\n",
      "Epoch 001  Batch 2428/10240  Batch Loss: 0.8307  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6731\n",
      "Epoch 001  Batch 2429/10240  Batch Loss: 2.4272  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6729\n",
      "Epoch 001  Batch 2430/10240  Batch Loss: 1.9256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6728\n",
      "Epoch 001  Batch 2431/10240  Batch Loss: 1.5721  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6727\n",
      "Epoch 001  Batch 2432/10240  Batch Loss: 1.2405  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6727\n",
      "Epoch 001  Batch 2433/10240  Batch Loss: 3.2505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6725\n",
      "Epoch 001  Batch 2434/10240  Batch Loss: 1.1983  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6724\n",
      "Epoch 001  Batch 2435/10240  Batch Loss: 1.7581  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6724\n",
      "Epoch 001  Batch 2436/10240  Batch Loss: 3.0020  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6722\n",
      "Epoch 001  Batch 2437/10240  Batch Loss: 1.5502  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6721\n",
      "Epoch 001  Batch 2438/10240  Batch Loss: 1.5310  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6721\n",
      "Epoch 001  Batch 2439/10240  Batch Loss: 0.7992  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6721\n",
      "Epoch 001  Batch 2440/10240  Batch Loss: 1.5085  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6720\n",
      "Epoch 001  Batch 2441/10240  Batch Loss: 1.4887  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6720\n",
      "Epoch 001  Batch 2442/10240  Batch Loss: 0.8743  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6720\n",
      "Epoch 001  Batch 2443/10240  Batch Loss: 0.9682  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6720\n",
      "Epoch 001  Batch 2444/10240  Batch Loss: 1.6343  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6719\n",
      "Epoch 001  Batch 2445/10240  Batch Loss: 1.3823  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6719\n",
      "Epoch 001  Batch 2446/10240  Batch Loss: 1.4679  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6718\n",
      "Epoch 001  Batch 2447/10240  Batch Loss: 0.8386  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6718\n",
      "Epoch 001  Batch 2448/10240  Batch Loss: 1.1434  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6718\n",
      "Epoch 001  Batch 2449/10240  Batch Loss: 1.3231  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6718\n",
      "Epoch 001  Batch 2450/10240  Batch Loss: 1.4345  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6718\n",
      "Epoch 001  Batch 2451/10240  Batch Loss: 1.9969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6717\n",
      "Epoch 001  Batch 2452/10240  Batch Loss: 1.4195  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6717\n",
      "Epoch 001  Batch 2453/10240  Batch Loss: 3.7968  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6714\n",
      "Epoch 001  Batch 2454/10240  Batch Loss: 0.8522  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6713\n",
      "Epoch 001  Batch 2455/10240  Batch Loss: 1.4542  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6713\n",
      "Epoch 001  Batch 2456/10240  Batch Loss: 1.8444  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6713\n",
      "Epoch 001  Batch 2457/10240  Batch Loss: 0.8637  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6713\n",
      "Epoch 001  Batch 2458/10240  Batch Loss: 1.4280  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6713\n",
      "Epoch 001  Batch 2459/10240  Batch Loss: 1.5005  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6712\n",
      "Epoch 001  Batch 2460/10240  Batch Loss: 1.2269  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6712\n",
      "Epoch 001  Batch 2461/10240  Batch Loss: 0.8078  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6712\n",
      "Epoch 001  Batch 2462/10240  Batch Loss: 3.6733  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6711\n",
      "Epoch 001  Batch 2463/10240  Batch Loss: 1.2949  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6710\n",
      "Epoch 001  Batch 2464/10240  Batch Loss: 1.4929  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6710\n",
      "Epoch 001  Batch 2465/10240  Batch Loss: 1.7385  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6709\n",
      "Epoch 001  Batch 2466/10240  Batch Loss: 1.2650  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6708\n",
      "Epoch 001  Batch 2467/10240  Batch Loss: 1.8773  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6707\n",
      "Epoch 001  Batch 2468/10240  Batch Loss: 1.6816  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6707\n",
      "Epoch 001  Batch 2469/10240  Batch Loss: 1.4728  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6707\n",
      "Epoch 001  Batch 2470/10240  Batch Loss: 3.2418  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6706\n",
      "Epoch 001  Batch 2471/10240  Batch Loss: 2.3197  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6705\n",
      "Epoch 001  Batch 2472/10240  Batch Loss: 1.5824  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6704\n",
      "Epoch 001  Batch 2473/10240  Batch Loss: 1.9461  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6703\n",
      "Epoch 001  Batch 2474/10240  Batch Loss: 1.8720  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6702\n",
      "Epoch 001  Batch 2475/10240  Batch Loss: 1.6605  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6702\n",
      "Epoch 001  Batch 2476/10240  Batch Loss: 0.9738  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6702\n",
      "Epoch 001  Batch 2477/10240  Batch Loss: 1.7173  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6702\n",
      "Epoch 001  Batch 2478/10240  Batch Loss: 1.8022  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6701\n",
      "Epoch 001  Batch 2479/10240  Batch Loss: 1.7245  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6700\n",
      "Epoch 001  Batch 2480/10240  Batch Loss: 1.0399  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6699\n",
      "Epoch 001  Batch 2481/10240  Batch Loss: 2.3437  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6699\n",
      "Epoch 001  Batch 2482/10240  Batch Loss: 1.2105  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6698\n",
      "Epoch 001  Batch 2483/10240  Batch Loss: 1.6916  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6698\n",
      "Epoch 001  Batch 2484/10240  Batch Loss: 1.7913  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6698\n",
      "Epoch 001  Batch 2485/10240  Batch Loss: 1.4374  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2486/10240  Batch Loss: 0.8231  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2487/10240  Batch Loss: 2.0234  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2488/10240  Batch Loss: 1.2122  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2489/10240  Batch Loss: 1.3070  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2490/10240  Batch Loss: 1.8342  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2491/10240  Batch Loss: 1.3271  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6697\n",
      "Epoch 001  Batch 2492/10240  Batch Loss: 1.1036  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6696\n",
      "Epoch 001  Batch 2493/10240  Batch Loss: 2.2944  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6695\n",
      "Epoch 001  Batch 2494/10240  Batch Loss: 1.9528  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6694\n",
      "Epoch 001  Batch 2495/10240  Batch Loss: 1.8440  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6693\n",
      "Epoch 001  Batch 2496/10240  Batch Loss: 0.9792  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6693\n",
      "Epoch 001  Batch 2497/10240  Batch Loss: 1.3306  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6693\n",
      "Epoch 001  Batch 2498/10240  Batch Loss: 1.7782  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6693\n",
      "Epoch 001  Batch 2499/10240  Batch Loss: 1.5625  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6692\n",
      "Epoch 001  Batch 2500/10240  Batch Loss: 0.8967  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6692\n",
      "Epoch 001  Batch 2501/10240  Batch Loss: 1.1973  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6692\n",
      "Epoch 001  Batch 2502/10240  Batch Loss: 0.8186  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6692\n",
      "Epoch 001  Batch 2503/10240  Batch Loss: 1.9943  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6691\n",
      "Epoch 001  Batch 2504/10240  Batch Loss: 1.8468  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6690\n",
      "Epoch 001  Batch 2505/10240  Batch Loss: 2.2997  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6689\n",
      "Epoch 001  Batch 2506/10240  Batch Loss: 2.2549  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6688\n",
      "Epoch 001  Batch 2507/10240  Batch Loss: 1.6568  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6688\n",
      "Epoch 001  Batch 2508/10240  Batch Loss: 1.8998  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6687\n",
      "Epoch 001  Batch 2509/10240  Batch Loss: 1.7332  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6687\n",
      "Epoch 001  Batch 2510/10240  Batch Loss: 1.3199  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6686\n",
      "Epoch 001  Batch 2511/10240  Batch Loss: 0.8041  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6686\n",
      "Epoch 001  Batch 2512/10240  Batch Loss: 1.3846  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6686\n",
      "Epoch 001  Batch 2513/10240  Batch Loss: 0.8110  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6686\n",
      "Epoch 001  Batch 2514/10240  Batch Loss: 2.2414  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6686\n",
      "Epoch 001  Batch 2515/10240  Batch Loss: 0.8284  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6686\n",
      "Epoch 001  Batch 2516/10240  Batch Loss: 1.4462  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6685\n",
      "Epoch 001  Batch 2517/10240  Batch Loss: 2.8523  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6683\n",
      "Epoch 001  Batch 2518/10240  Batch Loss: 1.1098  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6684\n",
      "Epoch 001  Batch 2519/10240  Batch Loss: 2.0695  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6683\n",
      "Epoch 001  Batch 2520/10240  Batch Loss: 1.5754  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6683\n",
      "Epoch 001  Batch 2521/10240  Batch Loss: 1.0715  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6682\n",
      "Epoch 001  Batch 2522/10240  Batch Loss: 3.3880  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6679\n",
      "Epoch 001  Batch 2523/10240  Batch Loss: 0.8444  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6679\n",
      "Epoch 001  Batch 2524/10240  Batch Loss: 1.9122  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6679\n",
      "Epoch 001  Batch 2525/10240  Batch Loss: 1.4733  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6679\n",
      "Epoch 001  Batch 2526/10240  Batch Loss: 1.8948  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6678\n",
      "Epoch 001  Batch 2527/10240  Batch Loss: 1.9342  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6677\n",
      "Epoch 001  Batch 2528/10240  Batch Loss: 1.5882  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6676\n",
      "Epoch 001  Batch 2529/10240  Batch Loss: 1.2646  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6676\n",
      "Epoch 001  Batch 2530/10240  Batch Loss: 2.4270  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2531/10240  Batch Loss: 0.8195  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2532/10240  Batch Loss: 0.9214  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2533/10240  Batch Loss: 1.6054  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2534/10240  Batch Loss: 1.1964  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2535/10240  Batch Loss: 1.9608  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2536/10240  Batch Loss: 1.1681  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2537/10240  Batch Loss: 1.5650  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6674\n",
      "Epoch 001  Batch 2538/10240  Batch Loss: 1.3675  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6673\n",
      "Epoch 001  Batch 2539/10240  Batch Loss: 1.6424  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6673\n",
      "Epoch 001  Batch 2540/10240  Batch Loss: 0.8851  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6673\n",
      "Epoch 001  Batch 2541/10240  Batch Loss: 3.2383  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6672\n",
      "Epoch 001  Batch 2542/10240  Batch Loss: 1.1187  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6672\n",
      "Epoch 001  Batch 2543/10240  Batch Loss: 0.8223  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6672\n",
      "Epoch 001  Batch 2544/10240  Batch Loss: 1.6913  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6671\n",
      "Epoch 001  Batch 2545/10240  Batch Loss: 1.2544  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6671\n",
      "Epoch 001  Batch 2546/10240  Batch Loss: 2.2120  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6670\n",
      "Epoch 001  Batch 2547/10240  Batch Loss: 0.9105  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6670\n",
      "Epoch 001  Batch 2548/10240  Batch Loss: 1.8924  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6670\n",
      "Epoch 001  Batch 2549/10240  Batch Loss: 1.4902  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6669\n",
      "Epoch 001  Batch 2550/10240  Batch Loss: 1.9145  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6669\n",
      "Epoch 001  Batch 2551/10240  Batch Loss: 1.4180  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6669\n",
      "Epoch 001  Batch 2552/10240  Batch Loss: 2.8204  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6668\n",
      "Epoch 001  Batch 2553/10240  Batch Loss: 1.1218  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6668\n",
      "Epoch 001  Batch 2554/10240  Batch Loss: 2.4683  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6666\n",
      "Epoch 001  Batch 2555/10240  Batch Loss: 1.2925  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6666\n",
      "Epoch 001  Batch 2556/10240  Batch Loss: 1.9929  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6665\n",
      "Epoch 001  Batch 2557/10240  Batch Loss: 1.4767  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6665\n",
      "Epoch 001  Batch 2558/10240  Batch Loss: 2.4521  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6664\n",
      "Epoch 001  Batch 2559/10240  Batch Loss: 1.9464  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6663\n",
      "Epoch 001  Batch 2560/10240  Batch Loss: 1.9157  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6662\n",
      "Epoch 001  Batch 2561/10240  Batch Loss: 1.7322  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6662\n",
      "Epoch 001  Batch 2562/10240  Batch Loss: 1.4472  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6662\n",
      "Epoch 001  Batch 2563/10240  Batch Loss: 1.5386  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6661\n",
      "Epoch 001  Batch 2564/10240  Batch Loss: 0.8189  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6661\n",
      "Epoch 001  Batch 2565/10240  Batch Loss: 4.0569  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6659\n",
      "Epoch 001  Batch 2566/10240  Batch Loss: 0.8143  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6659\n",
      "Epoch 001  Batch 2567/10240  Batch Loss: 2.0153  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6658\n",
      "Epoch 001  Batch 2568/10240  Batch Loss: 2.2127  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6657\n",
      "Epoch 001  Batch 2569/10240  Batch Loss: 1.2207  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6657\n",
      "Epoch 001  Batch 2570/10240  Batch Loss: 0.7837  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6657\n",
      "Epoch 001  Batch 2571/10240  Batch Loss: 1.7304  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6657\n",
      "Epoch 001  Batch 2572/10240  Batch Loss: 1.1423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6656\n",
      "Epoch 001  Batch 2573/10240  Batch Loss: 2.4353  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6655\n",
      "Epoch 001  Batch 2574/10240  Batch Loss: 1.8507  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6654\n",
      "Epoch 001  Batch 2575/10240  Batch Loss: 1.4796  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6654\n",
      "Epoch 001  Batch 2576/10240  Batch Loss: 1.8913  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6655\n",
      "Epoch 001  Batch 2577/10240  Batch Loss: 1.5813  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6654\n",
      "Epoch 001  Batch 2578/10240  Batch Loss: 1.7235  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6653\n",
      "Epoch 001  Batch 2579/10240  Batch Loss: 1.4806  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6653\n",
      "Epoch 001  Batch 2580/10240  Batch Loss: 1.4493  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6652\n",
      "Epoch 001  Batch 2581/10240  Batch Loss: 1.7317  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6651\n",
      "Epoch 001  Batch 2582/10240  Batch Loss: 1.3745  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6651\n",
      "Epoch 001  Batch 2583/10240  Batch Loss: 1.1018  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6651\n",
      "Epoch 001  Batch 2584/10240  Batch Loss: 0.9694  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6651\n",
      "Epoch 001  Batch 2585/10240  Batch Loss: 1.1348  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6652\n",
      "Epoch 001  Batch 2586/10240  Batch Loss: 2.6084  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6650\n",
      "Epoch 001  Batch 2587/10240  Batch Loss: 2.0283  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6649\n",
      "Epoch 001  Batch 2588/10240  Batch Loss: 2.1802  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6648\n",
      "Epoch 001  Batch 2589/10240  Batch Loss: 2.0241  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6647\n",
      "Epoch 001  Batch 2590/10240  Batch Loss: 1.8269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6647\n",
      "Epoch 001  Batch 2591/10240  Batch Loss: 2.8952  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6645\n",
      "Epoch 001  Batch 2592/10240  Batch Loss: 1.1092  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6645\n",
      "Epoch 001  Batch 2593/10240  Batch Loss: 1.8477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6645\n",
      "Epoch 001  Batch 2594/10240  Batch Loss: 1.5579  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6645\n",
      "Epoch 001  Batch 2595/10240  Batch Loss: 1.7711  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6645\n",
      "Epoch 001  Batch 2596/10240  Batch Loss: 0.8463  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6645\n",
      "Epoch 001  Batch 2597/10240  Batch Loss: 2.1762  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6644\n",
      "Epoch 001  Batch 2598/10240  Batch Loss: 1.5208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6643\n",
      "Epoch 001  Batch 2599/10240  Batch Loss: 1.6269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6643\n",
      "Epoch 001  Batch 2600/10240  Batch Loss: 1.0273  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6642\n",
      "Epoch 001  Batch 2601/10240  Batch Loss: 0.9219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6642\n",
      "Epoch 001  Batch 2602/10240  Batch Loss: 1.3413  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6643\n",
      "Epoch 001  Batch 2603/10240  Batch Loss: 0.8247  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6643\n",
      "Epoch 001  Batch 2604/10240  Batch Loss: 1.3696  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6643\n",
      "Epoch 001  Batch 2605/10240  Batch Loss: 2.5613  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6641\n",
      "Epoch 001  Batch 2606/10240  Batch Loss: 1.5304  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6641\n",
      "Epoch 001  Batch 2607/10240  Batch Loss: 2.3030  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6639\n",
      "Epoch 001  Batch 2608/10240  Batch Loss: 0.8262  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6639\n",
      "Epoch 001  Batch 2609/10240  Batch Loss: 1.9757  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6639\n",
      "Epoch 001  Batch 2610/10240  Batch Loss: 0.8195  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6639\n",
      "Epoch 001  Batch 2611/10240  Batch Loss: 0.8979  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6639\n",
      "Epoch 001  Batch 2612/10240  Batch Loss: 2.2929  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6637\n",
      "Epoch 001  Batch 2613/10240  Batch Loss: 1.1679  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6637\n",
      "Epoch 001  Batch 2614/10240  Batch Loss: 1.7875  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6636\n",
      "Epoch 001  Batch 2615/10240  Batch Loss: 1.7742  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6636\n",
      "Epoch 001  Batch 2616/10240  Batch Loss: 1.4625  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6635\n",
      "Epoch 001  Batch 2617/10240  Batch Loss: 1.4154  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6635\n",
      "Epoch 001  Batch 2618/10240  Batch Loss: 1.6269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6634\n",
      "Epoch 001  Batch 2619/10240  Batch Loss: 1.5693  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6634\n",
      "Epoch 001  Batch 2620/10240  Batch Loss: 2.7355  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6633\n",
      "Epoch 001  Batch 2621/10240  Batch Loss: 3.1775  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6631\n",
      "Epoch 001  Batch 2622/10240  Batch Loss: 1.7584  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6630\n",
      "Epoch 001  Batch 2623/10240  Batch Loss: 2.6273  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6629\n",
      "Epoch 001  Batch 2624/10240  Batch Loss: 0.8107  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6629\n",
      "Epoch 001  Batch 2625/10240  Batch Loss: 1.0170  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6629\n",
      "Epoch 001  Batch 2626/10240  Batch Loss: 1.4112  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6628\n",
      "Epoch 001  Batch 2627/10240  Batch Loss: 1.2426  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6628\n",
      "Epoch 001  Batch 2628/10240  Batch Loss: 1.2407  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6628\n",
      "Epoch 001  Batch 2629/10240  Batch Loss: 2.1199  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6627\n",
      "Epoch 001  Batch 2630/10240  Batch Loss: 2.8194  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6627\n",
      "Epoch 001  Batch 2631/10240  Batch Loss: 2.6252  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6626\n",
      "Epoch 001  Batch 2632/10240  Batch Loss: 2.4125  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6626\n",
      "Epoch 001  Batch 2633/10240  Batch Loss: 1.8222  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6625\n",
      "Epoch 001  Batch 2634/10240  Batch Loss: 2.6115  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6623\n",
      "Epoch 001  Batch 2635/10240  Batch Loss: 0.7853  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6623\n",
      "Epoch 001  Batch 2636/10240  Batch Loss: 1.7219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6623\n",
      "Epoch 001  Batch 2637/10240  Batch Loss: 1.8212  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6622\n",
      "Epoch 001  Batch 2638/10240  Batch Loss: 2.3264  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6621\n",
      "Epoch 001  Batch 2639/10240  Batch Loss: 1.5526  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6621\n",
      "Epoch 001  Batch 2640/10240  Batch Loss: 1.4431  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6621\n",
      "Epoch 001  Batch 2641/10240  Batch Loss: 0.8238  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6621\n",
      "Epoch 001  Batch 2642/10240  Batch Loss: 1.0950  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6621\n",
      "Epoch 001  Batch 2643/10240  Batch Loss: 1.3667  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6621\n",
      "Epoch 001  Batch 2644/10240  Batch Loss: 2.1234  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6619\n",
      "Epoch 001  Batch 2645/10240  Batch Loss: 1.1193  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6619\n",
      "Epoch 001  Batch 2646/10240  Batch Loss: 2.0854  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6619\n",
      "Epoch 001  Batch 2647/10240  Batch Loss: 1.0622  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6618\n",
      "Epoch 001  Batch 2648/10240  Batch Loss: 0.8301  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6618\n",
      "Epoch 001  Batch 2649/10240  Batch Loss: 1.8870  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6618\n",
      "Epoch 001  Batch 2650/10240  Batch Loss: 0.8212  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6618\n",
      "Epoch 001  Batch 2651/10240  Batch Loss: 0.9831  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6618\n",
      "Epoch 001  Batch 2652/10240  Batch Loss: 2.4976  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6617\n",
      "Epoch 001  Batch 2653/10240  Batch Loss: 2.6866  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6616\n",
      "Epoch 001  Batch 2654/10240  Batch Loss: 0.8225  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6616\n",
      "Epoch 001  Batch 2655/10240  Batch Loss: 2.0740  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6615\n",
      "Epoch 001  Batch 2656/10240  Batch Loss: 1.8942  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6614\n",
      "Epoch 001  Batch 2657/10240  Batch Loss: 1.8683  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6613\n",
      "Epoch 001  Batch 2658/10240  Batch Loss: 2.4343  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6612\n",
      "Epoch 001  Batch 2659/10240  Batch Loss: 1.8043  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6611\n",
      "Epoch 001  Batch 2660/10240  Batch Loss: 2.7605  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6610\n",
      "Epoch 001  Batch 2661/10240  Batch Loss: 2.5102  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6611\n",
      "Epoch 001  Batch 2662/10240  Batch Loss: 0.8180  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6611\n",
      "Epoch 001  Batch 2663/10240  Batch Loss: 1.6907  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6611\n",
      "Epoch 001  Batch 2664/10240  Batch Loss: 2.1194  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6610\n",
      "Epoch 001  Batch 2665/10240  Batch Loss: 0.8133  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6610\n",
      "Epoch 001  Batch 2666/10240  Batch Loss: 1.4330  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6609\n",
      "Epoch 001  Batch 2667/10240  Batch Loss: 1.8897  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6609\n",
      "Epoch 001  Batch 2668/10240  Batch Loss: 1.8137  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6608\n",
      "Epoch 001  Batch 2669/10240  Batch Loss: 2.0312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6608\n",
      "Epoch 001  Batch 2670/10240  Batch Loss: 1.0091  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6607\n",
      "Epoch 001  Batch 2671/10240  Batch Loss: 1.8006  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6608\n",
      "Epoch 001  Batch 2672/10240  Batch Loss: 1.0309  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6607\n",
      "Epoch 001  Batch 2673/10240  Batch Loss: 1.3916  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6607\n",
      "Epoch 001  Batch 2674/10240  Batch Loss: 1.3395  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6607\n",
      "Epoch 001  Batch 2675/10240  Batch Loss: 0.8281  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6607\n",
      "Epoch 001  Batch 2676/10240  Batch Loss: 0.8843  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6607\n",
      "Epoch 001  Batch 2677/10240  Batch Loss: 2.3759  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6606\n",
      "Epoch 001  Batch 2678/10240  Batch Loss: 1.5350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6606\n",
      "Epoch 001  Batch 2679/10240  Batch Loss: 1.3944  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6606\n",
      "Epoch 001  Batch 2680/10240  Batch Loss: 1.6637  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6605\n",
      "Epoch 001  Batch 2681/10240  Batch Loss: 0.8866  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6605\n",
      "Epoch 001  Batch 2682/10240  Batch Loss: 1.4765  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6604\n",
      "Epoch 001  Batch 2683/10240  Batch Loss: 0.8733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6604\n",
      "Epoch 001  Batch 2684/10240  Batch Loss: 1.6420  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6604\n",
      "Epoch 001  Batch 2685/10240  Batch Loss: 1.2416  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6604\n",
      "Epoch 001  Batch 2686/10240  Batch Loss: 1.4839  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6604\n",
      "Epoch 001  Batch 2687/10240  Batch Loss: 1.0433  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6603\n",
      "Epoch 001  Batch 2688/10240  Batch Loss: 1.6722  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6602\n",
      "Epoch 001  Batch 2689/10240  Batch Loss: 1.0673  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6602\n",
      "Epoch 001  Batch 2690/10240  Batch Loss: 1.3445  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6602\n",
      "Epoch 001  Batch 2691/10240  Batch Loss: 1.4602  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6601\n",
      "Epoch 001  Batch 2692/10240  Batch Loss: 3.0175  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2693/10240  Batch Loss: 1.1265  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2694/10240  Batch Loss: 0.8259  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2695/10240  Batch Loss: 1.9907  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2696/10240  Batch Loss: 1.5860  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2697/10240  Batch Loss: 1.3575  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2698/10240  Batch Loss: 0.9857  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2699/10240  Batch Loss: 0.8517  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2700/10240  Batch Loss: 0.8070  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6600\n",
      "Epoch 001  Batch 2701/10240  Batch Loss: 1.5131  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6599\n",
      "Epoch 001  Batch 2702/10240  Batch Loss: 1.4229  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6599\n",
      "Epoch 001  Batch 2703/10240  Batch Loss: 0.9682  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6598\n",
      "Epoch 001  Batch 2704/10240  Batch Loss: 2.1770  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6597\n",
      "Epoch 001  Batch 2705/10240  Batch Loss: 0.8230  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6597\n",
      "Epoch 001  Batch 2706/10240  Batch Loss: 1.4781  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6596\n",
      "Epoch 001  Batch 2707/10240  Batch Loss: 0.8229  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6596\n",
      "Epoch 001  Batch 2708/10240  Batch Loss: 2.2316  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6596\n",
      "Epoch 001  Batch 2709/10240  Batch Loss: 2.1924  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6595\n",
      "Epoch 001  Batch 2710/10240  Batch Loss: 0.9721  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6595\n",
      "Epoch 001  Batch 2711/10240  Batch Loss: 2.2788  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6594\n",
      "Epoch 001  Batch 2712/10240  Batch Loss: 2.5589  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6593\n",
      "Epoch 001  Batch 2713/10240  Batch Loss: 1.3918  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6592\n",
      "Epoch 001  Batch 2714/10240  Batch Loss: 1.8291  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2715/10240  Batch Loss: 0.8367  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2716/10240  Batch Loss: 1.5130  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2717/10240  Batch Loss: 1.1770  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2718/10240  Batch Loss: 2.0787  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6592\n",
      "Epoch 001  Batch 2719/10240  Batch Loss: 1.6571  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6592\n",
      "Epoch 001  Batch 2720/10240  Batch Loss: 1.3055  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2721/10240  Batch Loss: 0.7841  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2722/10240  Batch Loss: 1.0512  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2723/10240  Batch Loss: 3.0747  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2724/10240  Batch Loss: 1.1384  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6591\n",
      "Epoch 001  Batch 2725/10240  Batch Loss: 2.0871  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6589\n",
      "Epoch 001  Batch 2726/10240  Batch Loss: 1.5207  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6589\n",
      "Epoch 001  Batch 2727/10240  Batch Loss: 1.7688  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6588\n",
      "Epoch 001  Batch 2728/10240  Batch Loss: 1.6378  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6589\n",
      "Epoch 001  Batch 2729/10240  Batch Loss: 1.5735  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6588\n",
      "Epoch 001  Batch 2730/10240  Batch Loss: 1.4610  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6588\n",
      "Epoch 001  Batch 2731/10240  Batch Loss: 1.4733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6587\n",
      "Epoch 001  Batch 2732/10240  Batch Loss: 2.1123  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6587\n",
      "Epoch 001  Batch 2733/10240  Batch Loss: 0.8397  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6587\n",
      "Epoch 001  Batch 2734/10240  Batch Loss: 2.0428  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6586\n",
      "Epoch 001  Batch 2735/10240  Batch Loss: 1.0422  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6586\n",
      "Epoch 001  Batch 2736/10240  Batch Loss: 1.9793  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6585\n",
      "Epoch 001  Batch 2737/10240  Batch Loss: 1.7930  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6585\n",
      "Epoch 001  Batch 2738/10240  Batch Loss: 3.3531  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6584\n",
      "Epoch 001  Batch 2739/10240  Batch Loss: 1.1865  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6584\n",
      "Epoch 001  Batch 2740/10240  Batch Loss: 1.8268  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6583\n",
      "Epoch 001  Batch 2741/10240  Batch Loss: 2.1413  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6582\n",
      "Epoch 001  Batch 2742/10240  Batch Loss: 2.1244  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6582\n",
      "Epoch 001  Batch 2743/10240  Batch Loss: 1.4653  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6581\n",
      "Epoch 001  Batch 2744/10240  Batch Loss: 1.7018  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6581\n",
      "Epoch 001  Batch 2745/10240  Batch Loss: 0.9130  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6581\n",
      "Epoch 001  Batch 2746/10240  Batch Loss: 1.1981  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6580\n",
      "Epoch 001  Batch 2747/10240  Batch Loss: 2.6563  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6579\n",
      "Epoch 001  Batch 2748/10240  Batch Loss: 1.2844  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6579\n",
      "Epoch 001  Batch 2749/10240  Batch Loss: 0.9345  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6578\n",
      "Epoch 001  Batch 2750/10240  Batch Loss: 1.4063  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6578\n",
      "Epoch 001  Batch 2751/10240  Batch Loss: 2.3747  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6577\n",
      "Epoch 001  Batch 2752/10240  Batch Loss: 1.1169  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6577\n",
      "Epoch 001  Batch 2753/10240  Batch Loss: 1.3566  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6576\n",
      "Epoch 001  Batch 2754/10240  Batch Loss: 1.7319  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6575\n",
      "Epoch 001  Batch 2755/10240  Batch Loss: 1.5182  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6574\n",
      "Epoch 001  Batch 2756/10240  Batch Loss: 0.9017  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6574\n",
      "Epoch 001  Batch 2757/10240  Batch Loss: 1.5342  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6574\n",
      "Epoch 001  Batch 2758/10240  Batch Loss: 1.5868  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6573\n",
      "Epoch 001  Batch 2759/10240  Batch Loss: 3.0448  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6573\n",
      "Epoch 001  Batch 2760/10240  Batch Loss: 1.7484  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6572\n",
      "Epoch 001  Batch 2761/10240  Batch Loss: 1.1252  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6572\n",
      "Epoch 001  Batch 2762/10240  Batch Loss: 1.1706  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6572\n",
      "Epoch 001  Batch 2763/10240  Batch Loss: 0.7906  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6572\n",
      "Epoch 001  Batch 2764/10240  Batch Loss: 2.2990  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6571\n",
      "Epoch 001  Batch 2765/10240  Batch Loss: 0.8820  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6571\n",
      "Epoch 001  Batch 2766/10240  Batch Loss: 1.2050  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6571\n",
      "Epoch 001  Batch 2767/10240  Batch Loss: 1.2841  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6570\n",
      "Epoch 001  Batch 2768/10240  Batch Loss: 1.5120  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6570\n",
      "Epoch 001  Batch 2769/10240  Batch Loss: 3.2450  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6567\n",
      "Epoch 001  Batch 2770/10240  Batch Loss: 1.6819  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6567\n",
      "Epoch 001  Batch 2771/10240  Batch Loss: 1.1427  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6567\n",
      "Epoch 001  Batch 2772/10240  Batch Loss: 1.5926  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6567\n",
      "Epoch 001  Batch 2773/10240  Batch Loss: 2.1502  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6566\n",
      "Epoch 001  Batch 2774/10240  Batch Loss: 2.5535  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6565\n",
      "Epoch 001  Batch 2775/10240  Batch Loss: 2.3702  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6564\n",
      "Epoch 001  Batch 2776/10240  Batch Loss: 1.1521  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6564\n",
      "Epoch 001  Batch 2777/10240  Batch Loss: 1.9733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6563\n",
      "Epoch 001  Batch 2778/10240  Batch Loss: 1.5355  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6563\n",
      "Epoch 001  Batch 2779/10240  Batch Loss: 0.8232  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6563\n",
      "Epoch 001  Batch 2780/10240  Batch Loss: 0.9420  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6563\n",
      "Epoch 001  Batch 2781/10240  Batch Loss: 1.9666  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6562\n",
      "Epoch 001  Batch 2782/10240  Batch Loss: 1.5926  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6562\n",
      "Epoch 001  Batch 2783/10240  Batch Loss: 0.9380  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6562\n",
      "Epoch 001  Batch 2784/10240  Batch Loss: 1.6987  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6561\n",
      "Epoch 001  Batch 2785/10240  Batch Loss: 1.1955  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6561\n",
      "Epoch 001  Batch 2786/10240  Batch Loss: 0.8169  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6561\n",
      "Epoch 001  Batch 2787/10240  Batch Loss: 2.0084  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6561\n",
      "Epoch 001  Batch 2788/10240  Batch Loss: 1.0208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6561\n",
      "Epoch 001  Batch 2789/10240  Batch Loss: 1.8597  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6561\n",
      "Epoch 001  Batch 2790/10240  Batch Loss: 1.2426  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6560\n",
      "Epoch 001  Batch 2791/10240  Batch Loss: 3.2426  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6558\n",
      "Epoch 001  Batch 2792/10240  Batch Loss: 1.6165  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6558\n",
      "Epoch 001  Batch 2793/10240  Batch Loss: 1.5702  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6557\n",
      "Epoch 001  Batch 2794/10240  Batch Loss: 1.2839  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6557\n",
      "Epoch 001  Batch 2795/10240  Batch Loss: 2.2347  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6556\n",
      "Epoch 001  Batch 2796/10240  Batch Loss: 1.6569  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6555\n",
      "Epoch 001  Batch 2797/10240  Batch Loss: 1.6735  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6555\n",
      "Epoch 001  Batch 2798/10240  Batch Loss: 2.9071  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6554\n",
      "Epoch 001  Batch 2799/10240  Batch Loss: 1.4735  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6554\n",
      "Epoch 001  Batch 2800/10240  Batch Loss: 1.3534  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6554\n",
      "Epoch 001  Batch 2801/10240  Batch Loss: 2.6296  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6552\n",
      "Epoch 001  Batch 2802/10240  Batch Loss: 1.5279  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6552\n",
      "Epoch 001  Batch 2803/10240  Batch Loss: 1.0894  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6552\n",
      "Epoch 001  Batch 2804/10240  Batch Loss: 2.1667  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6550\n",
      "Epoch 001  Batch 2805/10240  Batch Loss: 1.7124  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6550\n",
      "Epoch 001  Batch 2806/10240  Batch Loss: 2.2151  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6548\n",
      "Epoch 001  Batch 2807/10240  Batch Loss: 1.3449  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6548\n",
      "Epoch 001  Batch 2808/10240  Batch Loss: 2.5521  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6547\n",
      "Epoch 001  Batch 2809/10240  Batch Loss: 1.4009  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6547\n",
      "Epoch 001  Batch 2810/10240  Batch Loss: 1.9780  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6546\n",
      "Epoch 001  Batch 2811/10240  Batch Loss: 1.5549  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6546\n",
      "Epoch 001  Batch 2812/10240  Batch Loss: 0.8499  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6546\n",
      "Epoch 001  Batch 2813/10240  Batch Loss: 0.9589  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6546\n",
      "Epoch 001  Batch 2814/10240  Batch Loss: 1.4389  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6545\n",
      "Epoch 001  Batch 2815/10240  Batch Loss: 1.9618  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6544\n",
      "Epoch 001  Batch 2816/10240  Batch Loss: 2.2884  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6545\n",
      "Epoch 001  Batch 2817/10240  Batch Loss: 1.3002  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6545\n",
      "Epoch 001  Batch 2818/10240  Batch Loss: 3.2921  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6543\n",
      "Epoch 001  Batch 2819/10240  Batch Loss: 1.0921  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6542\n",
      "Epoch 001  Batch 2820/10240  Batch Loss: 0.9412  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6542\n",
      "Epoch 001  Batch 2821/10240  Batch Loss: 1.5750  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6542\n",
      "Epoch 001  Batch 2822/10240  Batch Loss: 2.4840  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6541\n",
      "Epoch 001  Batch 2823/10240  Batch Loss: 1.7051  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6540\n",
      "Epoch 001  Batch 2824/10240  Batch Loss: 0.8459  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6540\n",
      "Epoch 001  Batch 2825/10240  Batch Loss: 1.1354  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6540\n",
      "Epoch 001  Batch 2826/10240  Batch Loss: 0.8752  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6540\n",
      "Epoch 001  Batch 2827/10240  Batch Loss: 1.7299  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6539\n",
      "Epoch 001  Batch 2828/10240  Batch Loss: 0.8138  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6539\n",
      "Epoch 001  Batch 2829/10240  Batch Loss: 1.2992  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6539\n",
      "Epoch 001  Batch 2830/10240  Batch Loss: 1.7508  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6538\n",
      "Epoch 001  Batch 2831/10240  Batch Loss: 1.9004  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6537\n",
      "Epoch 001  Batch 2832/10240  Batch Loss: 2.5526  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6537\n",
      "Epoch 001  Batch 2833/10240  Batch Loss: 1.8719  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6536\n",
      "Epoch 001  Batch 2834/10240  Batch Loss: 1.0834  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6536\n",
      "Epoch 001  Batch 2835/10240  Batch Loss: 2.2696  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6536\n",
      "Epoch 001  Batch 2836/10240  Batch Loss: 1.6218  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6536\n",
      "Epoch 001  Batch 2837/10240  Batch Loss: 2.3917  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6535\n",
      "Epoch 001  Batch 2838/10240  Batch Loss: 1.0396  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6535\n",
      "Epoch 001  Batch 2839/10240  Batch Loss: 1.2753  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6534\n",
      "Epoch 001  Batch 2840/10240  Batch Loss: 2.5862  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6533\n",
      "Epoch 001  Batch 2841/10240  Batch Loss: 1.9935  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6533\n",
      "Epoch 001  Batch 2842/10240  Batch Loss: 0.7879  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6533\n",
      "Epoch 001  Batch 2843/10240  Batch Loss: 0.7966  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6533\n",
      "Epoch 001  Batch 2844/10240  Batch Loss: 1.6882  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6533\n",
      "Epoch 001  Batch 2845/10240  Batch Loss: 1.5962  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2846/10240  Batch Loss: 1.8836  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2847/10240  Batch Loss: 0.7989  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2848/10240  Batch Loss: 1.6586  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2849/10240  Batch Loss: 1.2588  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2850/10240  Batch Loss: 0.8073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2851/10240  Batch Loss: 1.3381  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6532\n",
      "Epoch 001  Batch 2852/10240  Batch Loss: 1.6566  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6531\n",
      "Epoch 001  Batch 2853/10240  Batch Loss: 2.2553  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6531\n",
      "Epoch 001  Batch 2854/10240  Batch Loss: 1.6697  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6530\n",
      "Epoch 001  Batch 2855/10240  Batch Loss: 3.2026  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6529\n",
      "Epoch 001  Batch 2856/10240  Batch Loss: 1.1034  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2857/10240  Batch Loss: 0.8358  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2858/10240  Batch Loss: 0.9247  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2859/10240  Batch Loss: 0.8067  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2860/10240  Batch Loss: 2.6388  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2861/10240  Batch Loss: 1.6094  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2862/10240  Batch Loss: 1.0879  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2863/10240  Batch Loss: 1.2158  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2864/10240  Batch Loss: 1.7565  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2865/10240  Batch Loss: 1.4117  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2866/10240  Batch Loss: 1.3506  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2867/10240  Batch Loss: 0.8770  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2868/10240  Batch Loss: 0.7968  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6528\n",
      "Epoch 001  Batch 2869/10240  Batch Loss: 2.1409  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2870/10240  Batch Loss: 1.1341  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2871/10240  Batch Loss: 1.4308  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2872/10240  Batch Loss: 1.7017  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6527\n",
      "Epoch 001  Batch 2873/10240  Batch Loss: 1.3775  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6526\n",
      "Epoch 001  Batch 2874/10240  Batch Loss: 1.2274  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6526\n",
      "Epoch 001  Batch 2875/10240  Batch Loss: 1.8499  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6525\n",
      "Epoch 001  Batch 2876/10240  Batch Loss: 1.7670  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6525\n",
      "Epoch 001  Batch 2877/10240  Batch Loss: 2.3673  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6524\n",
      "Epoch 001  Batch 2878/10240  Batch Loss: 1.6891  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6523\n",
      "Epoch 001  Batch 2879/10240  Batch Loss: 2.3951  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6523\n",
      "Epoch 001  Batch 2880/10240  Batch Loss: 1.0869  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6523\n",
      "Epoch 001  Batch 2881/10240  Batch Loss: 1.3001  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6523\n",
      "Epoch 001  Batch 2882/10240  Batch Loss: 0.9023  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6523\n",
      "Epoch 001  Batch 2883/10240  Batch Loss: 2.5191  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6522\n",
      "Epoch 001  Batch 2884/10240  Batch Loss: 1.2354  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6522\n",
      "Epoch 001  Batch 2885/10240  Batch Loss: 1.9975  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6522\n",
      "Epoch 001  Batch 2886/10240  Batch Loss: 1.2190  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6521\n",
      "Epoch 001  Batch 2887/10240  Batch Loss: 2.3289  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6521\n",
      "Epoch 001  Batch 2888/10240  Batch Loss: 1.5965  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6521\n",
      "Epoch 001  Batch 2889/10240  Batch Loss: 1.1205  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6520\n",
      "Epoch 001  Batch 2890/10240  Batch Loss: 1.3624  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6520\n",
      "Epoch 001  Batch 2891/10240  Batch Loss: 0.8580  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6520\n",
      "Epoch 001  Batch 2892/10240  Batch Loss: 1.2501  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6521\n",
      "Epoch 001  Batch 2893/10240  Batch Loss: 2.0578  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6520\n",
      "Epoch 001  Batch 2894/10240  Batch Loss: 0.8115  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6520\n",
      "Epoch 001  Batch 2895/10240  Batch Loss: 1.8356  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6520\n",
      "Epoch 001  Batch 2896/10240  Batch Loss: 2.1201  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6519\n",
      "Epoch 001  Batch 2897/10240  Batch Loss: 2.3868  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6518\n",
      "Epoch 001  Batch 2898/10240  Batch Loss: 1.2251  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6518\n",
      "Epoch 001  Batch 2899/10240  Batch Loss: 2.0513  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6517\n",
      "Epoch 001  Batch 2900/10240  Batch Loss: 1.5053  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6517\n",
      "Epoch 001  Batch 2901/10240  Batch Loss: 1.3099  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6517\n",
      "Epoch 001  Batch 2902/10240  Batch Loss: 1.8522  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6517\n",
      "Epoch 001  Batch 2903/10240  Batch Loss: 2.1375  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6516\n",
      "Epoch 001  Batch 2904/10240  Batch Loss: 1.6025  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6515\n",
      "Epoch 001  Batch 2905/10240  Batch Loss: 3.2163  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6513\n",
      "Epoch 001  Batch 2906/10240  Batch Loss: 2.7746  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6512\n",
      "Epoch 001  Batch 2907/10240  Batch Loss: 2.1770  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6511\n",
      "Epoch 001  Batch 2908/10240  Batch Loss: 1.9286  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6511\n",
      "Epoch 001  Batch 2909/10240  Batch Loss: 1.9166  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6510\n",
      "Epoch 001  Batch 2910/10240  Batch Loss: 0.7927  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6510\n",
      "Epoch 001  Batch 2911/10240  Batch Loss: 1.3417  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6510\n",
      "Epoch 001  Batch 2912/10240  Batch Loss: 0.7939  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6510\n",
      "Epoch 001  Batch 2913/10240  Batch Loss: 1.6250  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6511\n",
      "Epoch 001  Batch 2914/10240  Batch Loss: 0.8120  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6511\n",
      "Epoch 001  Batch 2915/10240  Batch Loss: 1.4312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6510\n",
      "Epoch 001  Batch 2916/10240  Batch Loss: 2.7105  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6509\n",
      "Epoch 001  Batch 2917/10240  Batch Loss: 1.4524  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6509\n",
      "Epoch 001  Batch 2918/10240  Batch Loss: 1.5016  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6508\n",
      "Epoch 001  Batch 2919/10240  Batch Loss: 1.2490  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6508\n",
      "Epoch 001  Batch 2920/10240  Batch Loss: 1.1684  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6508\n",
      "Epoch 001  Batch 2921/10240  Batch Loss: 1.3838  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6508\n",
      "Epoch 001  Batch 2922/10240  Batch Loss: 1.6487  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6508\n",
      "Epoch 001  Batch 2923/10240  Batch Loss: 1.4361  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6507\n",
      "Epoch 001  Batch 2924/10240  Batch Loss: 2.7845  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6506\n",
      "Epoch 001  Batch 2925/10240  Batch Loss: 2.2652  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6506\n",
      "Epoch 001  Batch 2926/10240  Batch Loss: 1.7654  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6506\n",
      "Epoch 001  Batch 2927/10240  Batch Loss: 2.7852  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6505\n",
      "Epoch 001  Batch 2928/10240  Batch Loss: 2.8400  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6504\n",
      "Epoch 001  Batch 2929/10240  Batch Loss: 1.9182  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6503\n",
      "Epoch 001  Batch 2930/10240  Batch Loss: 1.1822  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6503\n",
      "Epoch 001  Batch 2931/10240  Batch Loss: 2.5061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6502\n",
      "Epoch 001  Batch 2932/10240  Batch Loss: 1.3771  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6502\n",
      "Epoch 001  Batch 2933/10240  Batch Loss: 1.1629  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6502\n",
      "Epoch 001  Batch 2934/10240  Batch Loss: 1.9281  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6502\n",
      "Epoch 001  Batch 2935/10240  Batch Loss: 0.9999  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6502\n",
      "Epoch 001  Batch 2936/10240  Batch Loss: 1.7887  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6501\n",
      "Epoch 001  Batch 2937/10240  Batch Loss: 1.8914  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6501\n",
      "Epoch 001  Batch 2938/10240  Batch Loss: 1.9751  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6500\n",
      "Epoch 001  Batch 2939/10240  Batch Loss: 1.6424  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6500\n",
      "Epoch 001  Batch 2940/10240  Batch Loss: 1.2119  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6500\n",
      "Epoch 001  Batch 2941/10240  Batch Loss: 1.5568  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6499\n",
      "Epoch 001  Batch 2942/10240  Batch Loss: 1.3132  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6499\n",
      "Epoch 001  Batch 2943/10240  Batch Loss: 2.4643  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6498\n",
      "Epoch 001  Batch 2944/10240  Batch Loss: 1.0386  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6497\n",
      "Epoch 001  Batch 2945/10240  Batch Loss: 1.9451  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6498\n",
      "Epoch 001  Batch 2946/10240  Batch Loss: 1.3515  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6497\n",
      "Epoch 001  Batch 2947/10240  Batch Loss: 1.8499  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6497\n",
      "Epoch 001  Batch 2948/10240  Batch Loss: 0.9965  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6497\n",
      "Epoch 001  Batch 2949/10240  Batch Loss: 1.3997  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6497\n",
      "Epoch 001  Batch 2950/10240  Batch Loss: 1.7047  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6497\n",
      "Epoch 001  Batch 2951/10240  Batch Loss: 1.7250  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6496\n",
      "Epoch 001  Batch 2952/10240  Batch Loss: 0.8371  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6496\n",
      "Epoch 001  Batch 2953/10240  Batch Loss: 0.8290  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6496\n",
      "Epoch 001  Batch 2954/10240  Batch Loss: 1.1141  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6496\n",
      "Epoch 001  Batch 2955/10240  Batch Loss: 1.2689  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6496\n",
      "Epoch 001  Batch 2956/10240  Batch Loss: 2.4689  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6495\n",
      "Epoch 001  Batch 2957/10240  Batch Loss: 1.5977  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6495\n",
      "Epoch 001  Batch 2958/10240  Batch Loss: 0.9240  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6495\n",
      "Epoch 001  Batch 2959/10240  Batch Loss: 2.2889  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6495\n",
      "Epoch 001  Batch 2960/10240  Batch Loss: 1.4102  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6494\n",
      "Epoch 001  Batch 2961/10240  Batch Loss: 1.2800  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6494\n",
      "Epoch 001  Batch 2962/10240  Batch Loss: 1.5710  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2963/10240  Batch Loss: 1.0125  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2964/10240  Batch Loss: 0.8698  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2965/10240  Batch Loss: 0.9269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2966/10240  Batch Loss: 1.2906  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2967/10240  Batch Loss: 0.9670  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2968/10240  Batch Loss: 1.2017  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2969/10240  Batch Loss: 1.4787  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2970/10240  Batch Loss: 1.6970  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6493\n",
      "Epoch 001  Batch 2971/10240  Batch Loss: 1.4625  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2972/10240  Batch Loss: 0.8170  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2973/10240  Batch Loss: 2.1182  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2974/10240  Batch Loss: 1.8141  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2975/10240  Batch Loss: 0.9104  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2976/10240  Batch Loss: 0.8464  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2977/10240  Batch Loss: 1.3237  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2978/10240  Batch Loss: 0.8745  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6492\n",
      "Epoch 001  Batch 2979/10240  Batch Loss: 1.6468  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6491\n",
      "Epoch 001  Batch 2980/10240  Batch Loss: 1.6297  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6491\n",
      "Epoch 001  Batch 2981/10240  Batch Loss: 1.9399  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6491\n",
      "Epoch 001  Batch 2982/10240  Batch Loss: 2.0798  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6490\n",
      "Epoch 001  Batch 2983/10240  Batch Loss: 1.3301  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6490\n",
      "Epoch 001  Batch 2984/10240  Batch Loss: 0.7899  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6490\n",
      "Epoch 001  Batch 2985/10240  Batch Loss: 0.8061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6490\n",
      "Epoch 001  Batch 2986/10240  Batch Loss: 1.1993  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6490\n",
      "Epoch 001  Batch 2987/10240  Batch Loss: 1.1898  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6489\n",
      "Epoch 001  Batch 2988/10240  Batch Loss: 0.9793  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6489\n",
      "Epoch 001  Batch 2989/10240  Batch Loss: 1.8780  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6490\n",
      "Epoch 001  Batch 2990/10240  Batch Loss: 1.6662  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6489\n",
      "Epoch 001  Batch 2991/10240  Batch Loss: 1.3168  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6489\n",
      "Epoch 001  Batch 2992/10240  Batch Loss: 0.7812  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6489\n",
      "Epoch 001  Batch 2993/10240  Batch Loss: 2.3849  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6488\n",
      "Epoch 001  Batch 2994/10240  Batch Loss: 1.7673  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6488\n",
      "Epoch 001  Batch 2995/10240  Batch Loss: 1.2490  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6487\n",
      "Epoch 001  Batch 2996/10240  Batch Loss: 1.1465  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6488\n",
      "Epoch 001  Batch 2997/10240  Batch Loss: 1.5977  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6487\n",
      "Epoch 001  Batch 2998/10240  Batch Loss: 1.3854  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6487\n",
      "Epoch 001  Batch 2999/10240  Batch Loss: 2.3582  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6486\n",
      "Epoch 001  Batch 3000/10240  Batch Loss: 1.3041  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6486\n",
      "Epoch 001  Batch 3001/10240  Batch Loss: 1.5311  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6486\n",
      "Epoch 001  Batch 3002/10240  Batch Loss: 1.0784  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6486\n",
      "Epoch 001  Batch 3003/10240  Batch Loss: 1.3764  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6485\n",
      "Epoch 001  Batch 3004/10240  Batch Loss: 2.0660  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6485\n",
      "Epoch 001  Batch 3005/10240  Batch Loss: 2.3246  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6484\n",
      "Epoch 001  Batch 3006/10240  Batch Loss: 1.9105  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6483\n",
      "Epoch 001  Batch 3007/10240  Batch Loss: 1.1090  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6484\n",
      "Epoch 001  Batch 3008/10240  Batch Loss: 2.0118  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6484\n",
      "Epoch 001  Batch 3009/10240  Batch Loss: 2.5287  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6483\n",
      "Epoch 001  Batch 3010/10240  Batch Loss: 1.5677  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6483\n",
      "Epoch 001  Batch 3011/10240  Batch Loss: 2.1104  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6483\n",
      "Epoch 001  Batch 3012/10240  Batch Loss: 2.0663  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6482\n",
      "Epoch 001  Batch 3013/10240  Batch Loss: 1.1542  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6482\n",
      "Epoch 001  Batch 3014/10240  Batch Loss: 2.3237  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6481\n",
      "Epoch 001  Batch 3015/10240  Batch Loss: 0.8521  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6481\n",
      "Epoch 001  Batch 3016/10240  Batch Loss: 2.0805  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6481\n",
      "Epoch 001  Batch 3017/10240  Batch Loss: 1.0733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6481\n",
      "Epoch 001  Batch 3018/10240  Batch Loss: 1.8761  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6481\n",
      "Epoch 001  Batch 3019/10240  Batch Loss: 0.8291  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3020/10240  Batch Loss: 0.9419  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3021/10240  Batch Loss: 2.4015  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3022/10240  Batch Loss: 0.9808  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3023/10240  Batch Loss: 1.5541  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3024/10240  Batch Loss: 0.7687  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3025/10240  Batch Loss: 1.0174  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6480\n",
      "Epoch 001  Batch 3026/10240  Batch Loss: 2.1183  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6479\n",
      "Epoch 001  Batch 3027/10240  Batch Loss: 0.9139  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6479\n",
      "Epoch 001  Batch 3028/10240  Batch Loss: 0.7844  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6479\n",
      "Epoch 001  Batch 3029/10240  Batch Loss: 1.1983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6479\n",
      "Epoch 001  Batch 3030/10240  Batch Loss: 1.8605  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6478\n",
      "Epoch 001  Batch 3031/10240  Batch Loss: 0.8194  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6478\n",
      "Epoch 001  Batch 3032/10240  Batch Loss: 1.7417  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6477\n",
      "Epoch 001  Batch 3033/10240  Batch Loss: 0.8943  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6477\n",
      "Epoch 001  Batch 3034/10240  Batch Loss: 0.7971  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6477\n",
      "Epoch 001  Batch 3035/10240  Batch Loss: 1.3395  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6477\n",
      "Epoch 001  Batch 3036/10240  Batch Loss: 2.0700  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6477\n",
      "Epoch 001  Batch 3037/10240  Batch Loss: 2.4857  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6476\n",
      "Epoch 001  Batch 3038/10240  Batch Loss: 1.1786  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6476\n",
      "Epoch 001  Batch 3039/10240  Batch Loss: 2.3606  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6474\n",
      "Epoch 001  Batch 3040/10240  Batch Loss: 1.9607  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6474\n",
      "Epoch 001  Batch 3041/10240  Batch Loss: 0.7555  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6474\n",
      "Epoch 001  Batch 3042/10240  Batch Loss: 1.8137  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6473\n",
      "Epoch 001  Batch 3043/10240  Batch Loss: 2.0040  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6472\n",
      "Epoch 001  Batch 3044/10240  Batch Loss: 3.5096  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6471\n",
      "Epoch 001  Batch 3045/10240  Batch Loss: 1.3339  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3046/10240  Batch Loss: 1.1662  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3047/10240  Batch Loss: 0.7794  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3048/10240  Batch Loss: 1.4962  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3049/10240  Batch Loss: 1.2500  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3050/10240  Batch Loss: 1.5872  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3051/10240  Batch Loss: 0.8885  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3052/10240  Batch Loss: 1.9244  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3053/10240  Batch Loss: 1.1942  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3054/10240  Batch Loss: 1.2300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6470\n",
      "Epoch 001  Batch 3055/10240  Batch Loss: 2.5475  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6469\n",
      "Epoch 001  Batch 3056/10240  Batch Loss: 2.4522  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6468\n",
      "Epoch 001  Batch 3057/10240  Batch Loss: 0.9286  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6468\n",
      "Epoch 001  Batch 3058/10240  Batch Loss: 1.6379  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6467\n",
      "Epoch 001  Batch 3059/10240  Batch Loss: 1.3507  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6467\n",
      "Epoch 001  Batch 3060/10240  Batch Loss: 1.1803  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6467\n",
      "Epoch 001  Batch 3061/10240  Batch Loss: 2.5284  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3062/10240  Batch Loss: 1.2596  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3063/10240  Batch Loss: 1.4341  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3064/10240  Batch Loss: 0.7797  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3065/10240  Batch Loss: 0.8948  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3066/10240  Batch Loss: 2.3776  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3067/10240  Batch Loss: 0.8350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6465\n",
      "Epoch 001  Batch 3068/10240  Batch Loss: 1.6049  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6464\n",
      "Epoch 001  Batch 3069/10240  Batch Loss: 0.8729  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6464\n",
      "Epoch 001  Batch 3070/10240  Batch Loss: 2.0792  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6464\n",
      "Epoch 001  Batch 3071/10240  Batch Loss: 1.3893  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6463\n",
      "Epoch 001  Batch 3072/10240  Batch Loss: 0.7610  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6463\n",
      "Epoch 001  Batch 3073/10240  Batch Loss: 1.3547  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6463\n",
      "Epoch 001  Batch 3074/10240  Batch Loss: 1.2822  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6463\n",
      "Epoch 001  Batch 3075/10240  Batch Loss: 1.7153  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6463\n",
      "Epoch 001  Batch 3076/10240  Batch Loss: 2.4698  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6462\n",
      "Epoch 001  Batch 3077/10240  Batch Loss: 0.9439  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6462\n",
      "Epoch 001  Batch 3078/10240  Batch Loss: 1.3714  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6462\n",
      "Epoch 001  Batch 3079/10240  Batch Loss: 1.8939  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6461\n",
      "Epoch 001  Batch 3080/10240  Batch Loss: 1.5346  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6461\n",
      "Epoch 001  Batch 3081/10240  Batch Loss: 1.4966  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6460\n",
      "Epoch 001  Batch 3082/10240  Batch Loss: 3.0115  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6460\n",
      "Epoch 001  Batch 3083/10240  Batch Loss: 2.1674  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6459\n",
      "Epoch 001  Batch 3084/10240  Batch Loss: 1.2338  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6458\n",
      "Epoch 001  Batch 3085/10240  Batch Loss: 1.2232  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6458\n",
      "Epoch 001  Batch 3086/10240  Batch Loss: 1.0088  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6458\n",
      "Epoch 001  Batch 3087/10240  Batch Loss: 1.3077  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6458\n",
      "Epoch 001  Batch 3088/10240  Batch Loss: 1.2656  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6458\n",
      "Epoch 001  Batch 3089/10240  Batch Loss: 0.8816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6458\n",
      "Epoch 001  Batch 3090/10240  Batch Loss: 1.3512  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6457\n",
      "Epoch 001  Batch 3091/10240  Batch Loss: 0.9109  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6457\n",
      "Epoch 001  Batch 3092/10240  Batch Loss: 1.3435  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6457\n",
      "Epoch 001  Batch 3093/10240  Batch Loss: 1.4069  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3094/10240  Batch Loss: 1.0030  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3095/10240  Batch Loss: 1.3521  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3096/10240  Batch Loss: 2.5659  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3097/10240  Batch Loss: 1.2129  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3098/10240  Batch Loss: 2.2181  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3099/10240  Batch Loss: 1.0670  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3100/10240  Batch Loss: 1.6999  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3101/10240  Batch Loss: 1.2423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6456\n",
      "Epoch 001  Batch 3102/10240  Batch Loss: 2.4073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6455\n",
      "Epoch 001  Batch 3103/10240  Batch Loss: 1.2104  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6454\n",
      "Epoch 001  Batch 3104/10240  Batch Loss: 1.1422  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6454\n",
      "Epoch 001  Batch 3105/10240  Batch Loss: 1.3498  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6454\n",
      "Epoch 001  Batch 3106/10240  Batch Loss: 1.1376  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6454\n",
      "Epoch 001  Batch 3107/10240  Batch Loss: 1.4709  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6454\n",
      "Epoch 001  Batch 3108/10240  Batch Loss: 1.0418  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6453\n",
      "Epoch 001  Batch 3109/10240  Batch Loss: 1.2291  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6453\n",
      "Epoch 001  Batch 3110/10240  Batch Loss: 1.5850  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6453\n",
      "Epoch 001  Batch 3111/10240  Batch Loss: 1.9199  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6452\n",
      "Epoch 001  Batch 3112/10240  Batch Loss: 2.4117  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6451\n",
      "Epoch 001  Batch 3113/10240  Batch Loss: 1.8156  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6451\n",
      "Epoch 001  Batch 3114/10240  Batch Loss: 1.6403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6450\n",
      "Epoch 001  Batch 3115/10240  Batch Loss: 0.7782  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6450\n",
      "Epoch 001  Batch 3116/10240  Batch Loss: 1.8442  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6450\n",
      "Epoch 001  Batch 3117/10240  Batch Loss: 1.4974  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6449\n",
      "Epoch 001  Batch 3118/10240  Batch Loss: 1.6057  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6450\n",
      "Epoch 001  Batch 3119/10240  Batch Loss: 1.8812  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6449\n",
      "Epoch 001  Batch 3120/10240  Batch Loss: 1.1693  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6449\n",
      "Epoch 001  Batch 3121/10240  Batch Loss: 0.8377  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6449\n",
      "Epoch 001  Batch 3122/10240  Batch Loss: 1.3627  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6449\n",
      "Epoch 001  Batch 3123/10240  Batch Loss: 2.2816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6449\n",
      "Epoch 001  Batch 3124/10240  Batch Loss: 2.2713  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6448\n",
      "Epoch 001  Batch 3125/10240  Batch Loss: 2.2127  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6447\n",
      "Epoch 001  Batch 3126/10240  Batch Loss: 1.4152  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6447\n",
      "Epoch 001  Batch 3127/10240  Batch Loss: 1.0787  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6447\n",
      "Epoch 001  Batch 3128/10240  Batch Loss: 1.7429  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6447\n",
      "Epoch 001  Batch 3129/10240  Batch Loss: 0.8921  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6447\n",
      "Epoch 001  Batch 3130/10240  Batch Loss: 1.1928  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6447\n",
      "Epoch 001  Batch 3131/10240  Batch Loss: 1.9357  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6446\n",
      "Epoch 001  Batch 3132/10240  Batch Loss: 1.0300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6446\n",
      "Epoch 001  Batch 3133/10240  Batch Loss: 0.9265  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6446\n",
      "Epoch 001  Batch 3134/10240  Batch Loss: 1.3506  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6445\n",
      "Epoch 001  Batch 3135/10240  Batch Loss: 1.8627  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6446\n",
      "Epoch 001  Batch 3136/10240  Batch Loss: 3.0868  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6444\n",
      "Epoch 001  Batch 3137/10240  Batch Loss: 1.1811  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6444\n",
      "Epoch 001  Batch 3138/10240  Batch Loss: 2.1428  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3139/10240  Batch Loss: 1.9519  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3140/10240  Batch Loss: 1.4304  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3141/10240  Batch Loss: 1.3241  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3142/10240  Batch Loss: 0.7786  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3143/10240  Batch Loss: 0.9061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3144/10240  Batch Loss: 2.3334  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3145/10240  Batch Loss: 1.3222  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3146/10240  Batch Loss: 1.1737  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3147/10240  Batch Loss: 0.7951  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3148/10240  Batch Loss: 0.9985  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3149/10240  Batch Loss: 1.1182  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3150/10240  Batch Loss: 1.1950  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6443\n",
      "Epoch 001  Batch 3151/10240  Batch Loss: 1.4147  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6442\n",
      "Epoch 001  Batch 3152/10240  Batch Loss: 1.0934  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6442\n",
      "Epoch 001  Batch 3153/10240  Batch Loss: 1.3904  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6442\n",
      "Epoch 001  Batch 3154/10240  Batch Loss: 0.7518  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6442\n",
      "Epoch 001  Batch 3155/10240  Batch Loss: 2.0533  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6441\n",
      "Epoch 001  Batch 3156/10240  Batch Loss: 3.5491  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3157/10240  Batch Loss: 0.7956  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3158/10240  Batch Loss: 1.8932  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6440\n",
      "Epoch 001  Batch 3159/10240  Batch Loss: 0.7903  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6440\n",
      "Epoch 001  Batch 3160/10240  Batch Loss: 1.5267  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3161/10240  Batch Loss: 1.3473  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3162/10240  Batch Loss: 0.9432  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3163/10240  Batch Loss: 1.7140  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3164/10240  Batch Loss: 2.1522  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6439\n",
      "Epoch 001  Batch 3165/10240  Batch Loss: 2.0562  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6438\n",
      "Epoch 001  Batch 3166/10240  Batch Loss: 0.8553  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6438\n",
      "Epoch 001  Batch 3167/10240  Batch Loss: 1.0972  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6438\n",
      "Epoch 001  Batch 3168/10240  Batch Loss: 1.3507  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6438\n",
      "Epoch 001  Batch 3169/10240  Batch Loss: 1.6796  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6438\n",
      "Epoch 001  Batch 3170/10240  Batch Loss: 1.2325  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6438\n",
      "Epoch 001  Batch 3171/10240  Batch Loss: 2.4800  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6436\n",
      "Epoch 001  Batch 3172/10240  Batch Loss: 2.9138  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3173/10240  Batch Loss: 0.7661  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3174/10240  Batch Loss: 2.2367  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3175/10240  Batch Loss: 1.5953  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3176/10240  Batch Loss: 0.8331  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3177/10240  Batch Loss: 2.1960  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3178/10240  Batch Loss: 0.9715  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3179/10240  Batch Loss: 1.1404  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6434\n",
      "Epoch 001  Batch 3180/10240  Batch Loss: 0.8828  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6433\n",
      "Epoch 001  Batch 3181/10240  Batch Loss: 1.2265  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6433\n",
      "Epoch 001  Batch 3182/10240  Batch Loss: 1.4879  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6433\n",
      "Epoch 001  Batch 3183/10240  Batch Loss: 1.8909  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6432\n",
      "Epoch 001  Batch 3184/10240  Batch Loss: 1.6416  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6432\n",
      "Epoch 001  Batch 3185/10240  Batch Loss: 1.0706  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6432\n",
      "Epoch 001  Batch 3186/10240  Batch Loss: 1.7095  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6432\n",
      "Epoch 001  Batch 3187/10240  Batch Loss: 1.2876  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6431\n",
      "Epoch 001  Batch 3188/10240  Batch Loss: 0.9338  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6431\n",
      "Epoch 001  Batch 3189/10240  Batch Loss: 1.9168  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6431\n",
      "Epoch 001  Batch 3190/10240  Batch Loss: 1.9123  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6431\n",
      "Epoch 001  Batch 3191/10240  Batch Loss: 1.2943  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6430\n",
      "Epoch 001  Batch 3192/10240  Batch Loss: 1.5240  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6429\n",
      "Epoch 001  Batch 3193/10240  Batch Loss: 1.2884  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6429\n",
      "Epoch 001  Batch 3194/10240  Batch Loss: 1.4230  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6429\n",
      "Epoch 001  Batch 3195/10240  Batch Loss: 1.4811  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6429\n",
      "Epoch 001  Batch 3196/10240  Batch Loss: 1.3981  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6429\n",
      "Epoch 001  Batch 3197/10240  Batch Loss: 1.8509  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6428\n",
      "Epoch 001  Batch 3198/10240  Batch Loss: 1.3612  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6428\n",
      "Epoch 001  Batch 3199/10240  Batch Loss: 1.7156  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6428\n",
      "Epoch 001  Batch 3200/10240  Batch Loss: 1.4073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6428\n",
      "Epoch 001  Batch 3201/10240  Batch Loss: 1.3445  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6427\n",
      "Epoch 001  Batch 3202/10240  Batch Loss: 1.1675  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6427\n",
      "Epoch 001  Batch 3203/10240  Batch Loss: 1.1779  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6427\n",
      "Epoch 001  Batch 3204/10240  Batch Loss: 2.6787  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6425\n",
      "Epoch 001  Batch 3205/10240  Batch Loss: 1.3985  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6425\n",
      "Epoch 001  Batch 3206/10240  Batch Loss: 0.9267  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6425\n",
      "Epoch 001  Batch 3207/10240  Batch Loss: 1.1119  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6424\n",
      "Epoch 001  Batch 3208/10240  Batch Loss: 0.7694  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6424\n",
      "Epoch 001  Batch 3209/10240  Batch Loss: 1.3336  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6424\n",
      "Epoch 001  Batch 3210/10240  Batch Loss: 1.1661  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6424\n",
      "Epoch 001  Batch 3211/10240  Batch Loss: 1.7435  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6424\n",
      "Epoch 001  Batch 3212/10240  Batch Loss: 1.9102  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6423\n",
      "Epoch 001  Batch 3213/10240  Batch Loss: 1.0102  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6423\n",
      "Epoch 001  Batch 3214/10240  Batch Loss: 1.2635  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6423\n",
      "Epoch 001  Batch 3215/10240  Batch Loss: 1.0654  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6423\n",
      "Epoch 001  Batch 3216/10240  Batch Loss: 1.3111  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6422\n",
      "Epoch 001  Batch 3217/10240  Batch Loss: 1.8373  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6422\n",
      "Epoch 001  Batch 3218/10240  Batch Loss: 2.0829  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3219/10240  Batch Loss: 1.1244  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3220/10240  Batch Loss: 1.2305  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3221/10240  Batch Loss: 1.6571  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3222/10240  Batch Loss: 1.8279  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3223/10240  Batch Loss: 1.7391  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3224/10240  Batch Loss: 0.9578  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6420\n",
      "Epoch 001  Batch 3225/10240  Batch Loss: 0.9450  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6421\n",
      "Epoch 001  Batch 3226/10240  Batch Loss: 1.7354  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6420\n",
      "Epoch 001  Batch 3227/10240  Batch Loss: 2.1207  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6420\n",
      "Epoch 001  Batch 3228/10240  Batch Loss: 1.9485  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6419\n",
      "Epoch 001  Batch 3229/10240  Batch Loss: 3.8787  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6416\n",
      "Epoch 001  Batch 3230/10240  Batch Loss: 1.4580  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6416\n",
      "Epoch 001  Batch 3231/10240  Batch Loss: 0.8004  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6416\n",
      "Epoch 001  Batch 3232/10240  Batch Loss: 0.7799  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6416\n",
      "Epoch 001  Batch 3233/10240  Batch Loss: 1.9426  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6415\n",
      "Epoch 001  Batch 3234/10240  Batch Loss: 1.4192  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6415\n",
      "Epoch 001  Batch 3235/10240  Batch Loss: 0.9425  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6415\n",
      "Epoch 001  Batch 3236/10240  Batch Loss: 1.3815  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6415\n",
      "Epoch 001  Batch 3237/10240  Batch Loss: 1.4504  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6415\n",
      "Epoch 001  Batch 3238/10240  Batch Loss: 1.0082  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6415\n",
      "Epoch 001  Batch 3239/10240  Batch Loss: 1.8365  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6414\n",
      "Epoch 001  Batch 3240/10240  Batch Loss: 2.0797  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6414\n",
      "Epoch 001  Batch 3241/10240  Batch Loss: 1.1694  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6414\n",
      "Epoch 001  Batch 3242/10240  Batch Loss: 0.9075  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6414\n",
      "Epoch 001  Batch 3243/10240  Batch Loss: 0.7531  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6414\n",
      "Epoch 001  Batch 3244/10240  Batch Loss: 1.0405  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6413\n",
      "Epoch 001  Batch 3245/10240  Batch Loss: 0.7509  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6413\n",
      "Epoch 001  Batch 3246/10240  Batch Loss: 0.9106  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6413\n",
      "Epoch 001  Batch 3247/10240  Batch Loss: 2.8560  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6413\n",
      "Epoch 001  Batch 3248/10240  Batch Loss: 1.2247  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6413\n",
      "Epoch 001  Batch 3249/10240  Batch Loss: 2.0630  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6411\n",
      "Epoch 001  Batch 3250/10240  Batch Loss: 0.7770  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6411\n",
      "Epoch 001  Batch 3251/10240  Batch Loss: 1.1860  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6411\n",
      "Epoch 001  Batch 3252/10240  Batch Loss: 1.2050  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6411\n",
      "Epoch 001  Batch 3253/10240  Batch Loss: 2.4514  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6410\n",
      "Epoch 001  Batch 3254/10240  Batch Loss: 2.2503  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6409\n",
      "Epoch 001  Batch 3255/10240  Batch Loss: 1.3669  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6409\n",
      "Epoch 001  Batch 3256/10240  Batch Loss: 0.8263  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6409\n",
      "Epoch 001  Batch 3257/10240  Batch Loss: 1.4958  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6408\n",
      "Epoch 001  Batch 3258/10240  Batch Loss: 0.9457  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6408\n",
      "Epoch 001  Batch 3259/10240  Batch Loss: 1.3429  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6408\n",
      "Epoch 001  Batch 3260/10240  Batch Loss: 2.5853  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6407\n",
      "Epoch 001  Batch 3261/10240  Batch Loss: 0.8736  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6407\n",
      "Epoch 001  Batch 3262/10240  Batch Loss: 1.3853  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6406\n",
      "Epoch 001  Batch 3263/10240  Batch Loss: 1.7063  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6405\n",
      "Epoch 001  Batch 3264/10240  Batch Loss: 2.5466  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6404\n",
      "Epoch 001  Batch 3265/10240  Batch Loss: 1.2424  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6403\n",
      "Epoch 001  Batch 3266/10240  Batch Loss: 1.2662  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6403\n",
      "Epoch 001  Batch 3267/10240  Batch Loss: 2.9155  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6403\n",
      "Epoch 001  Batch 3268/10240  Batch Loss: 2.9095  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6401\n",
      "Epoch 001  Batch 3269/10240  Batch Loss: 1.7020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6401\n",
      "Epoch 001  Batch 3270/10240  Batch Loss: 1.2599  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6400\n",
      "Epoch 001  Batch 3271/10240  Batch Loss: 1.2833  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6400\n",
      "Epoch 001  Batch 3272/10240  Batch Loss: 1.4456  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6400\n",
      "Epoch 001  Batch 3273/10240  Batch Loss: 1.2355  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6400\n",
      "Epoch 001  Batch 3274/10240  Batch Loss: 1.7224  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6399\n",
      "Epoch 001  Batch 3275/10240  Batch Loss: 3.1486  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6398\n",
      "Epoch 001  Batch 3276/10240  Batch Loss: 1.9878  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6396\n",
      "Epoch 001  Batch 3277/10240  Batch Loss: 1.0413  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6396\n",
      "Epoch 001  Batch 3278/10240  Batch Loss: 0.7662  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6396\n",
      "Epoch 001  Batch 3279/10240  Batch Loss: 0.9330  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6396\n",
      "Epoch 001  Batch 3280/10240  Batch Loss: 1.9167  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6395\n",
      "Epoch 001  Batch 3281/10240  Batch Loss: 1.1642  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6396\n",
      "Epoch 001  Batch 3282/10240  Batch Loss: 1.8497  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6395\n",
      "Epoch 001  Batch 3283/10240  Batch Loss: 1.2462  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6395\n",
      "Epoch 001  Batch 3284/10240  Batch Loss: 1.4204  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6394\n",
      "Epoch 001  Batch 3285/10240  Batch Loss: 1.5251  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6394\n",
      "Epoch 001  Batch 3286/10240  Batch Loss: 1.9448  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6395\n",
      "Epoch 001  Batch 3287/10240  Batch Loss: 1.6884  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6394\n",
      "Epoch 001  Batch 3288/10240  Batch Loss: 0.7403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6394\n",
      "Epoch 001  Batch 3289/10240  Batch Loss: 0.7743  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6394\n",
      "Epoch 001  Batch 3290/10240  Batch Loss: 2.3024  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6393\n",
      "Epoch 001  Batch 3291/10240  Batch Loss: 0.9444  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6393\n",
      "Epoch 001  Batch 3292/10240  Batch Loss: 2.1053  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6392\n",
      "Epoch 001  Batch 3293/10240  Batch Loss: 1.5330  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6392\n",
      "Epoch 001  Batch 3294/10240  Batch Loss: 2.8835  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6391\n",
      "Epoch 001  Batch 3295/10240  Batch Loss: 2.0186  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6391\n",
      "Epoch 001  Batch 3296/10240  Batch Loss: 1.1993  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6391\n",
      "Epoch 001  Batch 3297/10240  Batch Loss: 2.2815  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6390\n",
      "Epoch 001  Batch 3298/10240  Batch Loss: 1.8017  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6390\n",
      "Epoch 001  Batch 3299/10240  Batch Loss: 1.3330  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6389\n",
      "Epoch 001  Batch 3300/10240  Batch Loss: 1.4545  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6389\n",
      "Epoch 001  Batch 3301/10240  Batch Loss: 2.4410  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6388\n",
      "Epoch 001  Batch 3302/10240  Batch Loss: 1.7873  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3303/10240  Batch Loss: 1.2364  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3304/10240  Batch Loss: 0.7657  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3305/10240  Batch Loss: 1.2180  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3306/10240  Batch Loss: 0.7529  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3307/10240  Batch Loss: 0.7658  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3308/10240  Batch Loss: 2.3504  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6387\n",
      "Epoch 001  Batch 3309/10240  Batch Loss: 1.4988  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6386\n",
      "Epoch 001  Batch 3310/10240  Batch Loss: 1.1619  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6386\n",
      "Epoch 001  Batch 3311/10240  Batch Loss: 1.0294  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6386\n",
      "Epoch 001  Batch 3312/10240  Batch Loss: 0.9780  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6386\n",
      "Epoch 001  Batch 3313/10240  Batch Loss: 2.2132  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6385\n",
      "Epoch 001  Batch 3314/10240  Batch Loss: 1.9144  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6384\n",
      "Epoch 001  Batch 3315/10240  Batch Loss: 0.9338  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6384\n",
      "Epoch 001  Batch 3316/10240  Batch Loss: 2.0234  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6384\n",
      "Epoch 001  Batch 3317/10240  Batch Loss: 2.5379  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6383\n",
      "Epoch 001  Batch 3318/10240  Batch Loss: 1.2242  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6383\n",
      "Epoch 001  Batch 3319/10240  Batch Loss: 1.2034  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6382\n",
      "Epoch 001  Batch 3320/10240  Batch Loss: 0.8762  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6382\n",
      "Epoch 001  Batch 3321/10240  Batch Loss: 0.8269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6382\n",
      "Epoch 001  Batch 3322/10240  Batch Loss: 1.3002  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6382\n",
      "Epoch 001  Batch 3323/10240  Batch Loss: 2.2526  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6381\n",
      "Epoch 001  Batch 3324/10240  Batch Loss: 1.7803  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6380\n",
      "Epoch 001  Batch 3325/10240  Batch Loss: 0.7696  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6380\n",
      "Epoch 001  Batch 3326/10240  Batch Loss: 1.0559  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6380\n",
      "Epoch 001  Batch 3327/10240  Batch Loss: 2.4519  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6380\n",
      "Epoch 001  Batch 3328/10240  Batch Loss: 1.1768  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6380\n",
      "Epoch 001  Batch 3329/10240  Batch Loss: 1.4798  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6379\n",
      "Epoch 001  Batch 3330/10240  Batch Loss: 1.9731  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6379\n",
      "Epoch 001  Batch 3331/10240  Batch Loss: 1.1053  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6378\n",
      "Epoch 001  Batch 3332/10240  Batch Loss: 2.1710  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6378\n",
      "Epoch 001  Batch 3333/10240  Batch Loss: 1.8926  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6377\n",
      "Epoch 001  Batch 3334/10240  Batch Loss: 1.4050  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6377\n",
      "Epoch 001  Batch 3335/10240  Batch Loss: 1.5547  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6377\n",
      "Epoch 001  Batch 3336/10240  Batch Loss: 0.9378  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6377\n",
      "Epoch 001  Batch 3337/10240  Batch Loss: 1.0063  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6377\n",
      "Epoch 001  Batch 3338/10240  Batch Loss: 1.5063  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6377\n",
      "Epoch 001  Batch 3339/10240  Batch Loss: 2.4079  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6376\n",
      "Epoch 001  Batch 3340/10240  Batch Loss: 1.8913  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6375\n",
      "Epoch 001  Batch 3341/10240  Batch Loss: 1.0073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6375\n",
      "Epoch 001  Batch 3342/10240  Batch Loss: 2.3147  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6375\n",
      "Epoch 001  Batch 3343/10240  Batch Loss: 2.0432  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6374\n",
      "Epoch 001  Batch 3344/10240  Batch Loss: 1.6231  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6374\n",
      "Epoch 001  Batch 3345/10240  Batch Loss: 1.5586  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6374\n",
      "Epoch 001  Batch 3346/10240  Batch Loss: 1.3285  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6374\n",
      "Epoch 001  Batch 3347/10240  Batch Loss: 2.2669  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6373\n",
      "Epoch 001  Batch 3348/10240  Batch Loss: 1.5437  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6372\n",
      "Epoch 001  Batch 3349/10240  Batch Loss: 1.4466  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6372\n",
      "Epoch 001  Batch 3350/10240  Batch Loss: 1.1370  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6371\n",
      "Epoch 001  Batch 3351/10240  Batch Loss: 1.4923  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6371\n",
      "Epoch 001  Batch 3352/10240  Batch Loss: 1.3571  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6370\n",
      "Epoch 001  Batch 3353/10240  Batch Loss: 1.5660  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6370\n",
      "Epoch 001  Batch 3354/10240  Batch Loss: 1.7067  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6369\n",
      "Epoch 001  Batch 3355/10240  Batch Loss: 1.7527  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6369\n",
      "Epoch 001  Batch 3356/10240  Batch Loss: 2.3403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6368\n",
      "Epoch 001  Batch 3357/10240  Batch Loss: 3.1069  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6367\n",
      "Epoch 001  Batch 3358/10240  Batch Loss: 1.4836  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6367\n",
      "Epoch 001  Batch 3359/10240  Batch Loss: 2.5523  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6365\n",
      "Epoch 001  Batch 3360/10240  Batch Loss: 0.7716  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6365\n",
      "Epoch 001  Batch 3361/10240  Batch Loss: 1.0368  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6365\n",
      "Epoch 001  Batch 3362/10240  Batch Loss: 1.6391  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6365\n",
      "Epoch 001  Batch 3363/10240  Batch Loss: 1.1888  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6364\n",
      "Epoch 001  Batch 3364/10240  Batch Loss: 0.7652  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6364\n",
      "Epoch 001  Batch 3365/10240  Batch Loss: 1.3509  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6364\n",
      "Epoch 001  Batch 3366/10240  Batch Loss: 2.4585  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6363\n",
      "Epoch 001  Batch 3367/10240  Batch Loss: 1.3719  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6362\n",
      "Epoch 001  Batch 3368/10240  Batch Loss: 0.7794  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6362\n",
      "Epoch 001  Batch 3369/10240  Batch Loss: 0.8711  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6362\n",
      "Epoch 001  Batch 3370/10240  Batch Loss: 1.2341  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6363\n",
      "Epoch 001  Batch 3371/10240  Batch Loss: 1.2361  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6362\n",
      "Epoch 001  Batch 3372/10240  Batch Loss: 1.0796  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6362\n",
      "Epoch 001  Batch 3373/10240  Batch Loss: 1.7240  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6361\n",
      "Epoch 001  Batch 3374/10240  Batch Loss: 1.6690  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6362\n",
      "Epoch 001  Batch 3375/10240  Batch Loss: 1.4274  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6361\n",
      "Epoch 001  Batch 3376/10240  Batch Loss: 0.9332  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6361\n",
      "Epoch 001  Batch 3377/10240  Batch Loss: 2.4073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6361\n",
      "Epoch 001  Batch 3378/10240  Batch Loss: 1.4131  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6360\n",
      "Epoch 001  Batch 3379/10240  Batch Loss: 2.3255  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6361\n",
      "Epoch 001  Batch 3380/10240  Batch Loss: 1.9495  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6360\n",
      "Epoch 001  Batch 3381/10240  Batch Loss: 1.9985  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6360\n",
      "Epoch 001  Batch 3382/10240  Batch Loss: 1.6372  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6359\n",
      "Epoch 001  Batch 3383/10240  Batch Loss: 1.3911  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6359\n",
      "Epoch 001  Batch 3384/10240  Batch Loss: 0.7727  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6359\n",
      "Epoch 001  Batch 3385/10240  Batch Loss: 1.3142  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6359\n",
      "Epoch 001  Batch 3386/10240  Batch Loss: 1.4861  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6358\n",
      "Epoch 001  Batch 3387/10240  Batch Loss: 1.5445  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6358\n",
      "Epoch 001  Batch 3388/10240  Batch Loss: 1.0732  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6358\n",
      "Epoch 001  Batch 3389/10240  Batch Loss: 1.4660  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6358\n",
      "Epoch 001  Batch 3390/10240  Batch Loss: 1.6447  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3391/10240  Batch Loss: 0.8237  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3392/10240  Batch Loss: 1.3639  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3393/10240  Batch Loss: 1.1642  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3394/10240  Batch Loss: 1.6480  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3395/10240  Batch Loss: 1.1817  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3396/10240  Batch Loss: 0.7396  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3397/10240  Batch Loss: 2.0683  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3398/10240  Batch Loss: 0.7974  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3399/10240  Batch Loss: 1.2964  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3400/10240  Batch Loss: 2.5053  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6357\n",
      "Epoch 001  Batch 3401/10240  Batch Loss: 1.8792  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6356\n",
      "Epoch 001  Batch 3402/10240  Batch Loss: 2.4328  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6355\n",
      "Epoch 001  Batch 3403/10240  Batch Loss: 1.7037  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6354\n",
      "Epoch 001  Batch 3404/10240  Batch Loss: 1.7394  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6353\n",
      "Epoch 001  Batch 3405/10240  Batch Loss: 1.6180  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6353\n",
      "Epoch 001  Batch 3406/10240  Batch Loss: 0.8725  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6353\n",
      "Epoch 001  Batch 3407/10240  Batch Loss: 2.0208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6353\n",
      "Epoch 001  Batch 3408/10240  Batch Loss: 1.5479  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6353\n",
      "Epoch 001  Batch 3409/10240  Batch Loss: 1.4609  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6352\n",
      "Epoch 001  Batch 3410/10240  Batch Loss: 1.1779  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6352\n",
      "Epoch 001  Batch 3411/10240  Batch Loss: 2.7042  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6351\n",
      "Epoch 001  Batch 3412/10240  Batch Loss: 0.8089  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6351\n",
      "Epoch 001  Batch 3413/10240  Batch Loss: 1.4113  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6351\n",
      "Epoch 001  Batch 3414/10240  Batch Loss: 0.8244  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6350\n",
      "Epoch 001  Batch 3415/10240  Batch Loss: 1.5296  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6350\n",
      "Epoch 001  Batch 3416/10240  Batch Loss: 1.6132  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6349\n",
      "Epoch 001  Batch 3417/10240  Batch Loss: 1.5303  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3418/10240  Batch Loss: 0.7685  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3419/10240  Batch Loss: 0.7828  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3420/10240  Batch Loss: 1.0377  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3421/10240  Batch Loss: 0.7709  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3422/10240  Batch Loss: 0.8021  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3423/10240  Batch Loss: 1.2508  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6348\n",
      "Epoch 001  Batch 3424/10240  Batch Loss: 1.5292  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6347\n",
      "Epoch 001  Batch 3425/10240  Batch Loss: 1.9797  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6346\n",
      "Epoch 001  Batch 3426/10240  Batch Loss: 1.0176  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6346\n",
      "Epoch 001  Batch 3427/10240  Batch Loss: 0.9020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6346\n",
      "Epoch 001  Batch 3428/10240  Batch Loss: 1.1032  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6346\n",
      "Epoch 001  Batch 3429/10240  Batch Loss: 2.6079  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6345\n",
      "Epoch 001  Batch 3430/10240  Batch Loss: 1.7090  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6345\n",
      "Epoch 001  Batch 3431/10240  Batch Loss: 1.3107  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6345\n",
      "Epoch 001  Batch 3432/10240  Batch Loss: 1.0583  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6345\n",
      "Epoch 001  Batch 3433/10240  Batch Loss: 0.7423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6345\n",
      "Epoch 001  Batch 3434/10240  Batch Loss: 1.7385  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6344\n",
      "Epoch 001  Batch 3435/10240  Batch Loss: 1.2816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6344\n",
      "Epoch 001  Batch 3436/10240  Batch Loss: 1.1352  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6344\n",
      "Epoch 001  Batch 3437/10240  Batch Loss: 2.0417  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6343\n",
      "Epoch 001  Batch 3438/10240  Batch Loss: 2.4634  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6342\n",
      "Epoch 001  Batch 3439/10240  Batch Loss: 1.6744  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3440/10240  Batch Loss: 0.8026  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3441/10240  Batch Loss: 1.2347  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3442/10240  Batch Loss: 1.2634  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3443/10240  Batch Loss: 1.4768  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3444/10240  Batch Loss: 1.5153  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3445/10240  Batch Loss: 0.7602  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3446/10240  Batch Loss: 1.0290  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3447/10240  Batch Loss: 1.1726  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3448/10240  Batch Loss: 1.0284  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3449/10240  Batch Loss: 0.8183  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3450/10240  Batch Loss: 0.7469  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3451/10240  Batch Loss: 1.2696  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3452/10240  Batch Loss: 0.7523  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3453/10240  Batch Loss: 1.5586  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3454/10240  Batch Loss: 0.9650  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3455/10240  Batch Loss: 1.9841  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6341\n",
      "Epoch 001  Batch 3456/10240  Batch Loss: 1.0267  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3457/10240  Batch Loss: 1.0164  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3458/10240  Batch Loss: 1.1585  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3459/10240  Batch Loss: 1.4632  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3460/10240  Batch Loss: 1.5892  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3461/10240  Batch Loss: 0.9114  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3462/10240  Batch Loss: 2.0171  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6340\n",
      "Epoch 001  Batch 3463/10240  Batch Loss: 2.1397  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6339\n",
      "Epoch 001  Batch 3464/10240  Batch Loss: 1.5577  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6339\n",
      "Epoch 001  Batch 3465/10240  Batch Loss: 1.1848  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6339\n",
      "Epoch 001  Batch 3466/10240  Batch Loss: 2.0957  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6338\n",
      "Epoch 001  Batch 3467/10240  Batch Loss: 1.3685  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6338\n",
      "Epoch 001  Batch 3468/10240  Batch Loss: 1.4184  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6338\n",
      "Epoch 001  Batch 3469/10240  Batch Loss: 1.3925  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6337\n",
      "Epoch 001  Batch 3470/10240  Batch Loss: 2.6561  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6337\n",
      "Epoch 001  Batch 3471/10240  Batch Loss: 1.3245  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6337\n",
      "Epoch 001  Batch 3472/10240  Batch Loss: 1.4939  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6337\n",
      "Epoch 001  Batch 3473/10240  Batch Loss: 2.5415  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6336\n",
      "Epoch 001  Batch 3474/10240  Batch Loss: 0.7667  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6336\n",
      "Epoch 001  Batch 3475/10240  Batch Loss: 0.7369  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6336\n",
      "Epoch 001  Batch 3476/10240  Batch Loss: 2.0809  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6335\n",
      "Epoch 001  Batch 3477/10240  Batch Loss: 0.9410  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6334\n",
      "Epoch 001  Batch 3478/10240  Batch Loss: 2.0329  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6334\n",
      "Epoch 001  Batch 3479/10240  Batch Loss: 2.5523  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6332\n",
      "Epoch 001  Batch 3480/10240  Batch Loss: 2.4444  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6332\n",
      "Epoch 001  Batch 3481/10240  Batch Loss: 0.7483  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6332\n",
      "Epoch 001  Batch 3482/10240  Batch Loss: 1.7399  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6331\n",
      "Epoch 001  Batch 3483/10240  Batch Loss: 1.7130  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6331\n",
      "Epoch 001  Batch 3484/10240  Batch Loss: 2.5743  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6331\n",
      "Epoch 001  Batch 3485/10240  Batch Loss: 0.8659  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6331\n",
      "Epoch 001  Batch 3486/10240  Batch Loss: 1.0056  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6331\n",
      "Epoch 001  Batch 3487/10240  Batch Loss: 0.8106  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6331\n",
      "Epoch 001  Batch 3488/10240  Batch Loss: 1.4978  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6330\n",
      "Epoch 001  Batch 3489/10240  Batch Loss: 1.7913  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6329\n",
      "Epoch 001  Batch 3490/10240  Batch Loss: 1.3505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6329\n",
      "Epoch 001  Batch 3491/10240  Batch Loss: 0.7630  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6329\n",
      "Epoch 001  Batch 3492/10240  Batch Loss: 1.6794  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6329\n",
      "Epoch 001  Batch 3493/10240  Batch Loss: 1.8552  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6328\n",
      "Epoch 001  Batch 3494/10240  Batch Loss: 1.7444  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6328\n",
      "Epoch 001  Batch 3495/10240  Batch Loss: 2.0128  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6327\n",
      "Epoch 001  Batch 3496/10240  Batch Loss: 0.7449  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6327\n",
      "Epoch 001  Batch 3497/10240  Batch Loss: 2.0777  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6327\n",
      "Epoch 001  Batch 3498/10240  Batch Loss: 2.3200  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6326\n",
      "Epoch 001  Batch 3499/10240  Batch Loss: 1.7103  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6325\n",
      "Epoch 001  Batch 3500/10240  Batch Loss: 1.9445  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6325\n",
      "Epoch 001  Batch 3501/10240  Batch Loss: 0.9110  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6325\n",
      "Epoch 001  Batch 3502/10240  Batch Loss: 0.9487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6325\n",
      "Epoch 001  Batch 3503/10240  Batch Loss: 1.6629  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3504/10240  Batch Loss: 0.7266  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3505/10240  Batch Loss: 1.2570  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3506/10240  Batch Loss: 1.2774  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3507/10240  Batch Loss: 0.8216  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3508/10240  Batch Loss: 1.2657  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3509/10240  Batch Loss: 0.8702  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3510/10240  Batch Loss: 0.9081  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3511/10240  Batch Loss: 0.7922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3512/10240  Batch Loss: 0.9326  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3513/10240  Batch Loss: 0.7799  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6324\n",
      "Epoch 001  Batch 3514/10240  Batch Loss: 1.2313  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6323\n",
      "Epoch 001  Batch 3515/10240  Batch Loss: 1.4804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6323\n",
      "Epoch 001  Batch 3516/10240  Batch Loss: 3.2049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6322\n",
      "Epoch 001  Batch 3517/10240  Batch Loss: 0.9243  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6322\n",
      "Epoch 001  Batch 3518/10240  Batch Loss: 1.1996  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6322\n",
      "Epoch 001  Batch 3519/10240  Batch Loss: 1.1481  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6322\n",
      "Epoch 001  Batch 3520/10240  Batch Loss: 2.0311  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6322\n",
      "Epoch 001  Batch 3521/10240  Batch Loss: 1.8427  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6322\n",
      "Epoch 001  Batch 3522/10240  Batch Loss: 2.5399  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6321\n",
      "Epoch 001  Batch 3523/10240  Batch Loss: 0.8236  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6321\n",
      "Epoch 001  Batch 3524/10240  Batch Loss: 1.5436  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6321\n",
      "Epoch 001  Batch 3525/10240  Batch Loss: 1.1183  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6320\n",
      "Epoch 001  Batch 3526/10240  Batch Loss: 1.7135  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6320\n",
      "Epoch 001  Batch 3527/10240  Batch Loss: 2.3699  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6320\n",
      "Epoch 001  Batch 3528/10240  Batch Loss: 1.4972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6319\n",
      "Epoch 001  Batch 3529/10240  Batch Loss: 1.2366  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6319\n",
      "Epoch 001  Batch 3530/10240  Batch Loss: 2.1830  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6318\n",
      "Epoch 001  Batch 3531/10240  Batch Loss: 0.9034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6318\n",
      "Epoch 001  Batch 3532/10240  Batch Loss: 0.7252  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6318\n",
      "Epoch 001  Batch 3533/10240  Batch Loss: 2.0949  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6317\n",
      "Epoch 001  Batch 3534/10240  Batch Loss: 2.3573  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6316\n",
      "Epoch 001  Batch 3535/10240  Batch Loss: 0.7438  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6316\n",
      "Epoch 001  Batch 3536/10240  Batch Loss: 2.4688  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6316\n",
      "Epoch 001  Batch 3537/10240  Batch Loss: 1.6706  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6315\n",
      "Epoch 001  Batch 3538/10240  Batch Loss: 1.6014  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6315\n",
      "Epoch 001  Batch 3539/10240  Batch Loss: 2.0487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6314\n",
      "Epoch 001  Batch 3540/10240  Batch Loss: 0.9038  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6314\n",
      "Epoch 001  Batch 3541/10240  Batch Loss: 1.4482  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6313\n",
      "Epoch 001  Batch 3542/10240  Batch Loss: 0.7532  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6313\n",
      "Epoch 001  Batch 3543/10240  Batch Loss: 0.7368  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6313\n",
      "Epoch 001  Batch 3544/10240  Batch Loss: 2.1900  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6312\n",
      "Epoch 001  Batch 3545/10240  Batch Loss: 2.5892  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6313\n",
      "Epoch 001  Batch 3546/10240  Batch Loss: 1.8645  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6312\n",
      "Epoch 001  Batch 3547/10240  Batch Loss: 1.0527  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6311\n",
      "Epoch 001  Batch 3548/10240  Batch Loss: 1.7200  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6311\n",
      "Epoch 001  Batch 3549/10240  Batch Loss: 1.1579  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6311\n",
      "Epoch 001  Batch 3550/10240  Batch Loss: 1.2617  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6310\n",
      "Epoch 001  Batch 3551/10240  Batch Loss: 1.4473  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6310\n",
      "Epoch 001  Batch 3552/10240  Batch Loss: 1.3972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6310\n",
      "Epoch 001  Batch 3553/10240  Batch Loss: 1.7067  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6309\n",
      "Epoch 001  Batch 3554/10240  Batch Loss: 1.4667  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6309\n",
      "Epoch 001  Batch 3555/10240  Batch Loss: 1.9122  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3556/10240  Batch Loss: 1.0375  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3557/10240  Batch Loss: 1.0320  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6309\n",
      "Epoch 001  Batch 3558/10240  Batch Loss: 0.9632  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6309\n",
      "Epoch 001  Batch 3559/10240  Batch Loss: 2.2566  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3560/10240  Batch Loss: 1.1429  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3561/10240  Batch Loss: 2.3394  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3562/10240  Batch Loss: 1.1839  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3563/10240  Batch Loss: 1.4455  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3564/10240  Batch Loss: 0.7918  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6308\n",
      "Epoch 001  Batch 3565/10240  Batch Loss: 1.7431  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6307\n",
      "Epoch 001  Batch 3566/10240  Batch Loss: 1.3221  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6307\n",
      "Epoch 001  Batch 3567/10240  Batch Loss: 2.2143  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6306\n",
      "Epoch 001  Batch 3568/10240  Batch Loss: 3.9739  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6303\n",
      "Epoch 001  Batch 3569/10240  Batch Loss: 2.4030  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6302\n",
      "Epoch 001  Batch 3570/10240  Batch Loss: 1.4213  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6302\n",
      "Epoch 001  Batch 3571/10240  Batch Loss: 2.1376  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6302\n",
      "Epoch 001  Batch 3572/10240  Batch Loss: 0.9723  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6301\n",
      "Epoch 001  Batch 3573/10240  Batch Loss: 2.1804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6301\n",
      "Epoch 001  Batch 3574/10240  Batch Loss: 0.7683  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6301\n",
      "Epoch 001  Batch 3575/10240  Batch Loss: 2.9465  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6300\n",
      "Epoch 001  Batch 3576/10240  Batch Loss: 1.2064  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6299\n",
      "Epoch 001  Batch 3577/10240  Batch Loss: 1.2479  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6299\n",
      "Epoch 001  Batch 3578/10240  Batch Loss: 1.4060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6299\n",
      "Epoch 001  Batch 3579/10240  Batch Loss: 1.0801  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6299\n",
      "Epoch 001  Batch 3580/10240  Batch Loss: 1.7750  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6299\n",
      "Epoch 001  Batch 3581/10240  Batch Loss: 1.4502  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6298\n",
      "Epoch 001  Batch 3582/10240  Batch Loss: 1.1603  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6298\n",
      "Epoch 001  Batch 3583/10240  Batch Loss: 0.7361  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6298\n",
      "Epoch 001  Batch 3584/10240  Batch Loss: 0.8847  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6298\n",
      "Epoch 001  Batch 3585/10240  Batch Loss: 1.8801  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6298\n",
      "Epoch 001  Batch 3586/10240  Batch Loss: 1.7945  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6297\n",
      "Epoch 001  Batch 3587/10240  Batch Loss: 0.8865  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6297\n",
      "Epoch 001  Batch 3588/10240  Batch Loss: 2.1174  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6296\n",
      "Epoch 001  Batch 3589/10240  Batch Loss: 2.0714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6296\n",
      "Epoch 001  Batch 3590/10240  Batch Loss: 1.3806  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6295\n",
      "Epoch 001  Batch 3591/10240  Batch Loss: 1.1815  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6295\n",
      "Epoch 001  Batch 3592/10240  Batch Loss: 1.4180  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6295\n",
      "Epoch 001  Batch 3593/10240  Batch Loss: 1.7167  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6295\n",
      "Epoch 001  Batch 3594/10240  Batch Loss: 1.1739  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3595/10240  Batch Loss: 1.6415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6295\n",
      "Epoch 001  Batch 3596/10240  Batch Loss: 2.1293  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3597/10240  Batch Loss: 1.4406  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3598/10240  Batch Loss: 1.4469  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3599/10240  Batch Loss: 1.7902  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6293\n",
      "Epoch 001  Batch 3600/10240  Batch Loss: 1.4484  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3601/10240  Batch Loss: 0.7224  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3602/10240  Batch Loss: 1.3716  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6294\n",
      "Epoch 001  Batch 3603/10240  Batch Loss: 2.6654  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6293\n",
      "Epoch 001  Batch 3604/10240  Batch Loss: 0.9063  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3605/10240  Batch Loss: 0.7353  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3606/10240  Batch Loss: 1.1548  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3607/10240  Batch Loss: 1.2149  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3608/10240  Batch Loss: 0.8839  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3609/10240  Batch Loss: 1.0797  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3610/10240  Batch Loss: 1.0304  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3611/10240  Batch Loss: 0.7436  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3612/10240  Batch Loss: 1.0341  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3613/10240  Batch Loss: 1.6364  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3614/10240  Batch Loss: 0.7518  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6292\n",
      "Epoch 001  Batch 3615/10240  Batch Loss: 1.5697  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6291\n",
      "Epoch 001  Batch 3616/10240  Batch Loss: 2.0536  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6290\n",
      "Epoch 001  Batch 3617/10240  Batch Loss: 0.7452  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6290\n",
      "Epoch 001  Batch 3618/10240  Batch Loss: 1.7912  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6290\n",
      "Epoch 001  Batch 3619/10240  Batch Loss: 1.2188  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6290\n",
      "Epoch 001  Batch 3620/10240  Batch Loss: 1.4065  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6290\n",
      "Epoch 001  Batch 3621/10240  Batch Loss: 1.5278  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6289\n",
      "Epoch 001  Batch 3622/10240  Batch Loss: 1.1578  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6289\n",
      "Epoch 001  Batch 3623/10240  Batch Loss: 0.7629  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6289\n",
      "Epoch 001  Batch 3624/10240  Batch Loss: 1.5713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6289\n",
      "Epoch 001  Batch 3625/10240  Batch Loss: 2.3126  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6288\n",
      "Epoch 001  Batch 3626/10240  Batch Loss: 1.1965  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6288\n",
      "Epoch 001  Batch 3627/10240  Batch Loss: 1.1437  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6288\n",
      "Epoch 001  Batch 3628/10240  Batch Loss: 2.4791  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6286\n",
      "Epoch 001  Batch 3629/10240  Batch Loss: 1.3127  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6286\n",
      "Epoch 001  Batch 3630/10240  Batch Loss: 0.8075  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6286\n",
      "Epoch 001  Batch 3631/10240  Batch Loss: 1.5397  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6286\n",
      "Epoch 001  Batch 3632/10240  Batch Loss: 1.4640  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6285\n",
      "Epoch 001  Batch 3633/10240  Batch Loss: 0.9505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6286\n",
      "Epoch 001  Batch 3634/10240  Batch Loss: 1.5420  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6285\n",
      "Epoch 001  Batch 3635/10240  Batch Loss: 0.7302  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6285\n",
      "Epoch 001  Batch 3636/10240  Batch Loss: 0.7243  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6285\n",
      "Epoch 001  Batch 3637/10240  Batch Loss: 0.9466  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6285\n",
      "Epoch 001  Batch 3638/10240  Batch Loss: 2.3908  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6284\n",
      "Epoch 001  Batch 3639/10240  Batch Loss: 1.3610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6284\n",
      "Epoch 001  Batch 3640/10240  Batch Loss: 2.3462  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6283\n",
      "Epoch 001  Batch 3641/10240  Batch Loss: 0.7273  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6283\n",
      "Epoch 001  Batch 3642/10240  Batch Loss: 1.7349  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6282\n",
      "Epoch 001  Batch 3643/10240  Batch Loss: 1.6425  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6282\n",
      "Epoch 001  Batch 3644/10240  Batch Loss: 1.0680  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6282\n",
      "Epoch 001  Batch 3645/10240  Batch Loss: 1.6272  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6281\n",
      "Epoch 001  Batch 3646/10240  Batch Loss: 2.7251  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6280\n",
      "Epoch 001  Batch 3647/10240  Batch Loss: 1.3270  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6280\n",
      "Epoch 001  Batch 3648/10240  Batch Loss: 1.2960  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6280\n",
      "Epoch 001  Batch 3649/10240  Batch Loss: 3.1083  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6279\n",
      "Epoch 001  Batch 3650/10240  Batch Loss: 1.6614  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6279\n",
      "Epoch 001  Batch 3651/10240  Batch Loss: 3.1034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6278\n",
      "Epoch 001  Batch 3652/10240  Batch Loss: 1.2458  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6277\n",
      "Epoch 001  Batch 3653/10240  Batch Loss: 2.2725  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6277\n",
      "Epoch 001  Batch 3654/10240  Batch Loss: 0.9144  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6277\n",
      "Epoch 001  Batch 3655/10240  Batch Loss: 1.5085  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6276\n",
      "Epoch 001  Batch 3656/10240  Batch Loss: 1.3076  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6276\n",
      "Epoch 001  Batch 3657/10240  Batch Loss: 1.3175  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6275\n",
      "Epoch 001  Batch 3658/10240  Batch Loss: 2.2192  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6275\n",
      "Epoch 001  Batch 3659/10240  Batch Loss: 2.1097  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6274\n",
      "Epoch 001  Batch 3660/10240  Batch Loss: 2.0308  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3661/10240  Batch Loss: 0.7398  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3662/10240  Batch Loss: 1.7733  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3663/10240  Batch Loss: 1.4039  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3664/10240  Batch Loss: 2.7233  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6272\n",
      "Epoch 001  Batch 3665/10240  Batch Loss: 0.9860  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6272\n",
      "Epoch 001  Batch 3666/10240  Batch Loss: 1.0320  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3667/10240  Batch Loss: 0.8733  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3668/10240  Batch Loss: 0.7567  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3669/10240  Batch Loss: 2.0065  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6272\n",
      "Epoch 001  Batch 3670/10240  Batch Loss: 1.0781  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3671/10240  Batch Loss: 0.7500  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6273\n",
      "Epoch 001  Batch 3672/10240  Batch Loss: 2.7490  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6272\n",
      "Epoch 001  Batch 3673/10240  Batch Loss: 1.4878  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6272\n",
      "Epoch 001  Batch 3674/10240  Batch Loss: 1.2304  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6271\n",
      "Epoch 001  Batch 3675/10240  Batch Loss: 1.0856  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6271\n",
      "Epoch 001  Batch 3676/10240  Batch Loss: 1.7649  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6271\n",
      "Epoch 001  Batch 3677/10240  Batch Loss: 1.3933  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6271\n",
      "Epoch 001  Batch 3678/10240  Batch Loss: 1.2844  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6270\n",
      "Epoch 001  Batch 3679/10240  Batch Loss: 2.1736  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6270\n",
      "Epoch 001  Batch 3680/10240  Batch Loss: 1.5207  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6270\n",
      "Epoch 001  Batch 3681/10240  Batch Loss: 0.9873  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6270\n",
      "Epoch 001  Batch 3682/10240  Batch Loss: 2.0589  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6269\n",
      "Epoch 001  Batch 3683/10240  Batch Loss: 1.5785  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6269\n",
      "Epoch 001  Batch 3684/10240  Batch Loss: 1.1265  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6269\n",
      "Epoch 001  Batch 3685/10240  Batch Loss: 1.9194  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6268\n",
      "Epoch 001  Batch 3686/10240  Batch Loss: 1.7954  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6267\n",
      "Epoch 001  Batch 3687/10240  Batch Loss: 1.7757  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6267\n",
      "Epoch 001  Batch 3688/10240  Batch Loss: 0.7478  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6267\n",
      "Epoch 001  Batch 3689/10240  Batch Loss: 1.0497  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6267\n",
      "Epoch 001  Batch 3690/10240  Batch Loss: 1.7227  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6267\n",
      "Epoch 001  Batch 3691/10240  Batch Loss: 1.1772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6267\n",
      "Epoch 001  Batch 3692/10240  Batch Loss: 1.6254  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6266\n",
      "Epoch 001  Batch 3693/10240  Batch Loss: 1.8560  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6266\n",
      "Epoch 001  Batch 3694/10240  Batch Loss: 1.2771  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6266\n",
      "Epoch 001  Batch 3695/10240  Batch Loss: 1.2518  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6266\n",
      "Epoch 001  Batch 3696/10240  Batch Loss: 1.9705  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6265\n",
      "Epoch 001  Batch 3697/10240  Batch Loss: 1.8717  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6265\n",
      "Epoch 001  Batch 3698/10240  Batch Loss: 2.1820  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6264\n",
      "Epoch 001  Batch 3699/10240  Batch Loss: 1.0242  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6264\n",
      "Epoch 001  Batch 3700/10240  Batch Loss: 0.9705  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6264\n",
      "Epoch 001  Batch 3701/10240  Batch Loss: 1.0152  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6263\n",
      "Epoch 001  Batch 3702/10240  Batch Loss: 2.2191  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6264\n",
      "Epoch 001  Batch 3703/10240  Batch Loss: 1.5609  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6263\n",
      "Epoch 001  Batch 3704/10240  Batch Loss: 0.7759  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6263\n",
      "Epoch 001  Batch 3705/10240  Batch Loss: 2.0867  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6263\n",
      "Epoch 001  Batch 3706/10240  Batch Loss: 1.2250  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6263\n",
      "Epoch 001  Batch 3707/10240  Batch Loss: 1.2324  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6262\n",
      "Epoch 001  Batch 3708/10240  Batch Loss: 1.2946  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6262\n",
      "Epoch 001  Batch 3709/10240  Batch Loss: 1.9199  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6262\n",
      "Epoch 001  Batch 3710/10240  Batch Loss: 3.3658  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6262\n",
      "Epoch 001  Batch 3711/10240  Batch Loss: 1.1502  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6262\n",
      "Epoch 001  Batch 3712/10240  Batch Loss: 1.8127  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6261\n",
      "Epoch 001  Batch 3713/10240  Batch Loss: 1.0719  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6261\n",
      "Epoch 001  Batch 3714/10240  Batch Loss: 2.0173  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6261\n",
      "Epoch 001  Batch 3715/10240  Batch Loss: 1.5533  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6260\n",
      "Epoch 001  Batch 3716/10240  Batch Loss: 1.2279  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6261\n",
      "Epoch 001  Batch 3717/10240  Batch Loss: 0.7766  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6261\n",
      "Epoch 001  Batch 3718/10240  Batch Loss: 1.0249  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6261\n",
      "Epoch 001  Batch 3719/10240  Batch Loss: 1.1865  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6260\n",
      "Epoch 001  Batch 3720/10240  Batch Loss: 1.8410  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6260\n",
      "Epoch 001  Batch 3721/10240  Batch Loss: 1.7060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6259\n",
      "Epoch 001  Batch 3722/10240  Batch Loss: 1.5543  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6259\n",
      "Epoch 001  Batch 3723/10240  Batch Loss: 0.7774  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6259\n",
      "Epoch 001  Batch 3724/10240  Batch Loss: 1.6246  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6259\n",
      "Epoch 001  Batch 3725/10240  Batch Loss: 1.7223  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6259\n",
      "Epoch 001  Batch 3726/10240  Batch Loss: 2.4286  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6258\n",
      "Epoch 001  Batch 3727/10240  Batch Loss: 2.8644  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6257\n",
      "Epoch 001  Batch 3728/10240  Batch Loss: 1.3725  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6257\n",
      "Epoch 001  Batch 3729/10240  Batch Loss: 1.2365  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6257\n",
      "Epoch 001  Batch 3730/10240  Batch Loss: 1.4895  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6257\n",
      "Epoch 001  Batch 3731/10240  Batch Loss: 1.1908  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6256\n",
      "Epoch 001  Batch 3732/10240  Batch Loss: 2.3023  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6256\n",
      "Epoch 001  Batch 3733/10240  Batch Loss: 1.2741  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6255\n",
      "Epoch 001  Batch 3734/10240  Batch Loss: 3.0956  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6254\n",
      "Epoch 001  Batch 3735/10240  Batch Loss: 0.9340  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6254\n",
      "Epoch 001  Batch 3736/10240  Batch Loss: 1.0394  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6254\n",
      "Epoch 001  Batch 3737/10240  Batch Loss: 1.3715  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6254\n",
      "Epoch 001  Batch 3738/10240  Batch Loss: 3.2034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6252\n",
      "Epoch 001  Batch 3739/10240  Batch Loss: 2.7660  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6252\n",
      "Epoch 001  Batch 3740/10240  Batch Loss: 1.8233  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3741/10240  Batch Loss: 0.7472  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3742/10240  Batch Loss: 1.2017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3743/10240  Batch Loss: 0.7661  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3744/10240  Batch Loss: 1.0522  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3745/10240  Batch Loss: 1.2639  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3746/10240  Batch Loss: 2.1507  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3747/10240  Batch Loss: 1.5369  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3748/10240  Batch Loss: 1.0363  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3749/10240  Batch Loss: 2.1873  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6251\n",
      "Epoch 001  Batch 3750/10240  Batch Loss: 1.4248  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3751/10240  Batch Loss: 1.0218  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3752/10240  Batch Loss: 1.2336  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3753/10240  Batch Loss: 1.4989  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3754/10240  Batch Loss: 0.7790  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3755/10240  Batch Loss: 1.2593  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3756/10240  Batch Loss: 0.7569  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3757/10240  Batch Loss: 0.7306  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3758/10240  Batch Loss: 1.0241  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3759/10240  Batch Loss: 1.2989  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3760/10240  Batch Loss: 1.7979  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6249\n",
      "Epoch 001  Batch 3761/10240  Batch Loss: 1.4002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3762/10240  Batch Loss: 1.3327  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6250\n",
      "Epoch 001  Batch 3763/10240  Batch Loss: 2.2045  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6249\n",
      "Epoch 001  Batch 3764/10240  Batch Loss: 2.1862  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6248\n",
      "Epoch 001  Batch 3765/10240  Batch Loss: 2.2845  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6247\n",
      "Epoch 001  Batch 3766/10240  Batch Loss: 0.7610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6247\n",
      "Epoch 001  Batch 3767/10240  Batch Loss: 1.1598  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6247\n",
      "Epoch 001  Batch 3768/10240  Batch Loss: 2.3478  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3769/10240  Batch Loss: 1.8642  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3770/10240  Batch Loss: 0.8462  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3771/10240  Batch Loss: 1.3718  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3772/10240  Batch Loss: 0.9497  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3773/10240  Batch Loss: 1.6213  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3774/10240  Batch Loss: 0.7488  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6246\n",
      "Epoch 001  Batch 3775/10240  Batch Loss: 2.1924  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6245\n",
      "Epoch 001  Batch 3776/10240  Batch Loss: 2.2135  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6244\n",
      "Epoch 001  Batch 3777/10240  Batch Loss: 1.4899  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6243\n",
      "Epoch 001  Batch 3778/10240  Batch Loss: 1.6129  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6243\n",
      "Epoch 001  Batch 3779/10240  Batch Loss: 1.7026  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6242\n",
      "Epoch 001  Batch 3780/10240  Batch Loss: 0.7630  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6242\n",
      "Epoch 001  Batch 3781/10240  Batch Loss: 1.6620  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6242\n",
      "Epoch 001  Batch 3782/10240  Batch Loss: 1.4287  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6242\n",
      "Epoch 001  Batch 3783/10240  Batch Loss: 2.6165  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6241\n",
      "Epoch 001  Batch 3784/10240  Batch Loss: 1.3505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6240\n",
      "Epoch 001  Batch 3785/10240  Batch Loss: 1.4450  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6240\n",
      "Epoch 001  Batch 3786/10240  Batch Loss: 1.2841  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6240\n",
      "Epoch 001  Batch 3787/10240  Batch Loss: 0.9894  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6240\n",
      "Epoch 001  Batch 3788/10240  Batch Loss: 1.9933  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6239\n",
      "Epoch 001  Batch 3789/10240  Batch Loss: 0.8912  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6239\n",
      "Epoch 001  Batch 3790/10240  Batch Loss: 0.7488  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6239\n",
      "Epoch 001  Batch 3791/10240  Batch Loss: 1.6853  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6239\n",
      "Epoch 001  Batch 3792/10240  Batch Loss: 1.6599  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6238\n",
      "Epoch 001  Batch 3793/10240  Batch Loss: 2.5883  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3794/10240  Batch Loss: 1.0227  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3795/10240  Batch Loss: 1.8343  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3796/10240  Batch Loss: 0.9530  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3797/10240  Batch Loss: 2.3046  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3798/10240  Batch Loss: 0.7795  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3799/10240  Batch Loss: 1.4413  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3800/10240  Batch Loss: 2.3542  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3801/10240  Batch Loss: 1.6027  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3802/10240  Batch Loss: 0.7456  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3803/10240  Batch Loss: 1.2966  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6237\n",
      "Epoch 001  Batch 3804/10240  Batch Loss: 0.9425  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3805/10240  Batch Loss: 0.9156  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3806/10240  Batch Loss: 0.8088  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3807/10240  Batch Loss: 0.7556  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3808/10240  Batch Loss: 1.9000  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3809/10240  Batch Loss: 1.2461  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3810/10240  Batch Loss: 1.0815  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6236\n",
      "Epoch 001  Batch 3811/10240  Batch Loss: 1.9526  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6235\n",
      "Epoch 001  Batch 3812/10240  Batch Loss: 1.8932  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6235\n",
      "Epoch 001  Batch 3813/10240  Batch Loss: 2.1607  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6235\n",
      "Epoch 001  Batch 3814/10240  Batch Loss: 1.3060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6235\n",
      "Epoch 001  Batch 3815/10240  Batch Loss: 1.8122  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6234\n",
      "Epoch 001  Batch 3816/10240  Batch Loss: 1.4906  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6234\n",
      "Epoch 001  Batch 3817/10240  Batch Loss: 2.7065  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3818/10240  Batch Loss: 1.3233  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3819/10240  Batch Loss: 0.8047  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3820/10240  Batch Loss: 1.5148  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3821/10240  Batch Loss: 1.1797  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6233\n",
      "Epoch 001  Batch 3822/10240  Batch Loss: 2.2567  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3823/10240  Batch Loss: 1.6894  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3824/10240  Batch Loss: 1.6890  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6232\n",
      "Epoch 001  Batch 3825/10240  Batch Loss: 2.3085  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3826/10240  Batch Loss: 0.7382  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3827/10240  Batch Loss: 0.7394  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3828/10240  Batch Loss: 0.9427  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3829/10240  Batch Loss: 1.4719  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3830/10240  Batch Loss: 0.9303  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3831/10240  Batch Loss: 1.3341  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3832/10240  Batch Loss: 1.3639  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3833/10240  Batch Loss: 1.8978  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6230\n",
      "Epoch 001  Batch 3834/10240  Batch Loss: 0.9759  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6231\n",
      "Epoch 001  Batch 3835/10240  Batch Loss: 0.8930  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6230\n",
      "Epoch 001  Batch 3836/10240  Batch Loss: 1.0776  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6230\n",
      "Epoch 001  Batch 3837/10240  Batch Loss: 1.6890  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6230\n",
      "Epoch 001  Batch 3838/10240  Batch Loss: 0.9804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6230\n",
      "Epoch 001  Batch 3839/10240  Batch Loss: 1.5394  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6229\n",
      "Epoch 001  Batch 3840/10240  Batch Loss: 1.6388  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6229\n",
      "Epoch 001  Batch 3841/10240  Batch Loss: 0.7778  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6229\n",
      "Epoch 001  Batch 3842/10240  Batch Loss: 1.7288  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6229\n",
      "Epoch 001  Batch 3843/10240  Batch Loss: 0.9245  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6229\n",
      "Epoch 001  Batch 3844/10240  Batch Loss: 1.0713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6229\n",
      "Epoch 001  Batch 3845/10240  Batch Loss: 1.3576  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6228\n",
      "Epoch 001  Batch 3846/10240  Batch Loss: 1.6923  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6228\n",
      "Epoch 001  Batch 3847/10240  Batch Loss: 2.0057  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6227\n",
      "Epoch 001  Batch 3848/10240  Batch Loss: 1.0346  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6228\n",
      "Epoch 001  Batch 3849/10240  Batch Loss: 0.8923  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6228\n",
      "Epoch 001  Batch 3850/10240  Batch Loss: 2.3967  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6227\n",
      "Epoch 001  Batch 3851/10240  Batch Loss: 1.7765  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6226\n",
      "Epoch 001  Batch 3852/10240  Batch Loss: 1.2891  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6226\n",
      "Epoch 001  Batch 3853/10240  Batch Loss: 1.2927  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6226\n",
      "Epoch 001  Batch 3854/10240  Batch Loss: 2.2245  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3855/10240  Batch Loss: 1.4079  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3856/10240  Batch Loss: 0.9519  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3857/10240  Batch Loss: 0.7438  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3858/10240  Batch Loss: 1.4787  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3859/10240  Batch Loss: 1.3387  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3860/10240  Batch Loss: 1.1841  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3861/10240  Batch Loss: 1.0073  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3862/10240  Batch Loss: 0.9829  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3863/10240  Batch Loss: 1.1972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3864/10240  Batch Loss: 0.8422  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3865/10240  Batch Loss: 2.2753  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3866/10240  Batch Loss: 1.0392  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3867/10240  Batch Loss: 2.0024  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3868/10240  Batch Loss: 1.7924  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6225\n",
      "Epoch 001  Batch 3869/10240  Batch Loss: 1.6857  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6224\n",
      "Epoch 001  Batch 3870/10240  Batch Loss: 2.2179  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6224\n",
      "Epoch 001  Batch 3871/10240  Batch Loss: 1.7564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6223\n",
      "Epoch 001  Batch 3872/10240  Batch Loss: 1.0401  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6223\n",
      "Epoch 001  Batch 3873/10240  Batch Loss: 1.3694  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6223\n",
      "Epoch 001  Batch 3874/10240  Batch Loss: 1.4442  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6223\n",
      "Epoch 001  Batch 3875/10240  Batch Loss: 1.5471  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6223\n",
      "Epoch 001  Batch 3876/10240  Batch Loss: 1.2264  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6223\n",
      "Epoch 001  Batch 3877/10240  Batch Loss: 1.2595  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3878/10240  Batch Loss: 1.4663  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3879/10240  Batch Loss: 0.8456  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3880/10240  Batch Loss: 1.7715  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3881/10240  Batch Loss: 1.2356  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3882/10240  Batch Loss: 1.5349  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6221\n",
      "Epoch 001  Batch 3883/10240  Batch Loss: 2.2501  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3884/10240  Batch Loss: 0.7759  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3885/10240  Batch Loss: 1.4468  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6222\n",
      "Epoch 001  Batch 3886/10240  Batch Loss: 2.0820  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6221\n",
      "Epoch 001  Batch 3887/10240  Batch Loss: 1.0984  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6221\n",
      "Epoch 001  Batch 3888/10240  Batch Loss: 1.7509  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6221\n",
      "Epoch 001  Batch 3889/10240  Batch Loss: 0.7881  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6221\n",
      "Epoch 001  Batch 3890/10240  Batch Loss: 1.1814  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6221\n",
      "Epoch 001  Batch 3891/10240  Batch Loss: 1.7369  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6220\n",
      "Epoch 001  Batch 3892/10240  Batch Loss: 2.1451  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6219\n",
      "Epoch 001  Batch 3893/10240  Batch Loss: 2.4256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6219\n",
      "Epoch 001  Batch 3894/10240  Batch Loss: 1.3315  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3895/10240  Batch Loss: 0.7403  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3896/10240  Batch Loss: 1.7621  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3897/10240  Batch Loss: 2.0562  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3898/10240  Batch Loss: 2.3462  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3899/10240  Batch Loss: 1.3703  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6217\n",
      "Epoch 001  Batch 3900/10240  Batch Loss: 1.7075  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3901/10240  Batch Loss: 0.7473  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6218\n",
      "Epoch 001  Batch 3902/10240  Batch Loss: 1.8045  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6217\n",
      "Epoch 001  Batch 3903/10240  Batch Loss: 1.8117  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6217\n",
      "Epoch 001  Batch 3904/10240  Batch Loss: 0.8927  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6217\n",
      "Epoch 001  Batch 3905/10240  Batch Loss: 1.2437  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6216\n",
      "Epoch 001  Batch 3906/10240  Batch Loss: 1.0324  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6216\n",
      "Epoch 001  Batch 3907/10240  Batch Loss: 0.9090  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6216\n",
      "Epoch 001  Batch 3908/10240  Batch Loss: 1.5138  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6216\n",
      "Epoch 001  Batch 3909/10240  Batch Loss: 0.7562  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6216\n",
      "Epoch 001  Batch 3910/10240  Batch Loss: 1.0509  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3911/10240  Batch Loss: 1.1232  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3912/10240  Batch Loss: 1.0779  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3913/10240  Batch Loss: 1.4891  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3914/10240  Batch Loss: 0.8875  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3915/10240  Batch Loss: 2.6745  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6216\n",
      "Epoch 001  Batch 3916/10240  Batch Loss: 3.0481  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3917/10240  Batch Loss: 1.9160  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3918/10240  Batch Loss: 1.2474  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3919/10240  Batch Loss: 1.5267  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3920/10240  Batch Loss: 0.8677  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3921/10240  Batch Loss: 0.7559  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3922/10240  Batch Loss: 2.0202  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3923/10240  Batch Loss: 0.9079  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3924/10240  Batch Loss: 1.6449  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3925/10240  Batch Loss: 0.7622  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3926/10240  Batch Loss: 1.0985  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6215\n",
      "Epoch 001  Batch 3927/10240  Batch Loss: 0.8004  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3928/10240  Batch Loss: 1.9297  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3929/10240  Batch Loss: 1.5417  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3930/10240  Batch Loss: 1.3279  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6214\n",
      "Epoch 001  Batch 3931/10240  Batch Loss: 1.7792  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6213\n",
      "Epoch 001  Batch 3932/10240  Batch Loss: 1.1061  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6213\n",
      "Epoch 001  Batch 3933/10240  Batch Loss: 2.2204  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6212\n",
      "Epoch 001  Batch 3934/10240  Batch Loss: 1.3940  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6212\n",
      "Epoch 001  Batch 3935/10240  Batch Loss: 1.4936  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6212\n",
      "Epoch 001  Batch 3936/10240  Batch Loss: 1.6867  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6211\n",
      "Epoch 001  Batch 3937/10240  Batch Loss: 1.6541  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6211\n",
      "Epoch 001  Batch 3938/10240  Batch Loss: 1.5669  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6211\n",
      "Epoch 001  Batch 3939/10240  Batch Loss: 1.8713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6211\n",
      "Epoch 001  Batch 3940/10240  Batch Loss: 1.0828  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3941/10240  Batch Loss: 1.1518  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3942/10240  Batch Loss: 1.4093  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3943/10240  Batch Loss: 0.7742  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3944/10240  Batch Loss: 1.0267  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3945/10240  Batch Loss: 1.1827  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3946/10240  Batch Loss: 1.4296  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6210\n",
      "Epoch 001  Batch 3947/10240  Batch Loss: 0.8668  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6209\n",
      "Epoch 001  Batch 3948/10240  Batch Loss: 1.9790  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6209\n",
      "Epoch 001  Batch 3949/10240  Batch Loss: 0.8234  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6209\n",
      "Epoch 001  Batch 3950/10240  Batch Loss: 2.3985  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6209\n",
      "Epoch 001  Batch 3951/10240  Batch Loss: 1.1049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6208\n",
      "Epoch 001  Batch 3952/10240  Batch Loss: 1.2617  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6208\n",
      "Epoch 001  Batch 3953/10240  Batch Loss: 1.0140  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6208\n",
      "Epoch 001  Batch 3954/10240  Batch Loss: 1.1446  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6208\n",
      "Epoch 001  Batch 3955/10240  Batch Loss: 1.3043  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6208\n",
      "Epoch 001  Batch 3956/10240  Batch Loss: 1.5785  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6208\n",
      "Epoch 001  Batch 3957/10240  Batch Loss: 1.3581  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6207\n",
      "Epoch 001  Batch 3958/10240  Batch Loss: 0.9436  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6207\n",
      "Epoch 001  Batch 3959/10240  Batch Loss: 1.1228  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6207\n",
      "Epoch 001  Batch 3960/10240  Batch Loss: 0.7673  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6207\n",
      "Epoch 001  Batch 3961/10240  Batch Loss: 2.1417  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6206\n",
      "Epoch 001  Batch 3962/10240  Batch Loss: 1.6548  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6206\n",
      "Epoch 001  Batch 3963/10240  Batch Loss: 1.8319  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6205\n",
      "Epoch 001  Batch 3964/10240  Batch Loss: 0.7458  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6205\n",
      "Epoch 001  Batch 3965/10240  Batch Loss: 1.3113  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6205\n",
      "Epoch 001  Batch 3966/10240  Batch Loss: 1.0922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6205\n",
      "Epoch 001  Batch 3967/10240  Batch Loss: 1.9124  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6205\n",
      "Epoch 001  Batch 3968/10240  Batch Loss: 2.8081  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6204\n",
      "Epoch 001  Batch 3969/10240  Batch Loss: 2.2528  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6204\n",
      "Epoch 001  Batch 3970/10240  Batch Loss: 1.2926  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6204\n",
      "Epoch 001  Batch 3971/10240  Batch Loss: 1.7819  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6204\n",
      "Epoch 001  Batch 3972/10240  Batch Loss: 0.9380  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6204\n",
      "Epoch 001  Batch 3973/10240  Batch Loss: 1.6530  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6203\n",
      "Epoch 001  Batch 3974/10240  Batch Loss: 1.4895  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6203\n",
      "Epoch 001  Batch 3975/10240  Batch Loss: 1.6024  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6203\n",
      "Epoch 001  Batch 3976/10240  Batch Loss: 1.3268  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6203\n",
      "Epoch 001  Batch 3977/10240  Batch Loss: 0.9952  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3978/10240  Batch Loss: 1.7709  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3979/10240  Batch Loss: 1.0166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3980/10240  Batch Loss: 1.3087  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3981/10240  Batch Loss: 1.3983  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3982/10240  Batch Loss: 1.4004  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3983/10240  Batch Loss: 1.4292  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3984/10240  Batch Loss: 1.2796  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6202\n",
      "Epoch 001  Batch 3985/10240  Batch Loss: 1.7890  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6201\n",
      "Epoch 001  Batch 3986/10240  Batch Loss: 1.4264  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6201\n",
      "Epoch 001  Batch 3987/10240  Batch Loss: 0.8084  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6201\n",
      "Epoch 001  Batch 3988/10240  Batch Loss: 1.8355  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6201\n",
      "Epoch 001  Batch 3989/10240  Batch Loss: 1.2990  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6201\n",
      "Epoch 001  Batch 3990/10240  Batch Loss: 1.3088  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6201\n",
      "Epoch 001  Batch 3991/10240  Batch Loss: 1.2002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6200\n",
      "Epoch 001  Batch 3992/10240  Batch Loss: 1.7542  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6200\n",
      "Epoch 001  Batch 3993/10240  Batch Loss: 2.3753  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 3994/10240  Batch Loss: 1.6073  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 3995/10240  Batch Loss: 1.0977  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 3996/10240  Batch Loss: 1.0827  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 3997/10240  Batch Loss: 0.9551  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 3998/10240  Batch Loss: 1.8578  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 3999/10240  Batch Loss: 1.4903  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 4000/10240  Batch Loss: 1.1228  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 4001/10240  Batch Loss: 1.3101  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6199\n",
      "Epoch 001  Batch 4002/10240  Batch Loss: 1.2771  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6198\n",
      "Epoch 001  Batch 4003/10240  Batch Loss: 1.1005  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6198\n",
      "Epoch 001  Batch 4004/10240  Batch Loss: 1.8388  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6198\n",
      "Epoch 001  Batch 4005/10240  Batch Loss: 1.6084  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6197\n",
      "Epoch 001  Batch 4006/10240  Batch Loss: 1.0100  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6197\n",
      "Epoch 001  Batch 4007/10240  Batch Loss: 1.7877  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6196\n",
      "Epoch 001  Batch 4008/10240  Batch Loss: 1.0759  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6196\n",
      "Epoch 001  Batch 4009/10240  Batch Loss: 1.2807  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6196\n",
      "Epoch 001  Batch 4010/10240  Batch Loss: 1.3325  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6196\n",
      "Epoch 001  Batch 4011/10240  Batch Loss: 1.4331  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6196\n",
      "Epoch 001  Batch 4012/10240  Batch Loss: 1.2055  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6196\n",
      "Epoch 001  Batch 4013/10240  Batch Loss: 1.2770  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6195\n",
      "Epoch 001  Batch 4014/10240  Batch Loss: 1.0882  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6195\n",
      "Epoch 001  Batch 4015/10240  Batch Loss: 1.6978  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6195\n",
      "Epoch 001  Batch 4016/10240  Batch Loss: 1.6247  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6195\n",
      "Epoch 001  Batch 4017/10240  Batch Loss: 1.9683  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6194\n",
      "Epoch 001  Batch 4018/10240  Batch Loss: 2.2332  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6194\n",
      "Epoch 001  Batch 4019/10240  Batch Loss: 0.9293  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6194\n",
      "Epoch 001  Batch 4020/10240  Batch Loss: 1.4911  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6194\n",
      "Epoch 001  Batch 4021/10240  Batch Loss: 2.6863  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4022/10240  Batch Loss: 0.8528  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4023/10240  Batch Loss: 1.1666  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4024/10240  Batch Loss: 0.9770  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4025/10240  Batch Loss: 1.2488  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4026/10240  Batch Loss: 0.7765  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4027/10240  Batch Loss: 0.9099  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4028/10240  Batch Loss: 1.3844  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4029/10240  Batch Loss: 1.1458  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6193\n",
      "Epoch 001  Batch 4030/10240  Batch Loss: 2.1493  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6192\n",
      "Epoch 001  Batch 4031/10240  Batch Loss: 1.2915  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6191\n",
      "Epoch 001  Batch 4032/10240  Batch Loss: 1.0002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6191\n",
      "Epoch 001  Batch 4033/10240  Batch Loss: 1.8796  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6191\n",
      "Epoch 001  Batch 4034/10240  Batch Loss: 0.9026  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6191\n",
      "Epoch 001  Batch 4035/10240  Batch Loss: 2.6177  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4036/10240  Batch Loss: 1.3690  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4037/10240  Batch Loss: 1.1800  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4038/10240  Batch Loss: 1.4851  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4039/10240  Batch Loss: 2.6636  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6188\n",
      "Epoch 001  Batch 4040/10240  Batch Loss: 2.2281  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6188\n",
      "Epoch 001  Batch 4041/10240  Batch Loss: 0.7806  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6188\n",
      "Epoch 001  Batch 4042/10240  Batch Loss: 1.6794  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6188\n",
      "Epoch 001  Batch 4043/10240  Batch Loss: 2.3738  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4044/10240  Batch Loss: 0.7346  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4045/10240  Batch Loss: 0.7776  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6189\n",
      "Epoch 001  Batch 4046/10240  Batch Loss: 2.3341  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6188\n",
      "Epoch 001  Batch 4047/10240  Batch Loss: 0.8604  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6188\n",
      "Epoch 001  Batch 4048/10240  Batch Loss: 1.2404  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4049/10240  Batch Loss: 1.4633  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4050/10240  Batch Loss: 1.4265  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4051/10240  Batch Loss: 0.9153  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4052/10240  Batch Loss: 1.4159  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4053/10240  Batch Loss: 0.8679  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4054/10240  Batch Loss: 0.8424  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4055/10240  Batch Loss: 0.7197  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4056/10240  Batch Loss: 1.6400  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4057/10240  Batch Loss: 2.3323  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4058/10240  Batch Loss: 1.7552  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6187\n",
      "Epoch 001  Batch 4059/10240  Batch Loss: 1.9214  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6186\n",
      "Epoch 001  Batch 4060/10240  Batch Loss: 0.7269  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6186\n",
      "Epoch 001  Batch 4061/10240  Batch Loss: 1.5686  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6186\n",
      "Epoch 001  Batch 4062/10240  Batch Loss: 1.5037  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6186\n",
      "Epoch 001  Batch 4063/10240  Batch Loss: 1.2536  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6186\n",
      "Epoch 001  Batch 4064/10240  Batch Loss: 0.7748  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6186\n",
      "Epoch 001  Batch 4065/10240  Batch Loss: 1.4407  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4066/10240  Batch Loss: 1.4325  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4067/10240  Batch Loss: 1.2788  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4068/10240  Batch Loss: 0.7924  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4069/10240  Batch Loss: 1.3315  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4070/10240  Batch Loss: 1.2800  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4071/10240  Batch Loss: 0.9092  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6185\n",
      "Epoch 001  Batch 4072/10240  Batch Loss: 2.0763  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6184\n",
      "Epoch 001  Batch 4073/10240  Batch Loss: 1.8916  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6184\n",
      "Epoch 001  Batch 4074/10240  Batch Loss: 0.9343  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6184\n",
      "Epoch 001  Batch 4075/10240  Batch Loss: 1.6072  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6183\n",
      "Epoch 001  Batch 4076/10240  Batch Loss: 2.0407  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6183\n",
      "Epoch 001  Batch 4077/10240  Batch Loss: 3.4664  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6181\n",
      "Epoch 001  Batch 4078/10240  Batch Loss: 1.8645  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6181\n",
      "Epoch 001  Batch 4079/10240  Batch Loss: 0.7775  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6181\n",
      "Epoch 001  Batch 4080/10240  Batch Loss: 2.4047  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6180\n",
      "Epoch 001  Batch 4081/10240  Batch Loss: 2.0623  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6179\n",
      "Epoch 001  Batch 4082/10240  Batch Loss: 1.7070  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6179\n",
      "Epoch 001  Batch 4083/10240  Batch Loss: 1.8164  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6178\n",
      "Epoch 001  Batch 4084/10240  Batch Loss: 2.4725  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6177\n",
      "Epoch 001  Batch 4085/10240  Batch Loss: 1.1157  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6177\n",
      "Epoch 001  Batch 4086/10240  Batch Loss: 0.7642  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6177\n",
      "Epoch 001  Batch 4087/10240  Batch Loss: 0.8931  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6177\n",
      "Epoch 001  Batch 4088/10240  Batch Loss: 1.2754  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6177\n",
      "Epoch 001  Batch 4089/10240  Batch Loss: 1.7536  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6177\n",
      "Epoch 001  Batch 4090/10240  Batch Loss: 1.4776  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6176\n",
      "Epoch 001  Batch 4091/10240  Batch Loss: 2.4083  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6176\n",
      "Epoch 001  Batch 4092/10240  Batch Loss: 0.7179  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6176\n",
      "Epoch 001  Batch 4093/10240  Batch Loss: 1.7071  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6175\n",
      "Epoch 001  Batch 4094/10240  Batch Loss: 2.6340  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6175\n",
      "Epoch 001  Batch 4095/10240  Batch Loss: 1.0849  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6175\n",
      "Epoch 001  Batch 4096/10240  Batch Loss: 1.8250  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4097/10240  Batch Loss: 2.2819  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4098/10240  Batch Loss: 1.0698  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4099/10240  Batch Loss: 0.8537  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4100/10240  Batch Loss: 1.1094  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4101/10240  Batch Loss: 1.4900  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4102/10240  Batch Loss: 1.3163  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4103/10240  Batch Loss: 1.0327  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6174\n",
      "Epoch 001  Batch 4104/10240  Batch Loss: 1.3823  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4105/10240  Batch Loss: 1.3042  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4106/10240  Batch Loss: 1.6429  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4107/10240  Batch Loss: 1.9562  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4108/10240  Batch Loss: 1.5139  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4109/10240  Batch Loss: 1.4662  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4110/10240  Batch Loss: 1.7903  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4111/10240  Batch Loss: 1.0859  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4112/10240  Batch Loss: 0.9714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4113/10240  Batch Loss: 0.9183  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6173\n",
      "Epoch 001  Batch 4114/10240  Batch Loss: 1.2205  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6172\n",
      "Epoch 001  Batch 4115/10240  Batch Loss: 1.8161  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6172\n",
      "Epoch 001  Batch 4116/10240  Batch Loss: 1.5278  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6172\n",
      "Epoch 001  Batch 4117/10240  Batch Loss: 1.8547  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6172\n",
      "Epoch 001  Batch 4118/10240  Batch Loss: 1.2476  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6172\n",
      "Epoch 001  Batch 4119/10240  Batch Loss: 0.8859  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6171\n",
      "Epoch 001  Batch 4120/10240  Batch Loss: 1.4696  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6171\n",
      "Epoch 001  Batch 4121/10240  Batch Loss: 2.2480  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6171\n",
      "Epoch 001  Batch 4122/10240  Batch Loss: 2.5582  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6171\n",
      "Epoch 001  Batch 4123/10240  Batch Loss: 0.7639  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6171\n",
      "Epoch 001  Batch 4124/10240  Batch Loss: 1.6863  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4125/10240  Batch Loss: 0.7683  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4126/10240  Batch Loss: 1.2969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4127/10240  Batch Loss: 2.1291  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4128/10240  Batch Loss: 2.7044  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4129/10240  Batch Loss: 1.5547  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4130/10240  Batch Loss: 1.2211  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6170\n",
      "Epoch 001  Batch 4131/10240  Batch Loss: 1.6414  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6169\n",
      "Epoch 001  Batch 4132/10240  Batch Loss: 2.5770  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6169\n",
      "Epoch 001  Batch 4133/10240  Batch Loss: 2.1075  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6169\n",
      "Epoch 001  Batch 4134/10240  Batch Loss: 1.9559  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6168\n",
      "Epoch 001  Batch 4135/10240  Batch Loss: 1.4940  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6168\n",
      "Epoch 001  Batch 4136/10240  Batch Loss: 1.5792  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6168\n",
      "Epoch 001  Batch 4137/10240  Batch Loss: 1.4169  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6168\n",
      "Epoch 001  Batch 4138/10240  Batch Loss: 1.3954  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6168\n",
      "Epoch 001  Batch 4139/10240  Batch Loss: 1.5305  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6167\n",
      "Epoch 001  Batch 4140/10240  Batch Loss: 1.1649  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6167\n",
      "Epoch 001  Batch 4141/10240  Batch Loss: 1.1758  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6167\n",
      "Epoch 001  Batch 4142/10240  Batch Loss: 1.8737  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6167\n",
      "Epoch 001  Batch 4143/10240  Batch Loss: 0.7369  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6167\n",
      "Epoch 001  Batch 4144/10240  Batch Loss: 1.0861  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6167\n",
      "Epoch 001  Batch 4145/10240  Batch Loss: 1.4512  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4146/10240  Batch Loss: 2.8695  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4147/10240  Batch Loss: 1.0933  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4148/10240  Batch Loss: 1.7640  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4149/10240  Batch Loss: 1.3114  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4150/10240  Batch Loss: 1.0135  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4151/10240  Batch Loss: 1.2590  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4152/10240  Batch Loss: 1.8650  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6166\n",
      "Epoch 001  Batch 4153/10240  Batch Loss: 3.9986  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4154/10240  Batch Loss: 1.4230  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4155/10240  Batch Loss: 0.8005  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4156/10240  Batch Loss: 1.3860  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4157/10240  Batch Loss: 2.4395  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4158/10240  Batch Loss: 0.9778  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4159/10240  Batch Loss: 0.8556  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4160/10240  Batch Loss: 1.5339  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6165\n",
      "Epoch 001  Batch 4161/10240  Batch Loss: 1.3345  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6164\n",
      "Epoch 001  Batch 4162/10240  Batch Loss: 1.9795  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4163/10240  Batch Loss: 1.2018  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4164/10240  Batch Loss: 1.3375  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4165/10240  Batch Loss: 1.7752  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4166/10240  Batch Loss: 1.5436  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4167/10240  Batch Loss: 1.4533  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4168/10240  Batch Loss: 1.0982  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4169/10240  Batch Loss: 1.1049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6163\n",
      "Epoch 001  Batch 4170/10240  Batch Loss: 1.9184  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4171/10240  Batch Loss: 2.1017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4172/10240  Batch Loss: 1.3218  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4173/10240  Batch Loss: 1.1719  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4174/10240  Batch Loss: 1.2569  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4175/10240  Batch Loss: 1.5441  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4176/10240  Batch Loss: 0.9201  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4177/10240  Batch Loss: 2.5399  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4178/10240  Batch Loss: 1.5112  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4179/10240  Batch Loss: 0.9817  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4180/10240  Batch Loss: 1.4250  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6162\n",
      "Epoch 001  Batch 4181/10240  Batch Loss: 2.5587  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4182/10240  Batch Loss: 1.2511  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4183/10240  Batch Loss: 1.2694  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4184/10240  Batch Loss: 0.8305  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4185/10240  Batch Loss: 1.0629  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4186/10240  Batch Loss: 1.8347  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6161\n",
      "Epoch 001  Batch 4187/10240  Batch Loss: 2.9589  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6160\n",
      "Epoch 001  Batch 4188/10240  Batch Loss: 2.3646  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4189/10240  Batch Loss: 1.2953  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4190/10240  Batch Loss: 0.9579  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4191/10240  Batch Loss: 0.7267  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4192/10240  Batch Loss: 1.0705  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4193/10240  Batch Loss: 1.8295  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4194/10240  Batch Loss: 0.9273  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6160\n",
      "Epoch 001  Batch 4195/10240  Batch Loss: 0.9242  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4196/10240  Batch Loss: 1.1082  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4197/10240  Batch Loss: 0.9740  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4198/10240  Batch Loss: 1.6131  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4199/10240  Batch Loss: 0.7261  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4200/10240  Batch Loss: 1.3353  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6159\n",
      "Epoch 001  Batch 4201/10240  Batch Loss: 1.7817  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6158\n",
      "Epoch 001  Batch 4202/10240  Batch Loss: 2.3150  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6157\n",
      "Epoch 001  Batch 4203/10240  Batch Loss: 1.4427  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6157\n",
      "Epoch 001  Batch 4204/10240  Batch Loss: 1.3544  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6157\n",
      "Epoch 001  Batch 4205/10240  Batch Loss: 1.6206  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6156\n",
      "Epoch 001  Batch 4206/10240  Batch Loss: 2.3017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6156\n",
      "Epoch 001  Batch 4207/10240  Batch Loss: 1.4987  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6156\n",
      "Epoch 001  Batch 4208/10240  Batch Loss: 0.9022  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6156\n",
      "Epoch 001  Batch 4209/10240  Batch Loss: 2.3110  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6155\n",
      "Epoch 001  Batch 4210/10240  Batch Loss: 2.0521  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6154\n",
      "Epoch 001  Batch 4211/10240  Batch Loss: 0.7316  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6154\n",
      "Epoch 001  Batch 4212/10240  Batch Loss: 1.5837  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6154\n",
      "Epoch 001  Batch 4213/10240  Batch Loss: 1.6743  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6154\n",
      "Epoch 001  Batch 4214/10240  Batch Loss: 1.5490  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6153\n",
      "Epoch 001  Batch 4215/10240  Batch Loss: 0.9585  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6153\n",
      "Epoch 001  Batch 4216/10240  Batch Loss: 1.3139  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6153\n",
      "Epoch 001  Batch 4217/10240  Batch Loss: 2.1356  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6153\n",
      "Epoch 001  Batch 4218/10240  Batch Loss: 1.8925  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6153\n",
      "Epoch 001  Batch 4219/10240  Batch Loss: 4.5331  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4220/10240  Batch Loss: 0.7909  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4221/10240  Batch Loss: 1.3794  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4222/10240  Batch Loss: 0.7290  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4223/10240  Batch Loss: 1.2725  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4224/10240  Batch Loss: 1.7889  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4225/10240  Batch Loss: 0.8870  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4226/10240  Batch Loss: 0.8945  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4227/10240  Batch Loss: 1.0046  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4228/10240  Batch Loss: 1.4940  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6150\n",
      "Epoch 001  Batch 4229/10240  Batch Loss: 1.1409  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4230/10240  Batch Loss: 0.7700  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4231/10240  Batch Loss: 1.2129  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4232/10240  Batch Loss: 1.8198  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4233/10240  Batch Loss: 1.1524  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4234/10240  Batch Loss: 1.7962  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4235/10240  Batch Loss: 1.3741  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4236/10240  Batch Loss: 0.7285  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4237/10240  Batch Loss: 0.7602  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4238/10240  Batch Loss: 0.7138  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6149\n",
      "Epoch 001  Batch 4239/10240  Batch Loss: 1.0712  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4240/10240  Batch Loss: 2.2166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4241/10240  Batch Loss: 1.2355  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4242/10240  Batch Loss: 0.9484  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4243/10240  Batch Loss: 1.5724  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4244/10240  Batch Loss: 0.7166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4245/10240  Batch Loss: 1.0315  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6148\n",
      "Epoch 001  Batch 4246/10240  Batch Loss: 2.0143  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6147\n",
      "Epoch 001  Batch 4247/10240  Batch Loss: 1.3613  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6147\n",
      "Epoch 001  Batch 4248/10240  Batch Loss: 1.8738  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6147\n",
      "Epoch 001  Batch 4249/10240  Batch Loss: 1.2287  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6147\n",
      "Epoch 001  Batch 4250/10240  Batch Loss: 1.8386  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4251/10240  Batch Loss: 1.3199  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4252/10240  Batch Loss: 0.7286  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4253/10240  Batch Loss: 1.8292  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4254/10240  Batch Loss: 1.7335  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4255/10240  Batch Loss: 2.3633  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4256/10240  Batch Loss: 0.7515  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4257/10240  Batch Loss: 0.7515  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4258/10240  Batch Loss: 0.7290  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6146\n",
      "Epoch 001  Batch 4259/10240  Batch Loss: 1.0117  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6145\n",
      "Epoch 001  Batch 4260/10240  Batch Loss: 1.4680  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6145\n",
      "Epoch 001  Batch 4261/10240  Batch Loss: 1.1442  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6145\n",
      "Epoch 001  Batch 4262/10240  Batch Loss: 1.7178  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4263/10240  Batch Loss: 1.0241  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4264/10240  Batch Loss: 0.8901  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4265/10240  Batch Loss: 1.1814  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4266/10240  Batch Loss: 0.7310  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4267/10240  Batch Loss: 1.1170  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4268/10240  Batch Loss: 1.8087  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4269/10240  Batch Loss: 1.3323  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4270/10240  Batch Loss: 1.1025  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6144\n",
      "Epoch 001  Batch 4271/10240  Batch Loss: 1.9639  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6143\n",
      "Epoch 001  Batch 4272/10240  Batch Loss: 1.2503  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6143\n",
      "Epoch 001  Batch 4273/10240  Batch Loss: 0.7948  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6143\n",
      "Epoch 001  Batch 4274/10240  Batch Loss: 1.3766  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6143\n",
      "Epoch 001  Batch 4275/10240  Batch Loss: 1.3582  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6143\n",
      "Epoch 001  Batch 4276/10240  Batch Loss: 1.5469  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6142\n",
      "Epoch 001  Batch 4277/10240  Batch Loss: 1.6900  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6141\n",
      "Epoch 001  Batch 4278/10240  Batch Loss: 0.9112  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6141\n",
      "Epoch 001  Batch 4279/10240  Batch Loss: 1.7447  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6140\n",
      "Epoch 001  Batch 4280/10240  Batch Loss: 0.7139  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6140\n",
      "Epoch 001  Batch 4281/10240  Batch Loss: 1.3381  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6140\n",
      "Epoch 001  Batch 4282/10240  Batch Loss: 1.7103  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6139\n",
      "Epoch 001  Batch 4283/10240  Batch Loss: 2.0298  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6139\n",
      "Epoch 001  Batch 4284/10240  Batch Loss: 2.0371  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4285/10240  Batch Loss: 1.0313  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4286/10240  Batch Loss: 1.6640  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4287/10240  Batch Loss: 1.5890  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4288/10240  Batch Loss: 1.7809  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4289/10240  Batch Loss: 0.9349  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4290/10240  Batch Loss: 1.0076  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4291/10240  Batch Loss: 0.9939  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4292/10240  Batch Loss: 0.7459  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4293/10240  Batch Loss: 0.9445  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6138\n",
      "Epoch 001  Batch 4294/10240  Batch Loss: 1.3868  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6137\n",
      "Epoch 001  Batch 4295/10240  Batch Loss: 0.9891  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6137\n",
      "Epoch 001  Batch 4296/10240  Batch Loss: 0.7228  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6137\n",
      "Epoch 001  Batch 4297/10240  Batch Loss: 1.3247  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6137\n",
      "Epoch 001  Batch 4298/10240  Batch Loss: 2.1742  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6137\n",
      "Epoch 001  Batch 4299/10240  Batch Loss: 2.5572  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6136\n",
      "Epoch 001  Batch 4300/10240  Batch Loss: 1.5608  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6135\n",
      "Epoch 001  Batch 4301/10240  Batch Loss: 2.5691  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6134\n",
      "Epoch 001  Batch 4302/10240  Batch Loss: 1.7627  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6134\n",
      "Epoch 001  Batch 4303/10240  Batch Loss: 2.3483  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6133\n",
      "Epoch 001  Batch 4304/10240  Batch Loss: 1.1296  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6134\n",
      "Epoch 001  Batch 4305/10240  Batch Loss: 1.9152  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6133\n",
      "Epoch 001  Batch 4306/10240  Batch Loss: 2.7547  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6132\n",
      "Epoch 001  Batch 4307/10240  Batch Loss: 1.4641  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6132\n",
      "Epoch 001  Batch 4308/10240  Batch Loss: 0.8961  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6132\n",
      "Epoch 001  Batch 4309/10240  Batch Loss: 1.4394  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6131\n",
      "Epoch 001  Batch 4310/10240  Batch Loss: 1.1675  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6131\n",
      "Epoch 001  Batch 4311/10240  Batch Loss: 1.1872  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6131\n",
      "Epoch 001  Batch 4312/10240  Batch Loss: 1.2969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6131\n",
      "Epoch 001  Batch 4313/10240  Batch Loss: 1.3769  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6131\n",
      "Epoch 001  Batch 4314/10240  Batch Loss: 1.3185  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6131\n",
      "Epoch 001  Batch 4315/10240  Batch Loss: 1.2093  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6130\n",
      "Epoch 001  Batch 4316/10240  Batch Loss: 0.7327  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6130\n",
      "Epoch 001  Batch 4317/10240  Batch Loss: 1.7258  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6130\n",
      "Epoch 001  Batch 4318/10240  Batch Loss: 1.2740  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6130\n",
      "Epoch 001  Batch 4319/10240  Batch Loss: 1.0397  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6130\n",
      "Epoch 001  Batch 4320/10240  Batch Loss: 1.3951  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6130\n",
      "Epoch 001  Batch 4321/10240  Batch Loss: 0.9401  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6129\n",
      "Epoch 001  Batch 4322/10240  Batch Loss: 1.7460  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6129\n",
      "Epoch 001  Batch 4323/10240  Batch Loss: 1.6231  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6129\n",
      "Epoch 001  Batch 4324/10240  Batch Loss: 1.5709  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6128\n",
      "Epoch 001  Batch 4325/10240  Batch Loss: 1.1295  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6128\n",
      "Epoch 001  Batch 4326/10240  Batch Loss: 1.5017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6128\n",
      "Epoch 001  Batch 4327/10240  Batch Loss: 1.4881  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6128\n",
      "Epoch 001  Batch 4328/10240  Batch Loss: 1.0779  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6127\n",
      "Epoch 001  Batch 4329/10240  Batch Loss: 1.4175  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6127\n",
      "Epoch 001  Batch 4330/10240  Batch Loss: 1.5163  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4331/10240  Batch Loss: 1.1321  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4332/10240  Batch Loss: 1.0037  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4333/10240  Batch Loss: 1.2405  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4334/10240  Batch Loss: 0.7785  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4335/10240  Batch Loss: 1.1086  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4336/10240  Batch Loss: 0.9106  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6126\n",
      "Epoch 001  Batch 4337/10240  Batch Loss: 1.2291  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6125\n",
      "Epoch 001  Batch 4338/10240  Batch Loss: 1.1542  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6125\n",
      "Epoch 001  Batch 4339/10240  Batch Loss: 2.4959  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6125\n",
      "Epoch 001  Batch 4340/10240  Batch Loss: 1.9768  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6124\n",
      "Epoch 001  Batch 4341/10240  Batch Loss: 2.4922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6124\n",
      "Epoch 001  Batch 4342/10240  Batch Loss: 1.5795  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6124\n",
      "Epoch 001  Batch 4343/10240  Batch Loss: 0.9779  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6124\n",
      "Epoch 001  Batch 4344/10240  Batch Loss: 2.2666  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6123\n",
      "Epoch 001  Batch 4345/10240  Batch Loss: 1.9910  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6123\n",
      "Epoch 001  Batch 4346/10240  Batch Loss: 2.8772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6123\n",
      "Epoch 001  Batch 4347/10240  Batch Loss: 1.7977  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4348/10240  Batch Loss: 0.8885  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4349/10240  Batch Loss: 1.4882  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4350/10240  Batch Loss: 0.8616  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4351/10240  Batch Loss: 1.0972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4352/10240  Batch Loss: 1.5210  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4353/10240  Batch Loss: 1.7965  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4354/10240  Batch Loss: 1.0496  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4355/10240  Batch Loss: 1.8917  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4356/10240  Batch Loss: 0.7886  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4357/10240  Batch Loss: 1.5260  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4358/10240  Batch Loss: 1.9282  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6122\n",
      "Epoch 001  Batch 4359/10240  Batch Loss: 0.8228  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4360/10240  Batch Loss: 1.3566  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4361/10240  Batch Loss: 0.8068  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4362/10240  Batch Loss: 1.4963  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4363/10240  Batch Loss: 1.3976  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4364/10240  Batch Loss: 1.3782  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4365/10240  Batch Loss: 0.7335  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4366/10240  Batch Loss: 1.1221  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4367/10240  Batch Loss: 0.9187  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4368/10240  Batch Loss: 1.3856  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6120\n",
      "Epoch 001  Batch 4369/10240  Batch Loss: 1.6360  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4370/10240  Batch Loss: 1.4962  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4371/10240  Batch Loss: 1.1366  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6121\n",
      "Epoch 001  Batch 4372/10240  Batch Loss: 2.3676  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6120\n",
      "Epoch 001  Batch 4373/10240  Batch Loss: 1.7739  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6120\n",
      "Epoch 001  Batch 4374/10240  Batch Loss: 2.8190  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6119\n",
      "Epoch 001  Batch 4375/10240  Batch Loss: 1.3654  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6119\n",
      "Epoch 001  Batch 4376/10240  Batch Loss: 1.2002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6119\n",
      "Epoch 001  Batch 4377/10240  Batch Loss: 0.7440  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6119\n",
      "Epoch 001  Batch 4378/10240  Batch Loss: 2.1768  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6118\n",
      "Epoch 001  Batch 4379/10240  Batch Loss: 1.3629  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6117\n",
      "Epoch 001  Batch 4380/10240  Batch Loss: 0.7657  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6117\n",
      "Epoch 001  Batch 4381/10240  Batch Loss: 1.2432  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6117\n",
      "Epoch 001  Batch 4382/10240  Batch Loss: 1.1540  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6117\n",
      "Epoch 001  Batch 4383/10240  Batch Loss: 1.0189  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6117\n",
      "Epoch 001  Batch 4384/10240  Batch Loss: 1.6494  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6117\n",
      "Epoch 001  Batch 4385/10240  Batch Loss: 1.5948  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6116\n",
      "Epoch 001  Batch 4386/10240  Batch Loss: 0.8715  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6116\n",
      "Epoch 001  Batch 4387/10240  Batch Loss: 0.9715  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6116\n",
      "Epoch 001  Batch 4388/10240  Batch Loss: 1.3132  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6116\n",
      "Epoch 001  Batch 4389/10240  Batch Loss: 0.7266  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6116\n",
      "Epoch 001  Batch 4390/10240  Batch Loss: 1.0857  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6116\n",
      "Epoch 001  Batch 4391/10240  Batch Loss: 1.7103  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4392/10240  Batch Loss: 1.1300  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4393/10240  Batch Loss: 1.7820  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4394/10240  Batch Loss: 2.2121  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4395/10240  Batch Loss: 1.3558  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4396/10240  Batch Loss: 1.6411  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4397/10240  Batch Loss: 1.6574  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4398/10240  Batch Loss: 1.5787  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6114\n",
      "Epoch 001  Batch 4399/10240  Batch Loss: 1.4187  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6114\n",
      "Epoch 001  Batch 4400/10240  Batch Loss: 1.0044  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4401/10240  Batch Loss: 0.9172  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4402/10240  Batch Loss: 0.9681  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6115\n",
      "Epoch 001  Batch 4403/10240  Batch Loss: 1.3305  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6114\n",
      "Epoch 001  Batch 4404/10240  Batch Loss: 1.6326  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6114\n",
      "Epoch 001  Batch 4405/10240  Batch Loss: 1.5324  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6114\n",
      "Epoch 001  Batch 4406/10240  Batch Loss: 1.4629  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6114\n",
      "Epoch 001  Batch 4407/10240  Batch Loss: 2.7186  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6113\n",
      "Epoch 001  Batch 4408/10240  Batch Loss: 1.6676  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6113\n",
      "Epoch 001  Batch 4409/10240  Batch Loss: 1.2804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6113\n",
      "Epoch 001  Batch 4410/10240  Batch Loss: 1.0663  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6113\n",
      "Epoch 001  Batch 4411/10240  Batch Loss: 1.1333  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4412/10240  Batch Loss: 1.1112  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4413/10240  Batch Loss: 0.9912  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4414/10240  Batch Loss: 1.8853  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6113\n",
      "Epoch 001  Batch 4415/10240  Batch Loss: 1.6183  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4416/10240  Batch Loss: 1.4652  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4417/10240  Batch Loss: 1.4284  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4418/10240  Batch Loss: 1.2894  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4419/10240  Batch Loss: 0.7198  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4420/10240  Batch Loss: 0.7112  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6112\n",
      "Epoch 001  Batch 4421/10240  Batch Loss: 2.7212  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4422/10240  Batch Loss: 1.0532  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4423/10240  Batch Loss: 1.3282  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4424/10240  Batch Loss: 1.1888  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4425/10240  Batch Loss: 1.8787  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4426/10240  Batch Loss: 1.5892  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6111\n",
      "Epoch 001  Batch 4427/10240  Batch Loss: 2.0941  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6111\n",
      "Epoch 001  Batch 4428/10240  Batch Loss: 1.4105  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6111\n",
      "Epoch 001  Batch 4429/10240  Batch Loss: 2.0374  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4430/10240  Batch Loss: 1.4553  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4431/10240  Batch Loss: 1.2053  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4432/10240  Batch Loss: 0.7161  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4433/10240  Batch Loss: 2.3677  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4434/10240  Batch Loss: 1.8371  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4435/10240  Batch Loss: 0.9632  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6110\n",
      "Epoch 001  Batch 4436/10240  Batch Loss: 1.9320  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6109\n",
      "Epoch 001  Batch 4437/10240  Batch Loss: 1.6723  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6109\n",
      "Epoch 001  Batch 4438/10240  Batch Loss: 2.1261  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6109\n",
      "Epoch 001  Batch 4439/10240  Batch Loss: 1.7126  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6108\n",
      "Epoch 001  Batch 4440/10240  Batch Loss: 3.8847  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6108\n",
      "Epoch 001  Batch 4441/10240  Batch Loss: 1.3221  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6107\n",
      "Epoch 001  Batch 4442/10240  Batch Loss: 1.6640  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6107\n",
      "Epoch 001  Batch 4443/10240  Batch Loss: 1.3366  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6108\n",
      "Epoch 001  Batch 4444/10240  Batch Loss: 2.2591  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6107\n",
      "Epoch 001  Batch 4445/10240  Batch Loss: 1.9415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6107\n",
      "Epoch 001  Batch 4446/10240  Batch Loss: 0.8335  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6107\n",
      "Epoch 001  Batch 4447/10240  Batch Loss: 1.5561  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6106\n",
      "Epoch 001  Batch 4448/10240  Batch Loss: 1.8166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6106\n",
      "Epoch 001  Batch 4449/10240  Batch Loss: 0.7983  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6106\n",
      "Epoch 001  Batch 4450/10240  Batch Loss: 2.0437  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6106\n",
      "Epoch 001  Batch 4451/10240  Batch Loss: 2.7081  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6105\n",
      "Epoch 001  Batch 4452/10240  Batch Loss: 1.7019  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6104\n",
      "Epoch 001  Batch 4453/10240  Batch Loss: 1.6412  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6104\n",
      "Epoch 001  Batch 4454/10240  Batch Loss: 2.5197  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6103\n",
      "Epoch 001  Batch 4455/10240  Batch Loss: 1.8382  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6102\n",
      "Epoch 001  Batch 4456/10240  Batch Loss: 0.9160  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6102\n",
      "Epoch 001  Batch 4457/10240  Batch Loss: 2.3339  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6101\n",
      "Epoch 001  Batch 4458/10240  Batch Loss: 0.7324  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6101\n",
      "Epoch 001  Batch 4459/10240  Batch Loss: 1.2652  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6101\n",
      "Epoch 001  Batch 4460/10240  Batch Loss: 0.7893  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6101\n",
      "Epoch 001  Batch 4461/10240  Batch Loss: 1.5269  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6101\n",
      "Epoch 001  Batch 4462/10240  Batch Loss: 0.9868  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6101\n",
      "Epoch 001  Batch 4463/10240  Batch Loss: 2.8676  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4464/10240  Batch Loss: 1.2694  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6100\n",
      "Epoch 001  Batch 4465/10240  Batch Loss: 1.1284  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4466/10240  Batch Loss: 1.1412  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4467/10240  Batch Loss: 1.2892  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4468/10240  Batch Loss: 1.8755  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6100\n",
      "Epoch 001  Batch 4469/10240  Batch Loss: 1.4525  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4470/10240  Batch Loss: 0.9716  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4471/10240  Batch Loss: 2.1765  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6098\n",
      "Epoch 001  Batch 4472/10240  Batch Loss: 1.1547  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6099\n",
      "Epoch 001  Batch 4473/10240  Batch Loss: 1.2356  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6098\n",
      "Epoch 001  Batch 4474/10240  Batch Loss: 1.4564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6098\n",
      "Epoch 001  Batch 4475/10240  Batch Loss: 0.7384  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6098\n",
      "Epoch 001  Batch 4476/10240  Batch Loss: 0.7325  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6098\n",
      "Epoch 001  Batch 4477/10240  Batch Loss: 0.9710  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6098\n",
      "Epoch 001  Batch 4478/10240  Batch Loss: 2.0302  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6097\n",
      "Epoch 001  Batch 4479/10240  Batch Loss: 0.7214  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6097\n",
      "Epoch 001  Batch 4480/10240  Batch Loss: 1.0219  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6097\n",
      "Epoch 001  Batch 4481/10240  Batch Loss: 1.1179  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6097\n",
      "Epoch 001  Batch 4482/10240  Batch Loss: 0.9940  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6097\n",
      "Epoch 001  Batch 4483/10240  Batch Loss: 1.7414  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6096\n",
      "Epoch 001  Batch 4484/10240  Batch Loss: 1.4032  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6096\n",
      "Epoch 001  Batch 4485/10240  Batch Loss: 0.7330  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6096\n",
      "Epoch 001  Batch 4486/10240  Batch Loss: 1.6201  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6096\n",
      "Epoch 001  Batch 4487/10240  Batch Loss: 1.7467  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6095\n",
      "Epoch 001  Batch 4488/10240  Batch Loss: 2.4666  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6095\n",
      "Epoch 001  Batch 4489/10240  Batch Loss: 1.4981  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6095\n",
      "Epoch 001  Batch 4490/10240  Batch Loss: 2.9985  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6094\n",
      "Epoch 001  Batch 4491/10240  Batch Loss: 1.3585  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6094\n",
      "Epoch 001  Batch 4492/10240  Batch Loss: 1.4393  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6094\n",
      "Epoch 001  Batch 4493/10240  Batch Loss: 2.0806  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6094\n",
      "Epoch 001  Batch 4494/10240  Batch Loss: 1.0848  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6094\n",
      "Epoch 001  Batch 4495/10240  Batch Loss: 1.0605  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4496/10240  Batch Loss: 1.5539  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4497/10240  Batch Loss: 0.8236  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4498/10240  Batch Loss: 0.8037  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4499/10240  Batch Loss: 0.7487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4500/10240  Batch Loss: 1.0803  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4501/10240  Batch Loss: 1.1393  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4502/10240  Batch Loss: 1.3225  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4503/10240  Batch Loss: 0.8378  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6093\n",
      "Epoch 001  Batch 4504/10240  Batch Loss: 2.0917  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4505/10240  Batch Loss: 0.8714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4506/10240  Batch Loss: 1.2548  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4507/10240  Batch Loss: 1.4922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4508/10240  Batch Loss: 1.2919  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6091\n",
      "Epoch 001  Batch 4509/10240  Batch Loss: 1.4187  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6091\n",
      "Epoch 001  Batch 4510/10240  Batch Loss: 1.0166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4511/10240  Batch Loss: 1.5052  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4512/10240  Batch Loss: 1.6515  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4513/10240  Batch Loss: 1.7547  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4514/10240  Batch Loss: 1.0946  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6092\n",
      "Epoch 001  Batch 4515/10240  Batch Loss: 1.7253  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6091\n",
      "Epoch 001  Batch 4516/10240  Batch Loss: 1.2371  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6091\n",
      "Epoch 001  Batch 4517/10240  Batch Loss: 2.0408  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4518/10240  Batch Loss: 1.3211  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4519/10240  Batch Loss: 1.3670  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4520/10240  Batch Loss: 0.9699  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4521/10240  Batch Loss: 0.8770  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4522/10240  Batch Loss: 0.7267  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4523/10240  Batch Loss: 1.3415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4524/10240  Batch Loss: 0.8690  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4525/10240  Batch Loss: 1.5968  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6090\n",
      "Epoch 001  Batch 4526/10240  Batch Loss: 1.8750  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6089\n",
      "Epoch 001  Batch 4527/10240  Batch Loss: 0.7256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6089\n",
      "Epoch 001  Batch 4528/10240  Batch Loss: 1.9368  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4529/10240  Batch Loss: 1.9115  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4530/10240  Batch Loss: 0.9387  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4531/10240  Batch Loss: 1.3693  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4532/10240  Batch Loss: 1.7703  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4533/10240  Batch Loss: 0.8709  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4534/10240  Batch Loss: 1.3672  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6088\n",
      "Epoch 001  Batch 4535/10240  Batch Loss: 2.1192  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6087\n",
      "Epoch 001  Batch 4536/10240  Batch Loss: 1.2905  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6087\n",
      "Epoch 001  Batch 4537/10240  Batch Loss: 1.7467  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4538/10240  Batch Loss: 1.1091  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4539/10240  Batch Loss: 1.5142  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6087\n",
      "Epoch 001  Batch 4540/10240  Batch Loss: 2.2203  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4541/10240  Batch Loss: 2.0726  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4542/10240  Batch Loss: 1.2028  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4543/10240  Batch Loss: 1.2183  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4544/10240  Batch Loss: 2.1259  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4545/10240  Batch Loss: 1.0995  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6086\n",
      "Epoch 001  Batch 4546/10240  Batch Loss: 1.5060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6085\n",
      "Epoch 001  Batch 4547/10240  Batch Loss: 2.6822  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6084\n",
      "Epoch 001  Batch 4548/10240  Batch Loss: 1.0125  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6084\n",
      "Epoch 001  Batch 4549/10240  Batch Loss: 0.9510  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6084\n",
      "Epoch 001  Batch 4550/10240  Batch Loss: 2.7315  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6084\n",
      "Epoch 001  Batch 4551/10240  Batch Loss: 1.6905  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6083\n",
      "Epoch 001  Batch 4552/10240  Batch Loss: 0.7392  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6083\n",
      "Epoch 001  Batch 4553/10240  Batch Loss: 2.4384  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6083\n",
      "Epoch 001  Batch 4554/10240  Batch Loss: 1.1787  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6083\n",
      "Epoch 001  Batch 4555/10240  Batch Loss: 0.7819  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6083\n",
      "Epoch 001  Batch 4556/10240  Batch Loss: 1.6171  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6083\n",
      "Epoch 001  Batch 4557/10240  Batch Loss: 2.4533  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6082\n",
      "Epoch 001  Batch 4558/10240  Batch Loss: 2.0686  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6081\n",
      "Epoch 001  Batch 4559/10240  Batch Loss: 1.6127  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6081\n",
      "Epoch 001  Batch 4560/10240  Batch Loss: 1.5443  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6081\n",
      "Epoch 001  Batch 4561/10240  Batch Loss: 1.3955  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6081\n",
      "Epoch 001  Batch 4562/10240  Batch Loss: 0.9080  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6081\n",
      "Epoch 001  Batch 4563/10240  Batch Loss: 2.1724  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6080\n",
      "Epoch 001  Batch 4564/10240  Batch Loss: 0.8229  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6080\n",
      "Epoch 001  Batch 4565/10240  Batch Loss: 0.7213  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6080\n",
      "Epoch 001  Batch 4566/10240  Batch Loss: 0.7243  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6080\n",
      "Epoch 001  Batch 4567/10240  Batch Loss: 0.9618  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6080\n",
      "Epoch 001  Batch 4568/10240  Batch Loss: 1.7234  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6080\n",
      "Epoch 001  Batch 4569/10240  Batch Loss: 1.6324  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4570/10240  Batch Loss: 1.5488  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4571/10240  Batch Loss: 2.1505  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4572/10240  Batch Loss: 1.9031  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4573/10240  Batch Loss: 0.8772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4574/10240  Batch Loss: 0.9635  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4575/10240  Batch Loss: 0.9727  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4576/10240  Batch Loss: 1.0512  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4577/10240  Batch Loss: 1.0740  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4578/10240  Batch Loss: 1.8646  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4579/10240  Batch Loss: 2.1410  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4580/10240  Batch Loss: 0.9487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4581/10240  Batch Loss: 1.1072  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6079\n",
      "Epoch 001  Batch 4582/10240  Batch Loss: 1.3195  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6078\n",
      "Epoch 001  Batch 4583/10240  Batch Loss: 0.7330  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6078\n",
      "Epoch 001  Batch 4584/10240  Batch Loss: 1.0840  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6078\n",
      "Epoch 001  Batch 4585/10240  Batch Loss: 1.9743  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6078\n",
      "Epoch 001  Batch 4586/10240  Batch Loss: 1.0275  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6078\n",
      "Epoch 001  Batch 4587/10240  Batch Loss: 0.9526  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6078\n",
      "Epoch 001  Batch 4588/10240  Batch Loss: 1.8216  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6077\n",
      "Epoch 001  Batch 4589/10240  Batch Loss: 1.5852  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6077\n",
      "Epoch 001  Batch 4590/10240  Batch Loss: 2.3030  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6077\n",
      "Epoch 001  Batch 4591/10240  Batch Loss: 2.8855  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6077\n",
      "Epoch 001  Batch 4592/10240  Batch Loss: 1.5279  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4593/10240  Batch Loss: 1.0324  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4594/10240  Batch Loss: 0.8835  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4595/10240  Batch Loss: 1.0989  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4596/10240  Batch Loss: 2.1274  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4597/10240  Batch Loss: 0.7230  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4598/10240  Batch Loss: 1.1483  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6076\n",
      "Epoch 001  Batch 4599/10240  Batch Loss: 1.8470  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4600/10240  Batch Loss: 1.2489  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4601/10240  Batch Loss: 0.8483  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4602/10240  Batch Loss: 2.0449  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4603/10240  Batch Loss: 0.7172  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4604/10240  Batch Loss: 1.7104  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4605/10240  Batch Loss: 1.0176  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4606/10240  Batch Loss: 1.2212  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4607/10240  Batch Loss: 1.0713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4608/10240  Batch Loss: 1.1532  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4609/10240  Batch Loss: 1.3597  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4610/10240  Batch Loss: 1.2161  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6075\n",
      "Epoch 001  Batch 4611/10240  Batch Loss: 2.6879  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6074\n",
      "Epoch 001  Batch 4612/10240  Batch Loss: 0.7194  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6074\n",
      "Epoch 001  Batch 4613/10240  Batch Loss: 1.7893  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6074\n",
      "Epoch 001  Batch 4614/10240  Batch Loss: 1.0491  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6074\n",
      "Epoch 001  Batch 4615/10240  Batch Loss: 0.9233  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6074\n",
      "Epoch 001  Batch 4616/10240  Batch Loss: 1.6338  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6073\n",
      "Epoch 001  Batch 4617/10240  Batch Loss: 1.4021  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6073\n",
      "Epoch 001  Batch 4618/10240  Batch Loss: 1.7164  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6073\n",
      "Epoch 001  Batch 4619/10240  Batch Loss: 2.2298  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6072\n",
      "Epoch 001  Batch 4620/10240  Batch Loss: 1.2144  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6072\n",
      "Epoch 001  Batch 4621/10240  Batch Loss: 1.3977  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6072\n",
      "Epoch 001  Batch 4622/10240  Batch Loss: 1.1845  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6072\n",
      "Epoch 001  Batch 4623/10240  Batch Loss: 1.9081  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6071\n",
      "Epoch 001  Batch 4624/10240  Batch Loss: 0.7199  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6071\n",
      "Epoch 001  Batch 4625/10240  Batch Loss: 0.7234  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6071\n",
      "Epoch 001  Batch 4626/10240  Batch Loss: 1.0783  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6071\n",
      "Epoch 001  Batch 4627/10240  Batch Loss: 1.2754  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6071\n",
      "Epoch 001  Batch 4628/10240  Batch Loss: 1.3965  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6071\n",
      "Epoch 001  Batch 4629/10240  Batch Loss: 0.9273  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6070\n",
      "Epoch 001  Batch 4630/10240  Batch Loss: 0.9010  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6070\n",
      "Epoch 001  Batch 4631/10240  Batch Loss: 1.0270  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6070\n",
      "Epoch 001  Batch 4632/10240  Batch Loss: 0.7504  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6070\n",
      "Epoch 001  Batch 4633/10240  Batch Loss: 2.6236  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4634/10240  Batch Loss: 0.9884  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4635/10240  Batch Loss: 1.2566  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4636/10240  Batch Loss: 0.7228  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4637/10240  Batch Loss: 1.3912  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4638/10240  Batch Loss: 1.3332  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4639/10240  Batch Loss: 1.6277  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4640/10240  Batch Loss: 1.4012  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4641/10240  Batch Loss: 1.0828  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4642/10240  Batch Loss: 0.9993  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4643/10240  Batch Loss: 0.9856  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4644/10240  Batch Loss: 0.7643  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4645/10240  Batch Loss: 2.0260  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6070\n",
      "Epoch 001  Batch 4646/10240  Batch Loss: 1.8713  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4647/10240  Batch Loss: 1.5468  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4648/10240  Batch Loss: 1.2845  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4649/10240  Batch Loss: 1.6087  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4650/10240  Batch Loss: 1.1157  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4651/10240  Batch Loss: 0.9313  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6069\n",
      "Epoch 001  Batch 4652/10240  Batch Loss: 1.5593  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6068\n",
      "Epoch 001  Batch 4653/10240  Batch Loss: 2.2880  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6067\n",
      "Epoch 001  Batch 4654/10240  Batch Loss: 2.3229  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6068\n",
      "Epoch 001  Batch 4655/10240  Batch Loss: 1.5704  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6068\n",
      "Epoch 001  Batch 4656/10240  Batch Loss: 0.7390  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6068\n",
      "Epoch 001  Batch 4657/10240  Batch Loss: 1.0012  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6068\n",
      "Epoch 001  Batch 4658/10240  Batch Loss: 1.2607  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6067\n",
      "Epoch 001  Batch 4659/10240  Batch Loss: 1.6610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6067\n",
      "Epoch 001  Batch 4660/10240  Batch Loss: 1.2223  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6066\n",
      "Epoch 001  Batch 4661/10240  Batch Loss: 1.9726  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6066\n",
      "Epoch 001  Batch 4662/10240  Batch Loss: 1.2260  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4663/10240  Batch Loss: 0.7190  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4664/10240  Batch Loss: 1.3548  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6066\n",
      "Epoch 001  Batch 4665/10240  Batch Loss: 1.2002  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4666/10240  Batch Loss: 2.1607  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4667/10240  Batch Loss: 1.5256  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4668/10240  Batch Loss: 0.8553  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4669/10240  Batch Loss: 1.2804  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4670/10240  Batch Loss: 1.0926  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4671/10240  Batch Loss: 0.7122  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4672/10240  Batch Loss: 0.7347  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4673/10240  Batch Loss: 1.4223  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4674/10240  Batch Loss: 2.9029  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6065\n",
      "Epoch 001  Batch 4675/10240  Batch Loss: 2.6010  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6064\n",
      "Epoch 001  Batch 4676/10240  Batch Loss: 1.0139  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6064\n",
      "Epoch 001  Batch 4677/10240  Batch Loss: 3.0350  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4678/10240  Batch Loss: 0.7686  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4679/10240  Batch Loss: 1.1943  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6063\n",
      "Epoch 001  Batch 4680/10240  Batch Loss: 0.7164  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6063\n",
      "Epoch 001  Batch 4681/10240  Batch Loss: 2.8786  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4682/10240  Batch Loss: 0.8421  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4683/10240  Batch Loss: 1.2605  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4684/10240  Batch Loss: 1.7896  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4685/10240  Batch Loss: 1.0017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6062\n",
      "Epoch 001  Batch 4686/10240  Batch Loss: 1.6393  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4687/10240  Batch Loss: 0.7167  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4688/10240  Batch Loss: 1.1409  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4689/10240  Batch Loss: 0.7272  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4690/10240  Batch Loss: 1.4172  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4691/10240  Batch Loss: 1.5114  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4692/10240  Batch Loss: 1.5380  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4693/10240  Batch Loss: 1.1833  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4694/10240  Batch Loss: 1.0922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4695/10240  Batch Loss: 2.3329  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4696/10240  Batch Loss: 1.0808  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4697/10240  Batch Loss: 0.7484  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4698/10240  Batch Loss: 1.4294  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4699/10240  Batch Loss: 1.6311  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4700/10240  Batch Loss: 0.7185  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4701/10240  Batch Loss: 1.2288  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4702/10240  Batch Loss: 0.7218  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4703/10240  Batch Loss: 1.5112  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6061\n",
      "Epoch 001  Batch 4704/10240  Batch Loss: 1.5439  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6060\n",
      "Epoch 001  Batch 4705/10240  Batch Loss: 0.7429  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6060\n",
      "Epoch 001  Batch 4706/10240  Batch Loss: 1.3369  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6060\n",
      "Epoch 001  Batch 4707/10240  Batch Loss: 1.4092  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6059\n",
      "Epoch 001  Batch 4708/10240  Batch Loss: 4.4884  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6058\n",
      "Epoch 001  Batch 4709/10240  Batch Loss: 1.7128  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6057\n",
      "Epoch 001  Batch 4710/10240  Batch Loss: 0.9975  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6058\n",
      "Epoch 001  Batch 4711/10240  Batch Loss: 0.7064  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6058\n",
      "Epoch 001  Batch 4712/10240  Batch Loss: 1.4030  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6058\n",
      "Epoch 001  Batch 4713/10240  Batch Loss: 1.1180  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6058\n",
      "Epoch 001  Batch 4714/10240  Batch Loss: 1.2351  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6057\n",
      "Epoch 001  Batch 4715/10240  Batch Loss: 1.9474  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4716/10240  Batch Loss: 0.7207  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4717/10240  Batch Loss: 1.8796  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4718/10240  Batch Loss: 1.8996  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4719/10240  Batch Loss: 1.5295  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4720/10240  Batch Loss: 0.7575  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4721/10240  Batch Loss: 0.7092  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4722/10240  Batch Loss: 1.5889  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4723/10240  Batch Loss: 1.5061  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6056\n",
      "Epoch 001  Batch 4724/10240  Batch Loss: 2.3889  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6055\n",
      "Epoch 001  Batch 4725/10240  Batch Loss: 0.7138  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6055\n",
      "Epoch 001  Batch 4726/10240  Batch Loss: 1.2314  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6055\n",
      "Epoch 001  Batch 4727/10240  Batch Loss: 0.7137  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6055\n",
      "Epoch 001  Batch 4728/10240  Batch Loss: 1.3277  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4729/10240  Batch Loss: 1.3645  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4730/10240  Batch Loss: 1.5555  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4731/10240  Batch Loss: 0.9118  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4732/10240  Batch Loss: 1.0855  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4733/10240  Batch Loss: 2.1770  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4734/10240  Batch Loss: 0.7642  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4735/10240  Batch Loss: 0.8320  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4736/10240  Batch Loss: 0.7586  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4737/10240  Batch Loss: 1.0321  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4738/10240  Batch Loss: 1.1225  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6054\n",
      "Epoch 001  Batch 4739/10240  Batch Loss: 1.2790  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6053\n",
      "Epoch 001  Batch 4740/10240  Batch Loss: 1.3994  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6053\n",
      "Epoch 001  Batch 4741/10240  Batch Loss: 0.7061  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6053\n",
      "Epoch 001  Batch 4742/10240  Batch Loss: 1.0945  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6053\n",
      "Epoch 001  Batch 4743/10240  Batch Loss: 2.0785  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6052\n",
      "Epoch 001  Batch 4744/10240  Batch Loss: 0.7051  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6052\n",
      "Epoch 001  Batch 4745/10240  Batch Loss: 1.4366  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6052\n",
      "Epoch 001  Batch 4746/10240  Batch Loss: 1.0524  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6052\n",
      "Epoch 001  Batch 4747/10240  Batch Loss: 1.5465  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6051\n",
      "Epoch 001  Batch 4748/10240  Batch Loss: 1.5289  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6051\n",
      "Epoch 001  Batch 4749/10240  Batch Loss: 0.6992  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6051\n",
      "Epoch 001  Batch 4750/10240  Batch Loss: 2.1601  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6050\n",
      "Epoch 001  Batch 4751/10240  Batch Loss: 2.0457  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6050\n",
      "Epoch 001  Batch 4752/10240  Batch Loss: 1.4633  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6049\n",
      "Epoch 001  Batch 4753/10240  Batch Loss: 0.8146  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6049\n",
      "Epoch 001  Batch 4754/10240  Batch Loss: 0.7141  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6049\n",
      "Epoch 001  Batch 4755/10240  Batch Loss: 1.0780  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6049\n",
      "Epoch 001  Batch 4756/10240  Batch Loss: 1.0433  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6049\n",
      "Epoch 001  Batch 4757/10240  Batch Loss: 1.9895  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6049\n",
      "Epoch 001  Batch 4758/10240  Batch Loss: 2.2337  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6048\n",
      "Epoch 001  Batch 4759/10240  Batch Loss: 1.5288  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6048\n",
      "Epoch 001  Batch 4760/10240  Batch Loss: 2.6486  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6048\n",
      "Epoch 001  Batch 4761/10240  Batch Loss: 1.8550  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6048\n",
      "Epoch 001  Batch 4762/10240  Batch Loss: 0.7897  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6048\n",
      "Epoch 001  Batch 4763/10240  Batch Loss: 2.1469  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6047\n",
      "Epoch 001  Batch 4764/10240  Batch Loss: 1.6510  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6046\n",
      "Epoch 001  Batch 4765/10240  Batch Loss: 1.2452  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6046\n",
      "Epoch 001  Batch 4766/10240  Batch Loss: 1.6769  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6046\n",
      "Epoch 001  Batch 4767/10240  Batch Loss: 0.7071  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6046\n",
      "Epoch 001  Batch 4768/10240  Batch Loss: 1.1168  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6046\n",
      "Epoch 001  Batch 4769/10240  Batch Loss: 1.5490  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6046\n",
      "Epoch 001  Batch 4770/10240  Batch Loss: 1.7194  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6045\n",
      "Epoch 001  Batch 4771/10240  Batch Loss: 1.4777  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6045\n",
      "Epoch 001  Batch 4772/10240  Batch Loss: 1.3114  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6045\n",
      "Epoch 001  Batch 4773/10240  Batch Loss: 1.3797  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6045\n",
      "Epoch 001  Batch 4774/10240  Batch Loss: 2.4372  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6045\n",
      "Epoch 001  Batch 4775/10240  Batch Loss: 2.4533  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6045\n",
      "Epoch 001  Batch 4776/10240  Batch Loss: 1.3068  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4777/10240  Batch Loss: 2.2263  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4778/10240  Batch Loss: 0.9564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4779/10240  Batch Loss: 1.5105  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4780/10240  Batch Loss: 1.0878  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4781/10240  Batch Loss: 1.8542  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4782/10240  Batch Loss: 1.3359  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4783/10240  Batch Loss: 2.0387  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4784/10240  Batch Loss: 1.3096  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4785/10240  Batch Loss: 0.7386  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4786/10240  Batch Loss: 1.3054  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4787/10240  Batch Loss: 1.8292  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6044\n",
      "Epoch 001  Batch 4788/10240  Batch Loss: 2.7805  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6043\n",
      "Epoch 001  Batch 4789/10240  Batch Loss: 1.7274  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6043\n",
      "Epoch 001  Batch 4790/10240  Batch Loss: 2.0299  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6042\n",
      "Epoch 001  Batch 4791/10240  Batch Loss: 1.2608  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6042\n",
      "Epoch 001  Batch 4792/10240  Batch Loss: 0.7273  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6042\n",
      "Epoch 001  Batch 4793/10240  Batch Loss: 2.0112  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6042\n",
      "Epoch 001  Batch 4794/10240  Batch Loss: 1.6207  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6041\n",
      "Epoch 001  Batch 4795/10240  Batch Loss: 1.0989  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6041\n",
      "Epoch 001  Batch 4796/10240  Batch Loss: 0.7319  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6041\n",
      "Epoch 001  Batch 4797/10240  Batch Loss: 1.2446  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6041\n",
      "Epoch 001  Batch 4798/10240  Batch Loss: 1.7807  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6041\n",
      "Epoch 001  Batch 4799/10240  Batch Loss: 2.0236  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4800/10240  Batch Loss: 0.8506  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4801/10240  Batch Loss: 1.5912  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4802/10240  Batch Loss: 1.1616  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4803/10240  Batch Loss: 1.5267  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4804/10240  Batch Loss: 1.2768  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4805/10240  Batch Loss: 0.7171  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4806/10240  Batch Loss: 1.3325  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6040\n",
      "Epoch 001  Batch 4807/10240  Batch Loss: 0.7910  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6039\n",
      "Epoch 001  Batch 4808/10240  Batch Loss: 1.3640  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6039\n",
      "Epoch 001  Batch 4809/10240  Batch Loss: 1.4692  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6039\n",
      "Epoch 001  Batch 4810/10240  Batch Loss: 1.6963  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6038\n",
      "Epoch 001  Batch 4811/10240  Batch Loss: 2.5406  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6038\n",
      "Epoch 001  Batch 4812/10240  Batch Loss: 1.9758  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4813/10240  Batch Loss: 1.4090  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6038\n",
      "Epoch 001  Batch 4814/10240  Batch Loss: 0.9422  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4815/10240  Batch Loss: 1.7623  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4816/10240  Batch Loss: 1.2054  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4817/10240  Batch Loss: 0.9043  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4818/10240  Batch Loss: 2.3333  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4819/10240  Batch Loss: 0.7004  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6037\n",
      "Epoch 001  Batch 4820/10240  Batch Loss: 2.1276  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6035\n",
      "Epoch 001  Batch 4821/10240  Batch Loss: 2.4405  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6035\n",
      "Epoch 001  Batch 4822/10240  Batch Loss: 0.9549  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6035\n",
      "Epoch 001  Batch 4823/10240  Batch Loss: 1.3877  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6035\n",
      "Epoch 001  Batch 4824/10240  Batch Loss: 1.4275  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6035\n",
      "Epoch 001  Batch 4825/10240  Batch Loss: 1.2952  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6035\n",
      "Epoch 001  Batch 4826/10240  Batch Loss: 1.5630  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4827/10240  Batch Loss: 1.8697  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4828/10240  Batch Loss: 1.2851  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4829/10240  Batch Loss: 1.3510  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4830/10240  Batch Loss: 1.0579  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4831/10240  Batch Loss: 1.4017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4832/10240  Batch Loss: 1.6613  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4833/10240  Batch Loss: 0.7102  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4834/10240  Batch Loss: 1.1242  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6034\n",
      "Epoch 001  Batch 4835/10240  Batch Loss: 1.9620  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4836/10240  Batch Loss: 1.8110  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4837/10240  Batch Loss: 1.8984  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4838/10240  Batch Loss: 0.9898  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4839/10240  Batch Loss: 0.9962  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4840/10240  Batch Loss: 0.7260  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4841/10240  Batch Loss: 0.8123  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4842/10240  Batch Loss: 1.5828  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4843/10240  Batch Loss: 1.0527  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4844/10240  Batch Loss: 1.0201  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4845/10240  Batch Loss: 2.2554  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4846/10240  Batch Loss: 1.6525  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4847/10240  Batch Loss: 1.0362  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4848/10240  Batch Loss: 0.7224  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6033\n",
      "Epoch 001  Batch 4849/10240  Batch Loss: 1.8004  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6032\n",
      "Epoch 001  Batch 4850/10240  Batch Loss: 1.5970  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6032\n",
      "Epoch 001  Batch 4851/10240  Batch Loss: 2.3285  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4852/10240  Batch Loss: 0.7115  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4853/10240  Batch Loss: 0.8944  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6032\n",
      "Epoch 001  Batch 4854/10240  Batch Loss: 1.3941  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4855/10240  Batch Loss: 1.6767  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4856/10240  Batch Loss: 0.7117  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4857/10240  Batch Loss: 0.7168  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4858/10240  Batch Loss: 0.8637  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4859/10240  Batch Loss: 1.8560  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4860/10240  Batch Loss: 1.2772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4861/10240  Batch Loss: 0.7478  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4862/10240  Batch Loss: 0.6904  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4863/10240  Batch Loss: 2.2082  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6031\n",
      "Epoch 001  Batch 4864/10240  Batch Loss: 2.0737  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6030\n",
      "Epoch 001  Batch 4865/10240  Batch Loss: 0.9969  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6030\n",
      "Epoch 001  Batch 4866/10240  Batch Loss: 1.7334  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6030\n",
      "Epoch 001  Batch 4867/10240  Batch Loss: 1.8714  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6029\n",
      "Epoch 001  Batch 4868/10240  Batch Loss: 3.3014  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4869/10240  Batch Loss: 1.2261  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4870/10240  Batch Loss: 0.7121  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4871/10240  Batch Loss: 2.3355  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4872/10240  Batch Loss: 2.2363  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4873/10240  Batch Loss: 1.5685  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4874/10240  Batch Loss: 2.1166  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4875/10240  Batch Loss: 0.9827  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6028\n",
      "Epoch 001  Batch 4876/10240  Batch Loss: 2.0935  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6027\n",
      "Epoch 001  Batch 4877/10240  Batch Loss: 1.2306  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6027\n",
      "Epoch 001  Batch 4878/10240  Batch Loss: 2.3601  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6026\n",
      "Epoch 001  Batch 4879/10240  Batch Loss: 1.3772  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6026\n",
      "Epoch 001  Batch 4880/10240  Batch Loss: 1.0616  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6026\n",
      "Epoch 001  Batch 4881/10240  Batch Loss: 2.7737  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4882/10240  Batch Loss: 1.6671  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4883/10240  Batch Loss: 1.1905  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4884/10240  Batch Loss: 1.1304  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4885/10240  Batch Loss: 0.7281  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4886/10240  Batch Loss: 1.1506  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4887/10240  Batch Loss: 0.6970  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6025\n",
      "Epoch 001  Batch 4888/10240  Batch Loss: 1.6908  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6024\n",
      "Epoch 001  Batch 4889/10240  Batch Loss: 0.8979  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6024\n",
      "Epoch 001  Batch 4890/10240  Batch Loss: 1.4814  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6024\n",
      "Epoch 001  Batch 4891/10240  Batch Loss: 1.2956  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6024\n",
      "Epoch 001  Batch 4892/10240  Batch Loss: 1.8999  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6023\n",
      "Epoch 001  Batch 4893/10240  Batch Loss: 0.9728  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6023\n",
      "Epoch 001  Batch 4894/10240  Batch Loss: 1.8481  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6023\n",
      "Epoch 001  Batch 4895/10240  Batch Loss: 1.2915  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6022\n",
      "Epoch 001  Batch 4896/10240  Batch Loss: 2.1312  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6022\n",
      "Epoch 001  Batch 4897/10240  Batch Loss: 0.9858  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6022\n",
      "Epoch 001  Batch 4898/10240  Batch Loss: 2.4538  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6021\n",
      "Epoch 001  Batch 4899/10240  Batch Loss: 2.7439  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6021\n",
      "Epoch 001  Batch 4900/10240  Batch Loss: 1.7065  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6021\n",
      "Epoch 001  Batch 4901/10240  Batch Loss: 0.7535  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6021\n",
      "Epoch 001  Batch 4902/10240  Batch Loss: 1.8946  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6021\n",
      "Epoch 001  Batch 4903/10240  Batch Loss: 1.6946  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6021\n",
      "Epoch 001  Batch 4904/10240  Batch Loss: 1.7269  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6020\n",
      "Epoch 001  Batch 4905/10240  Batch Loss: 1.3146  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6020\n",
      "Epoch 001  Batch 4906/10240  Batch Loss: 0.7729  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6020\n",
      "Epoch 001  Batch 4907/10240  Batch Loss: 1.1493  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6020\n",
      "Epoch 001  Batch 4908/10240  Batch Loss: 1.0983  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6020\n",
      "Epoch 001  Batch 4909/10240  Batch Loss: 1.6595  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6020\n",
      "Epoch 001  Batch 4910/10240  Batch Loss: 1.8592  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6019\n",
      "Epoch 001  Batch 4911/10240  Batch Loss: 1.3648  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6019\n",
      "Epoch 001  Batch 4912/10240  Batch Loss: 1.3185  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6019\n",
      "Epoch 001  Batch 4913/10240  Batch Loss: 1.0552  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6019\n",
      "Epoch 001  Batch 4914/10240  Batch Loss: 1.2490  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6019\n",
      "Epoch 001  Batch 4915/10240  Batch Loss: 2.9305  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6018\n",
      "Epoch 001  Batch 4916/10240  Batch Loss: 1.7711  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4917/10240  Batch Loss: 0.7063  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4918/10240  Batch Loss: 1.3638  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4919/10240  Batch Loss: 1.7209  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4920/10240  Batch Loss: 1.4347  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4921/10240  Batch Loss: 1.2057  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4922/10240  Batch Loss: 0.9870  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4923/10240  Batch Loss: 1.7151  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4924/10240  Batch Loss: 1.0775  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6017\n",
      "Epoch 001  Batch 4925/10240  Batch Loss: 1.9789  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6016\n",
      "Epoch 001  Batch 4926/10240  Batch Loss: 0.7086  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6016\n",
      "Epoch 001  Batch 4927/10240  Batch Loss: 1.6766  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6016\n",
      "Epoch 001  Batch 4928/10240  Batch Loss: 1.6085  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6016\n",
      "Epoch 001  Batch 4929/10240  Batch Loss: 1.5108  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6016\n",
      "Epoch 001  Batch 4930/10240  Batch Loss: 2.2120  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6016\n",
      "Epoch 001  Batch 4931/10240  Batch Loss: 2.8296  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6015\n",
      "Epoch 001  Batch 4932/10240  Batch Loss: 1.6334  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6014\n",
      "Epoch 001  Batch 4933/10240  Batch Loss: 1.5510  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6014\n",
      "Epoch 001  Batch 4934/10240  Batch Loss: 1.2210  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6014\n",
      "Epoch 001  Batch 4935/10240  Batch Loss: 1.3575  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6014\n",
      "Epoch 001  Batch 4936/10240  Batch Loss: 2.4688  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6013\n",
      "Epoch 001  Batch 4937/10240  Batch Loss: 0.7181  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6013\n",
      "Epoch 001  Batch 4938/10240  Batch Loss: 0.7735  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6013\n",
      "Epoch 001  Batch 4939/10240  Batch Loss: 1.8592  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6013\n",
      "Epoch 001  Batch 4940/10240  Batch Loss: 1.6873  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6013\n",
      "Epoch 001  Batch 4941/10240  Batch Loss: 0.7910  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6013\n",
      "Epoch 001  Batch 4942/10240  Batch Loss: 1.8194  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4943/10240  Batch Loss: 2.3812  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4944/10240  Batch Loss: 1.4858  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4945/10240  Batch Loss: 0.7875  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4946/10240  Batch Loss: 1.7858  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4947/10240  Batch Loss: 1.6506  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4948/10240  Batch Loss: 1.0028  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4949/10240  Batch Loss: 0.7212  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4950/10240  Batch Loss: 1.6730  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4951/10240  Batch Loss: 0.7116  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4952/10240  Batch Loss: 0.9753  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4953/10240  Batch Loss: 0.7239  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4954/10240  Batch Loss: 1.1462  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4955/10240  Batch Loss: 1.5415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4956/10240  Batch Loss: 1.7858  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4957/10240  Batch Loss: 0.7216  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4958/10240  Batch Loss: 2.0972  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4959/10240  Batch Loss: 0.7664  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6012\n",
      "Epoch 001  Batch 4960/10240  Batch Loss: 2.3805  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6011\n",
      "Epoch 001  Batch 4961/10240  Batch Loss: 1.0557  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6010\n",
      "Epoch 001  Batch 4962/10240  Batch Loss: 2.8881  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6009\n",
      "Epoch 001  Batch 4963/10240  Batch Loss: 1.5163  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6009\n",
      "Epoch 001  Batch 4964/10240  Batch Loss: 1.8352  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6009\n",
      "Epoch 001  Batch 4965/10240  Batch Loss: 0.9466  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6009\n",
      "Epoch 001  Batch 4966/10240  Batch Loss: 1.3605  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6009\n",
      "Epoch 001  Batch 4967/10240  Batch Loss: 1.9672  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6008\n",
      "Epoch 001  Batch 4968/10240  Batch Loss: 0.7045  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6008\n",
      "Epoch 001  Batch 4969/10240  Batch Loss: 2.4925  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6008\n",
      "Epoch 001  Batch 4970/10240  Batch Loss: 1.1787  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6008\n",
      "Epoch 001  Batch 4971/10240  Batch Loss: 2.3862  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6008\n",
      "Epoch 001  Batch 4972/10240  Batch Loss: 0.8032  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6008\n",
      "Epoch 001  Batch 4973/10240  Batch Loss: 1.0318  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6007\n",
      "Epoch 001  Batch 4974/10240  Batch Loss: 1.0939  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6007\n",
      "Epoch 001  Batch 4975/10240  Batch Loss: 1.6275  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6007\n",
      "Epoch 001  Batch 4976/10240  Batch Loss: 1.3910  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6007\n",
      "Epoch 001  Batch 4977/10240  Batch Loss: 1.8487  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6006\n",
      "Epoch 001  Batch 4978/10240  Batch Loss: 1.4129  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6006\n",
      "Epoch 001  Batch 4979/10240  Batch Loss: 3.4924  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6006\n",
      "Epoch 001  Batch 4980/10240  Batch Loss: 1.0135  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6005\n",
      "Epoch 001  Batch 4981/10240  Batch Loss: 2.0695  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6005\n",
      "Epoch 001  Batch 4982/10240  Batch Loss: 0.7306  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6005\n",
      "Epoch 001  Batch 4983/10240  Batch Loss: 1.9274  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6004\n",
      "Epoch 001  Batch 4984/10240  Batch Loss: 0.9630  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6004\n",
      "Epoch 001  Batch 4985/10240  Batch Loss: 1.9376  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6004\n",
      "Epoch 001  Batch 4986/10240  Batch Loss: 1.4354  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6004\n",
      "Epoch 001  Batch 4987/10240  Batch Loss: 1.6671  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6004\n",
      "Epoch 001  Batch 4988/10240  Batch Loss: 1.8537  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6003\n",
      "Epoch 001  Batch 4989/10240  Batch Loss: 2.1037  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6003\n",
      "Epoch 001  Batch 4990/10240  Batch Loss: 2.4066  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6002\n",
      "Epoch 001  Batch 4991/10240  Batch Loss: 2.5887  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4992/10240  Batch Loss: 0.7387  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4993/10240  Batch Loss: 0.8859  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4994/10240  Batch Loss: 1.4734  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4995/10240  Batch Loss: 1.1156  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4996/10240  Batch Loss: 1.7062  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4997/10240  Batch Loss: 1.0347  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6001\n",
      "Epoch 001  Batch 4998/10240  Batch Loss: 1.2927  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6000\n",
      "Epoch 001  Batch 4999/10240  Batch Loss: 1.1739  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.6000\n",
      "Epoch 001  Batch 5000/10240  Batch Loss: 1.5900  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5999\n",
      "Epoch 001  Batch 5001/10240  Batch Loss: 1.2672  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5999\n",
      "Epoch 001  Batch 5002/10240  Batch Loss: 1.2031  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5999\n",
      "Epoch 001  Batch 5003/10240  Batch Loss: 1.1668  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5999\n",
      "Epoch 001  Batch 5004/10240  Batch Loss: 1.6906  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5005/10240  Batch Loss: 0.9034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5006/10240  Batch Loss: 2.1834  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5007/10240  Batch Loss: 1.6396  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5008/10240  Batch Loss: 0.9319  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5009/10240  Batch Loss: 2.4021  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5010/10240  Batch Loss: 1.2346  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5011/10240  Batch Loss: 0.8479  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5998\n",
      "Epoch 001  Batch 5012/10240  Batch Loss: 1.8024  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5013/10240  Batch Loss: 1.9678  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5014/10240  Batch Loss: 0.7992  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5015/10240  Batch Loss: 0.7327  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5016/10240  Batch Loss: 1.7538  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5017/10240  Batch Loss: 1.3877  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5018/10240  Batch Loss: 1.9264  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5019/10240  Batch Loss: 1.1976  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5020/10240  Batch Loss: 1.5811  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5021/10240  Batch Loss: 1.7240  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5022/10240  Batch Loss: 1.1348  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5023/10240  Batch Loss: 0.9621  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5024/10240  Batch Loss: 1.1856  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5025/10240  Batch Loss: 1.9882  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5026/10240  Batch Loss: 1.4242  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5027/10240  Batch Loss: 1.4755  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5997\n",
      "Epoch 001  Batch 5028/10240  Batch Loss: 0.8861  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5029/10240  Batch Loss: 0.7158  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5030/10240  Batch Loss: 1.1029  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5031/10240  Batch Loss: 1.0125  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5032/10240  Batch Loss: 1.3702  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5033/10240  Batch Loss: 1.4947  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5034/10240  Batch Loss: 0.7339  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5035/10240  Batch Loss: 1.8854  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5996\n",
      "Epoch 001  Batch 5036/10240  Batch Loss: 2.4076  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5037/10240  Batch Loss: 1.3011  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5038/10240  Batch Loss: 0.8049  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5039/10240  Batch Loss: 1.2687  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5040/10240  Batch Loss: 0.7824  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5041/10240  Batch Loss: 1.1527  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5042/10240  Batch Loss: 1.0295  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5043/10240  Batch Loss: 2.4415  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5044/10240  Batch Loss: 2.0198  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5045/10240  Batch Loss: 1.0459  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5995\n",
      "Epoch 001  Batch 5046/10240  Batch Loss: 1.4906  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5047/10240  Batch Loss: 0.9681  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5048/10240  Batch Loss: 1.5921  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5049/10240  Batch Loss: 1.1045  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5050/10240  Batch Loss: 1.8262  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5051/10240  Batch Loss: 2.2706  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5052/10240  Batch Loss: 0.9397  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5053/10240  Batch Loss: 1.2307  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5054/10240  Batch Loss: 0.7251  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5055/10240  Batch Loss: 0.9595  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5056/10240  Batch Loss: 2.5115  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5057/10240  Batch Loss: 2.0958  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5994\n",
      "Epoch 001  Batch 5058/10240  Batch Loss: 1.6579  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5059/10240  Batch Loss: 1.5437  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5060/10240  Batch Loss: 1.6583  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5061/10240  Batch Loss: 1.1434  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5062/10240  Batch Loss: 1.1145  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5063/10240  Batch Loss: 1.0543  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5064/10240  Batch Loss: 1.8340  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5065/10240  Batch Loss: 0.6983  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5066/10240  Batch Loss: 1.9248  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5067/10240  Batch Loss: 1.5392  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5068/10240  Batch Loss: 1.6441  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5069/10240  Batch Loss: 0.7913  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5070/10240  Batch Loss: 1.2017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5071/10240  Batch Loss: 1.5056  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5072/10240  Batch Loss: 0.7147  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5073/10240  Batch Loss: 0.8992  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5074/10240  Batch Loss: 1.8449  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5075/10240  Batch Loss: 2.0719  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5993\n",
      "Epoch 001  Batch 5076/10240  Batch Loss: 1.8755  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5077/10240  Batch Loss: 1.3887  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5078/10240  Batch Loss: 0.7070  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5079/10240  Batch Loss: 1.1684  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5080/10240  Batch Loss: 1.4917  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5081/10240  Batch Loss: 1.6374  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5082/10240  Batch Loss: 0.7245  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5083/10240  Batch Loss: 1.4039  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5084/10240  Batch Loss: 0.9240  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5085/10240  Batch Loss: 1.7289  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5086/10240  Batch Loss: 1.4309  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5087/10240  Batch Loss: 1.4403  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5088/10240  Batch Loss: 1.4327  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5089/10240  Batch Loss: 1.1771  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5992\n",
      "Epoch 001  Batch 5090/10240  Batch Loss: 2.8984  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5991\n",
      "Epoch 001  Batch 5091/10240  Batch Loss: 1.0769  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5991\n",
      "Epoch 001  Batch 5092/10240  Batch Loss: 1.2757  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5991\n",
      "Epoch 001  Batch 5093/10240  Batch Loss: 1.0816  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5094/10240  Batch Loss: 1.2526  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5991\n",
      "Epoch 001  Batch 5095/10240  Batch Loss: 0.9096  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5096/10240  Batch Loss: 1.4241  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5097/10240  Batch Loss: 2.4058  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5098/10240  Batch Loss: 1.1479  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5099/10240  Batch Loss: 1.2977  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5100/10240  Batch Loss: 2.4545  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5101/10240  Batch Loss: 1.0084  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5102/10240  Batch Loss: 1.8975  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5990\n",
      "Epoch 001  Batch 5103/10240  Batch Loss: 1.0573  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5989\n",
      "Epoch 001  Batch 5104/10240  Batch Loss: 1.9708  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5989\n",
      "Epoch 001  Batch 5105/10240  Batch Loss: 0.8614  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5989\n",
      "Epoch 001  Batch 5106/10240  Batch Loss: 3.1108  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5988\n",
      "Epoch 001  Batch 5107/10240  Batch Loss: 1.5005  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5988\n",
      "Epoch 001  Batch 5108/10240  Batch Loss: 1.1007  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5988\n",
      "Epoch 001  Batch 5109/10240  Batch Loss: 1.0300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5988\n",
      "Epoch 001  Batch 5110/10240  Batch Loss: 1.2169  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5988\n",
      "Epoch 001  Batch 5111/10240  Batch Loss: 1.1109  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5112/10240  Batch Loss: 1.1504  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5113/10240  Batch Loss: 2.0490  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5114/10240  Batch Loss: 1.0938  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5115/10240  Batch Loss: 1.2253  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5116/10240  Batch Loss: 0.8792  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5117/10240  Batch Loss: 0.7735  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5118/10240  Batch Loss: 1.9065  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5119/10240  Batch Loss: 1.1961  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5120/10240  Batch Loss: 2.0098  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5987\n",
      "Epoch 001  Batch 5121/10240  Batch Loss: 1.2650  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5122/10240  Batch Loss: 2.0802  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5123/10240  Batch Loss: 1.4438  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5124/10240  Batch Loss: 1.7920  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5125/10240  Batch Loss: 1.4903  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5126/10240  Batch Loss: 0.9348  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5986\n",
      "Epoch 001  Batch 5127/10240  Batch Loss: 2.1812  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5128/10240  Batch Loss: 1.3457  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5129/10240  Batch Loss: 0.7229  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5130/10240  Batch Loss: 0.7302  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5131/10240  Batch Loss: 1.9619  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5132/10240  Batch Loss: 1.3163  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5133/10240  Batch Loss: 1.2038  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5985\n",
      "Epoch 001  Batch 5134/10240  Batch Loss: 2.6893  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5135/10240  Batch Loss: 2.1236  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5136/10240  Batch Loss: 0.7194  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5137/10240  Batch Loss: 0.7517  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5138/10240  Batch Loss: 1.5342  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5139/10240  Batch Loss: 1.9330  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5140/10240  Batch Loss: 1.1762  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5141/10240  Batch Loss: 1.2591  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5984\n",
      "Epoch 001  Batch 5142/10240  Batch Loss: 0.8411  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5983\n",
      "Epoch 001  Batch 5143/10240  Batch Loss: 1.5959  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5983\n",
      "Epoch 001  Batch 5144/10240  Batch Loss: 1.2390  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5983\n",
      "Epoch 001  Batch 5145/10240  Batch Loss: 0.7994  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5983\n",
      "Epoch 001  Batch 5146/10240  Batch Loss: 1.4197  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5147/10240  Batch Loss: 1.5633  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5148/10240  Batch Loss: 1.3895  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5149/10240  Batch Loss: 1.2297  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5150/10240  Batch Loss: 1.4811  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5151/10240  Batch Loss: 0.7004  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5152/10240  Batch Loss: 1.2822  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5982\n",
      "Epoch 001  Batch 5153/10240  Batch Loss: 2.1062  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5154/10240  Batch Loss: 1.2111  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5155/10240  Batch Loss: 2.0061  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5156/10240  Batch Loss: 2.0901  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5157/10240  Batch Loss: 1.2936  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5158/10240  Batch Loss: 1.3524  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5159/10240  Batch Loss: 1.3886  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5160/10240  Batch Loss: 1.0652  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5161/10240  Batch Loss: 0.7172  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5162/10240  Batch Loss: 1.4922  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5163/10240  Batch Loss: 0.8868  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5164/10240  Batch Loss: 1.1607  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5165/10240  Batch Loss: 1.5997  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5166/10240  Batch Loss: 0.8172  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5167/10240  Batch Loss: 0.9232  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5168/10240  Batch Loss: 1.4887  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5169/10240  Batch Loss: 1.0158  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5170/10240  Batch Loss: 1.4992  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5171/10240  Batch Loss: 1.5464  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5172/10240  Batch Loss: 0.9628  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5173/10240  Batch Loss: 1.1350  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5174/10240  Batch Loss: 1.4691  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5981\n",
      "Epoch 001  Batch 5175/10240  Batch Loss: 1.4829  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5980\n",
      "Epoch 001  Batch 5176/10240  Batch Loss: 1.4540  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5980\n",
      "Epoch 001  Batch 5177/10240  Batch Loss: 1.1172  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5980\n",
      "Epoch 001  Batch 5178/10240  Batch Loss: 2.5579  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5979\n",
      "Epoch 001  Batch 5179/10240  Batch Loss: 3.3835  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5978\n",
      "Epoch 001  Batch 5180/10240  Batch Loss: 2.0858  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5977\n",
      "Epoch 001  Batch 5181/10240  Batch Loss: 1.0341  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5977\n",
      "Epoch 001  Batch 5182/10240  Batch Loss: 1.9208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5183/10240  Batch Loss: 1.5196  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5184/10240  Batch Loss: 1.8045  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5185/10240  Batch Loss: 1.8384  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5977\n",
      "Epoch 001  Batch 5186/10240  Batch Loss: 1.5631  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5977\n",
      "Epoch 001  Batch 5187/10240  Batch Loss: 1.4344  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5188/10240  Batch Loss: 1.4525  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5977\n",
      "Epoch 001  Batch 5189/10240  Batch Loss: 1.0617  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5190/10240  Batch Loss: 1.4268  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5191/10240  Batch Loss: 0.7107  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5192/10240  Batch Loss: 1.3532  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5193/10240  Batch Loss: 1.4038  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5194/10240  Batch Loss: 0.9636  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5195/10240  Batch Loss: 1.9455  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5196/10240  Batch Loss: 0.7552  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5197/10240  Batch Loss: 1.4302  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5198/10240  Batch Loss: 1.6085  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5976\n",
      "Epoch 001  Batch 5199/10240  Batch Loss: 1.5552  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5975\n",
      "Epoch 001  Batch 5200/10240  Batch Loss: 1.4239  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5975\n",
      "Epoch 001  Batch 5201/10240  Batch Loss: 2.2151  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5975\n",
      "Epoch 001  Batch 5202/10240  Batch Loss: 0.7040  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5975\n",
      "Epoch 001  Batch 5203/10240  Batch Loss: 1.4070  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5975\n",
      "Epoch 001  Batch 5204/10240  Batch Loss: 1.5249  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5205/10240  Batch Loss: 0.7113  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5206/10240  Batch Loss: 1.3412  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5207/10240  Batch Loss: 1.1326  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5208/10240  Batch Loss: 0.9070  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5209/10240  Batch Loss: 1.2891  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5210/10240  Batch Loss: 1.1162  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5211/10240  Batch Loss: 1.3477  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5212/10240  Batch Loss: 1.0489  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5974\n",
      "Epoch 001  Batch 5213/10240  Batch Loss: 2.4782  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5973\n",
      "Epoch 001  Batch 5214/10240  Batch Loss: 1.2757  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5973\n",
      "Epoch 001  Batch 5215/10240  Batch Loss: 1.2843  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5973\n",
      "Epoch 001  Batch 5216/10240  Batch Loss: 1.1144  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5973\n",
      "Epoch 001  Batch 5217/10240  Batch Loss: 1.8054  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5973\n",
      "Epoch 001  Batch 5218/10240  Batch Loss: 1.6334  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5972\n",
      "Epoch 001  Batch 5219/10240  Batch Loss: 2.4171  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5220/10240  Batch Loss: 1.4017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5221/10240  Batch Loss: 3.1820  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5222/10240  Batch Loss: 1.1565  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5223/10240  Batch Loss: 1.6520  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5224/10240  Batch Loss: 1.2897  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5225/10240  Batch Loss: 0.7140  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5226/10240  Batch Loss: 1.0018  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5227/10240  Batch Loss: 1.4406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5228/10240  Batch Loss: 0.7390  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5229/10240  Batch Loss: 1.1641  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5230/10240  Batch Loss: 0.7728  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5231/10240  Batch Loss: 1.0746  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5232/10240  Batch Loss: 1.0465  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5233/10240  Batch Loss: 1.9122  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5234/10240  Batch Loss: 1.6315  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5971\n",
      "Epoch 001  Batch 5235/10240  Batch Loss: 1.5952  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5236/10240  Batch Loss: 1.2658  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5237/10240  Batch Loss: 1.2780  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5238/10240  Batch Loss: 0.9797  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5239/10240  Batch Loss: 1.0034  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5240/10240  Batch Loss: 0.8792  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5241/10240  Batch Loss: 1.1672  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5242/10240  Batch Loss: 1.0973  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5969\n",
      "Epoch 001  Batch 5243/10240  Batch Loss: 2.8611  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5244/10240  Batch Loss: 1.5970  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5970\n",
      "Epoch 001  Batch 5245/10240  Batch Loss: 1.3720  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5969\n",
      "Epoch 001  Batch 5246/10240  Batch Loss: 1.0503  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5969\n",
      "Epoch 001  Batch 5247/10240  Batch Loss: 0.9233  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5969\n",
      "Epoch 001  Batch 5248/10240  Batch Loss: 2.0564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5969\n",
      "Epoch 001  Batch 5249/10240  Batch Loss: 1.8060  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5968\n",
      "Epoch 001  Batch 5250/10240  Batch Loss: 0.9523  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5968\n",
      "Epoch 001  Batch 5251/10240  Batch Loss: 1.5158  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5968\n",
      "Epoch 001  Batch 5252/10240  Batch Loss: 2.6091  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5968\n",
      "Epoch 001  Batch 5253/10240  Batch Loss: 1.5886  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5968\n",
      "Epoch 001  Batch 5254/10240  Batch Loss: 1.4911  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5968\n",
      "Epoch 001  Batch 5255/10240  Batch Loss: 1.9182  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5967\n",
      "Epoch 001  Batch 5256/10240  Batch Loss: 0.7162  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5967\n",
      "Epoch 001  Batch 5257/10240  Batch Loss: 1.1984  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5967\n",
      "Epoch 001  Batch 5258/10240  Batch Loss: 2.4142  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5967\n",
      "Epoch 001  Batch 5259/10240  Batch Loss: 1.3258  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5967\n",
      "Epoch 001  Batch 5260/10240  Batch Loss: 0.9498  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5261/10240  Batch Loss: 1.0317  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5262/10240  Batch Loss: 0.7217  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5263/10240  Batch Loss: 0.7152  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5264/10240  Batch Loss: 1.3381  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5967\n",
      "Epoch 001  Batch 5265/10240  Batch Loss: 1.2396  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5266/10240  Batch Loss: 1.7194  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5267/10240  Batch Loss: 1.9858  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5268/10240  Batch Loss: 1.3717  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5966\n",
      "Epoch 001  Batch 5269/10240  Batch Loss: 1.8440  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5965\n",
      "Epoch 001  Batch 5270/10240  Batch Loss: 2.0782  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5965\n",
      "Epoch 001  Batch 5271/10240  Batch Loss: 0.7913  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5965\n",
      "Epoch 001  Batch 5272/10240  Batch Loss: 1.4472  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5965\n",
      "Epoch 001  Batch 5273/10240  Batch Loss: 1.2391  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5964\n",
      "Epoch 001  Batch 5274/10240  Batch Loss: 2.2211  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5964\n",
      "Epoch 001  Batch 5275/10240  Batch Loss: 1.1491  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5964\n",
      "Epoch 001  Batch 5276/10240  Batch Loss: 1.3041  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5963\n",
      "Epoch 001  Batch 5277/10240  Batch Loss: 1.2496  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5963\n",
      "Epoch 001  Batch 5278/10240  Batch Loss: 1.4083  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5963\n",
      "Epoch 001  Batch 5279/10240  Batch Loss: 2.0564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5280/10240  Batch Loss: 0.8549  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5281/10240  Batch Loss: 0.7065  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5282/10240  Batch Loss: 0.9779  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5283/10240  Batch Loss: 0.9139  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5284/10240  Batch Loss: 1.8293  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5285/10240  Batch Loss: 0.7017  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5286/10240  Batch Loss: 0.9925  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5287/10240  Batch Loss: 1.1995  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5962\n",
      "Epoch 001  Batch 5288/10240  Batch Loss: 0.9013  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5289/10240  Batch Loss: 0.8481  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5290/10240  Batch Loss: 0.8308  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5291/10240  Batch Loss: 2.3564  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5292/10240  Batch Loss: 1.5614  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5293/10240  Batch Loss: 2.1251  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5294/10240  Batch Loss: 0.9439  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5295/10240  Batch Loss: 0.7149  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5296/10240  Batch Loss: 0.7378  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5297/10240  Batch Loss: 0.9111  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5298/10240  Batch Loss: 1.3213  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5299/10240  Batch Loss: 1.1144  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5300/10240  Batch Loss: 1.5501  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5301/10240  Batch Loss: 1.6188  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5961\n",
      "Epoch 001  Batch 5302/10240  Batch Loss: 1.5832  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5960\n",
      "Epoch 001  Batch 5303/10240  Batch Loss: 3.1956  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5959\n",
      "Epoch 001  Batch 5304/10240  Batch Loss: 0.8922  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5959\n",
      "Epoch 001  Batch 5305/10240  Batch Loss: 2.5725  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5959\n",
      "Epoch 001  Batch 5306/10240  Batch Loss: 1.1254  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5959\n",
      "Epoch 001  Batch 5307/10240  Batch Loss: 1.4835  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5958\n",
      "Epoch 001  Batch 5308/10240  Batch Loss: 2.4124  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5309/10240  Batch Loss: 2.3809  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5310/10240  Batch Loss: 0.7027  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5311/10240  Batch Loss: 0.8236  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5312/10240  Batch Loss: 0.7995  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5313/10240  Batch Loss: 1.8710  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5314/10240  Batch Loss: 1.1790  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5957\n",
      "Epoch 001  Batch 5315/10240  Batch Loss: 2.3077  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5956\n",
      "Epoch 001  Batch 5316/10240  Batch Loss: 0.9206  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5956\n",
      "Epoch 001  Batch 5317/10240  Batch Loss: 1.0137  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5956\n",
      "Epoch 001  Batch 5318/10240  Batch Loss: 0.7258  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5956\n",
      "Epoch 001  Batch 5319/10240  Batch Loss: 1.0638  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5956\n",
      "Epoch 001  Batch 5320/10240  Batch Loss: 2.0929  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5955\n",
      "Epoch 001  Batch 5321/10240  Batch Loss: 1.6787  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5955\n",
      "Epoch 001  Batch 5322/10240  Batch Loss: 1.6986  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5955\n",
      "Epoch 001  Batch 5323/10240  Batch Loss: 1.1740  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5955\n",
      "Epoch 001  Batch 5324/10240  Batch Loss: 2.1610  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5325/10240  Batch Loss: 1.8015  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5326/10240  Batch Loss: 2.3088  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5327/10240  Batch Loss: 2.4329  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5328/10240  Batch Loss: 1.4720  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5329/10240  Batch Loss: 1.3850  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5330/10240  Batch Loss: 1.5933  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5331/10240  Batch Loss: 1.6970  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5332/10240  Batch Loss: 1.5965  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5333/10240  Batch Loss: 1.1653  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5334/10240  Batch Loss: 2.2565  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5955\n",
      "Epoch 001  Batch 5335/10240  Batch Loss: 1.5214  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5336/10240  Batch Loss: 1.3783  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5337/10240  Batch Loss: 2.3607  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5954\n",
      "Epoch 001  Batch 5338/10240  Batch Loss: 1.7156  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5953\n",
      "Epoch 001  Batch 5339/10240  Batch Loss: 0.9070  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5953\n",
      "Epoch 001  Batch 5340/10240  Batch Loss: 1.7449  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5953\n",
      "Epoch 001  Batch 5341/10240  Batch Loss: 2.4647  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5953\n",
      "Epoch 001  Batch 5342/10240  Batch Loss: 1.1308  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5953\n",
      "Epoch 001  Batch 5343/10240  Batch Loss: 1.1024  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5344/10240  Batch Loss: 1.7661  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5345/10240  Batch Loss: 1.7010  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5346/10240  Batch Loss: 0.8701  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5347/10240  Batch Loss: 1.1818  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5348/10240  Batch Loss: 1.6028  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5349/10240  Batch Loss: 0.7133  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5350/10240  Batch Loss: 0.7826  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5351/10240  Batch Loss: 1.6406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5352/10240  Batch Loss: 1.8629  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5353/10240  Batch Loss: 2.0296  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5952\n",
      "Epoch 001  Batch 5354/10240  Batch Loss: 1.9398  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5951\n",
      "Epoch 001  Batch 5355/10240  Batch Loss: 2.5727  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5950\n",
      "Epoch 001  Batch 5356/10240  Batch Loss: 1.2363  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5950\n",
      "Epoch 001  Batch 5357/10240  Batch Loss: 1.0064  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5950\n",
      "Epoch 001  Batch 5358/10240  Batch Loss: 2.2498  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5359/10240  Batch Loss: 0.7204  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5360/10240  Batch Loss: 0.7131  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5361/10240  Batch Loss: 2.2297  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5362/10240  Batch Loss: 1.8524  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5363/10240  Batch Loss: 1.1857  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5364/10240  Batch Loss: 0.9104  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5365/10240  Batch Loss: 2.7386  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5366/10240  Batch Loss: 1.3479  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5367/10240  Batch Loss: 1.6977  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5368/10240  Batch Loss: 2.1704  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5950\n",
      "Epoch 001  Batch 5369/10240  Batch Loss: 1.8190  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5370/10240  Batch Loss: 0.7134  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5371/10240  Batch Loss: 1.5662  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5372/10240  Batch Loss: 0.7870  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5373/10240  Batch Loss: 1.1717  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5374/10240  Batch Loss: 1.8140  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5375/10240  Batch Loss: 1.0755  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5376/10240  Batch Loss: 0.7228  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5949\n",
      "Epoch 001  Batch 5377/10240  Batch Loss: 1.9125  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5378/10240  Batch Loss: 0.7343  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5379/10240  Batch Loss: 1.3814  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5380/10240  Batch Loss: 1.0773  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5381/10240  Batch Loss: 1.6240  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5382/10240  Batch Loss: 2.3833  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5383/10240  Batch Loss: 0.7297  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5384/10240  Batch Loss: 1.5037  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5385/10240  Batch Loss: 1.4553  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5386/10240  Batch Loss: 0.8941  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5387/10240  Batch Loss: 0.7644  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5388/10240  Batch Loss: 0.7269  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5948\n",
      "Epoch 001  Batch 5389/10240  Batch Loss: 2.6010  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5390/10240  Batch Loss: 0.7029  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5391/10240  Batch Loss: 0.7000  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5392/10240  Batch Loss: 0.8842  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5393/10240  Batch Loss: 1.2956  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5394/10240  Batch Loss: 1.7620  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5395/10240  Batch Loss: 1.9685  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5396/10240  Batch Loss: 1.2506  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5397/10240  Batch Loss: 0.9757  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5398/10240  Batch Loss: 0.7513  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5399/10240  Batch Loss: 1.5329  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5400/10240  Batch Loss: 1.0270  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5401/10240  Batch Loss: 1.8719  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5402/10240  Batch Loss: 1.1557  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5403/10240  Batch Loss: 0.8144  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5947\n",
      "Epoch 001  Batch 5404/10240  Batch Loss: 2.4644  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5946\n",
      "Epoch 001  Batch 5405/10240  Batch Loss: 2.4402  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5406/10240  Batch Loss: 0.8950  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5407/10240  Batch Loss: 1.6447  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5408/10240  Batch Loss: 1.3263  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5409/10240  Batch Loss: 1.3431  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5410/10240  Batch Loss: 0.9733  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5411/10240  Batch Loss: 1.0602  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5945\n",
      "Epoch 001  Batch 5412/10240  Batch Loss: 1.5953  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5413/10240  Batch Loss: 0.7231  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5414/10240  Batch Loss: 1.7047  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5415/10240  Batch Loss: 1.7983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5416/10240  Batch Loss: 0.9006  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5417/10240  Batch Loss: 0.7194  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5418/10240  Batch Loss: 0.9535  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5419/10240  Batch Loss: 0.7118  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5420/10240  Batch Loss: 1.0275  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5421/10240  Batch Loss: 1.4529  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5422/10240  Batch Loss: 1.4338  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5944\n",
      "Epoch 001  Batch 5423/10240  Batch Loss: 1.3162  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5943\n",
      "Epoch 001  Batch 5424/10240  Batch Loss: 0.7061  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5943\n",
      "Epoch 001  Batch 5425/10240  Batch Loss: 1.8809  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5943\n",
      "Epoch 001  Batch 5426/10240  Batch Loss: 2.0499  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5943\n",
      "Epoch 001  Batch 5427/10240  Batch Loss: 1.9454  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5942\n",
      "Epoch 001  Batch 5428/10240  Batch Loss: 1.2449  | train F1: 0.0071  | train precision: 0.0036  | train recall: 0.5942\n",
      "Epoch 001  Batch 5429/10240  Batch Loss: 2.3435  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5942\n",
      "Epoch 001  Batch 5430/10240  Batch Loss: 1.1769  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5942\n",
      "Epoch 001  Batch 5431/10240  Batch Loss: 1.6195  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5941\n",
      "Epoch 001  Batch 5432/10240  Batch Loss: 2.0749  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5942\n",
      "Epoch 001  Batch 5433/10240  Batch Loss: 1.4620  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5942\n",
      "Epoch 001  Batch 5434/10240  Batch Loss: 2.5513  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5435/10240  Batch Loss: 1.2815  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5436/10240  Batch Loss: 1.0861  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5437/10240  Batch Loss: 1.3150  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5438/10240  Batch Loss: 1.5753  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5439/10240  Batch Loss: 1.6780  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5440/10240  Batch Loss: 2.5355  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5441/10240  Batch Loss: 0.7304  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5442/10240  Batch Loss: 1.5537  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5443/10240  Batch Loss: 1.3737  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5444/10240  Batch Loss: 1.6092  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5445/10240  Batch Loss: 0.7987  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5446/10240  Batch Loss: 1.4530  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5447/10240  Batch Loss: 0.8718  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5448/10240  Batch Loss: 0.9365  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5940\n",
      "Epoch 001  Batch 5449/10240  Batch Loss: 1.6037  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5939\n",
      "Epoch 001  Batch 5450/10240  Batch Loss: 3.0587  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5938\n",
      "Epoch 001  Batch 5451/10240  Batch Loss: 0.8265  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5938\n",
      "Epoch 001  Batch 5452/10240  Batch Loss: 1.2994  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5938\n",
      "Epoch 001  Batch 5453/10240  Batch Loss: 1.5612  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5938\n",
      "Epoch 001  Batch 5454/10240  Batch Loss: 1.6521  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5938\n",
      "Epoch 001  Batch 5455/10240  Batch Loss: 1.6867  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5456/10240  Batch Loss: 0.6998  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5457/10240  Batch Loss: 0.8530  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5458/10240  Batch Loss: 1.3510  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5459/10240  Batch Loss: 1.3118  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5460/10240  Batch Loss: 0.7350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5461/10240  Batch Loss: 1.1632  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5462/10240  Batch Loss: 2.7129  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5463/10240  Batch Loss: 1.0779  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5464/10240  Batch Loss: 0.6904  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5465/10240  Batch Loss: 1.6587  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5466/10240  Batch Loss: 1.2300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5467/10240  Batch Loss: 1.2430  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5936\n",
      "Epoch 001  Batch 5468/10240  Batch Loss: 1.5698  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5936\n",
      "Epoch 001  Batch 5469/10240  Batch Loss: 1.5485  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5937\n",
      "Epoch 001  Batch 5470/10240  Batch Loss: 1.6432  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5936\n",
      "Epoch 001  Batch 5471/10240  Batch Loss: 0.7116  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5936\n",
      "Epoch 001  Batch 5472/10240  Batch Loss: 1.4175  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5936\n",
      "Epoch 001  Batch 5473/10240  Batch Loss: 2.9840  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5474/10240  Batch Loss: 1.9766  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5475/10240  Batch Loss: 2.0889  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5476/10240  Batch Loss: 1.9794  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5477/10240  Batch Loss: 2.5421  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5478/10240  Batch Loss: 0.7176  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5479/10240  Batch Loss: 1.9358  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5480/10240  Batch Loss: 1.8909  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5481/10240  Batch Loss: 1.3995  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5935\n",
      "Epoch 001  Batch 5482/10240  Batch Loss: 1.4626  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5483/10240  Batch Loss: 2.0061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5484/10240  Batch Loss: 0.8866  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5485/10240  Batch Loss: 0.7113  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5486/10240  Batch Loss: 0.7249  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5487/10240  Batch Loss: 1.5029  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5488/10240  Batch Loss: 1.8061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5489/10240  Batch Loss: 2.1965  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5490/10240  Batch Loss: 1.9382  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5933\n",
      "Epoch 001  Batch 5491/10240  Batch Loss: 1.4968  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5933\n",
      "Epoch 001  Batch 5492/10240  Batch Loss: 0.6941  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5933\n",
      "Epoch 001  Batch 5493/10240  Batch Loss: 1.0222  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5494/10240  Batch Loss: 1.3879  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5934\n",
      "Epoch 001  Batch 5495/10240  Batch Loss: 2.1760  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5933\n",
      "Epoch 001  Batch 5496/10240  Batch Loss: 1.8631  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5497/10240  Batch Loss: 1.0766  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5498/10240  Batch Loss: 1.5360  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5499/10240  Batch Loss: 1.2836  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5500/10240  Batch Loss: 1.2336  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5501/10240  Batch Loss: 1.0354  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5502/10240  Batch Loss: 2.8676  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5503/10240  Batch Loss: 1.5821  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5504/10240  Batch Loss: 1.1636  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5505/10240  Batch Loss: 1.6625  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5932\n",
      "Epoch 001  Batch 5506/10240  Batch Loss: 1.6137  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5931\n",
      "Epoch 001  Batch 5507/10240  Batch Loss: 1.6229  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5931\n",
      "Epoch 001  Batch 5508/10240  Batch Loss: 2.4228  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5509/10240  Batch Loss: 1.8600  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5510/10240  Batch Loss: 2.3428  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5931\n",
      "Epoch 001  Batch 5511/10240  Batch Loss: 1.0498  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5931\n",
      "Epoch 001  Batch 5512/10240  Batch Loss: 2.9594  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5513/10240  Batch Loss: 2.3071  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5514/10240  Batch Loss: 1.8612  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5515/10240  Batch Loss: 2.0789  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5516/10240  Batch Loss: 1.8189  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5517/10240  Batch Loss: 0.8339  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5518/10240  Batch Loss: 1.9622  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5519/10240  Batch Loss: 1.0869  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5520/10240  Batch Loss: 1.3267  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5930\n",
      "Epoch 001  Batch 5521/10240  Batch Loss: 1.3769  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5522/10240  Batch Loss: 0.7214  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5523/10240  Batch Loss: 2.2991  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5524/10240  Batch Loss: 1.4982  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5525/10240  Batch Loss: 1.1189  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5929\n",
      "Epoch 001  Batch 5526/10240  Batch Loss: 1.5887  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5527/10240  Batch Loss: 1.9125  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5528/10240  Batch Loss: 1.7931  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5529/10240  Batch Loss: 1.8628  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5530/10240  Batch Loss: 1.4279  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5531/10240  Batch Loss: 1.7146  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5532/10240  Batch Loss: 3.4439  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5533/10240  Batch Loss: 1.2322  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5534/10240  Batch Loss: 0.9321  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5535/10240  Batch Loss: 0.7729  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5536/10240  Batch Loss: 0.8206  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5537/10240  Batch Loss: 1.3452  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5538/10240  Batch Loss: 1.1120  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5539/10240  Batch Loss: 0.7453  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5540/10240  Batch Loss: 1.3570  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5541/10240  Batch Loss: 0.7252  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5542/10240  Batch Loss: 2.3532  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5543/10240  Batch Loss: 1.5775  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5544/10240  Batch Loss: 1.3290  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5545/10240  Batch Loss: 0.9039  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5546/10240  Batch Loss: 1.7891  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5547/10240  Batch Loss: 2.7943  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5548/10240  Batch Loss: 2.4107  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5549/10240  Batch Loss: 0.9814  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5550/10240  Batch Loss: 1.2707  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5551/10240  Batch Loss: 1.8608  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5552/10240  Batch Loss: 0.7222  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5553/10240  Batch Loss: 0.7334  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5554/10240  Batch Loss: 1.1966  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5555/10240  Batch Loss: 0.9800  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5556/10240  Batch Loss: 1.6697  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5557/10240  Batch Loss: 1.9814  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5558/10240  Batch Loss: 2.0831  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5559/10240  Batch Loss: 2.0308  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5560/10240  Batch Loss: 1.6433  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5561/10240  Batch Loss: 0.7078  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5562/10240  Batch Loss: 1.0921  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5563/10240  Batch Loss: 0.8916  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5564/10240  Batch Loss: 1.9995  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5565/10240  Batch Loss: 0.8743  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5566/10240  Batch Loss: 0.9589  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5567/10240  Batch Loss: 1.1556  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5568/10240  Batch Loss: 2.3889  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5569/10240  Batch Loss: 1.6003  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5570/10240  Batch Loss: 0.7700  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5571/10240  Batch Loss: 2.7309  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5572/10240  Batch Loss: 1.3962  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5573/10240  Batch Loss: 1.2597  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5574/10240  Batch Loss: 1.3009  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5575/10240  Batch Loss: 1.2145  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5576/10240  Batch Loss: 1.5675  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5577/10240  Batch Loss: 1.5403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5578/10240  Batch Loss: 0.9956  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5579/10240  Batch Loss: 1.2084  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5580/10240  Batch Loss: 0.9658  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5581/10240  Batch Loss: 1.4408  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5582/10240  Batch Loss: 1.6382  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5583/10240  Batch Loss: 1.8691  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5584/10240  Batch Loss: 2.0470  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5585/10240  Batch Loss: 0.7201  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5586/10240  Batch Loss: 1.9290  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5587/10240  Batch Loss: 0.7314  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5588/10240  Batch Loss: 1.9228  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5928\n",
      "Epoch 001  Batch 5589/10240  Batch Loss: 2.5263  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5927\n",
      "Epoch 001  Batch 5590/10240  Batch Loss: 1.0832  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5591/10240  Batch Loss: 1.8954  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5592/10240  Batch Loss: 1.0827  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5593/10240  Batch Loss: 1.1883  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5594/10240  Batch Loss: 1.4101  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5595/10240  Batch Loss: 1.1986  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5596/10240  Batch Loss: 0.7428  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5597/10240  Batch Loss: 1.2960  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5598/10240  Batch Loss: 1.3922  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5599/10240  Batch Loss: 0.8477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5600/10240  Batch Loss: 2.2707  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5601/10240  Batch Loss: 0.7189  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5602/10240  Batch Loss: 0.7898  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5603/10240  Batch Loss: 1.5131  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5604/10240  Batch Loss: 0.8838  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5926\n",
      "Epoch 001  Batch 5605/10240  Batch Loss: 1.4106  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5606/10240  Batch Loss: 1.3973  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5607/10240  Batch Loss: 1.1347  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5608/10240  Batch Loss: 2.5347  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5609/10240  Batch Loss: 1.4013  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5610/10240  Batch Loss: 2.6854  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5924\n",
      "Epoch 001  Batch 5611/10240  Batch Loss: 2.2378  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5612/10240  Batch Loss: 0.9248  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5613/10240  Batch Loss: 1.7468  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5925\n",
      "Epoch 001  Batch 5614/10240  Batch Loss: 1.7350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5924\n",
      "Epoch 001  Batch 5615/10240  Batch Loss: 2.0090  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5923\n",
      "Epoch 001  Batch 5616/10240  Batch Loss: 1.4423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5924\n",
      "Epoch 001  Batch 5617/10240  Batch Loss: 0.8231  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5924\n",
      "Epoch 001  Batch 5618/10240  Batch Loss: 0.7555  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5924\n",
      "Epoch 001  Batch 5619/10240  Batch Loss: 1.4872  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5923\n",
      "Epoch 001  Batch 5620/10240  Batch Loss: 2.0501  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5923\n",
      "Epoch 001  Batch 5621/10240  Batch Loss: 1.5486  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5923\n",
      "Epoch 001  Batch 5622/10240  Batch Loss: 0.7935  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5923\n",
      "Epoch 001  Batch 5623/10240  Batch Loss: 1.4169  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5923\n",
      "Epoch 001  Batch 5624/10240  Batch Loss: 1.5312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5625/10240  Batch Loss: 1.4477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5626/10240  Batch Loss: 1.6568  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5627/10240  Batch Loss: 1.4029  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5628/10240  Batch Loss: 1.1849  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5629/10240  Batch Loss: 1.2406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5630/10240  Batch Loss: 0.7852  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5631/10240  Batch Loss: 1.9135  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5632/10240  Batch Loss: 1.0765  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5633/10240  Batch Loss: 1.3251  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5921\n",
      "Epoch 001  Batch 5634/10240  Batch Loss: 2.4049  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5921\n",
      "Epoch 001  Batch 5635/10240  Batch Loss: 1.2203  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5922\n",
      "Epoch 001  Batch 5636/10240  Batch Loss: 1.5127  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5921\n",
      "Epoch 001  Batch 5637/10240  Batch Loss: 1.0466  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5921\n",
      "Epoch 001  Batch 5638/10240  Batch Loss: 0.7325  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5921\n",
      "Epoch 001  Batch 5639/10240  Batch Loss: 1.6328  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5921\n",
      "Epoch 001  Batch 5640/10240  Batch Loss: 1.2796  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5641/10240  Batch Loss: 0.7294  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5642/10240  Batch Loss: 1.4447  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5643/10240  Batch Loss: 1.8689  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5644/10240  Batch Loss: 2.1748  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5645/10240  Batch Loss: 0.7317  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5646/10240  Batch Loss: 1.5383  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5647/10240  Batch Loss: 1.3027  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5648/10240  Batch Loss: 1.9350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5649/10240  Batch Loss: 1.3936  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5650/10240  Batch Loss: 1.1383  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5651/10240  Batch Loss: 1.4981  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5652/10240  Batch Loss: 0.9067  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5653/10240  Batch Loss: 1.6357  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5654/10240  Batch Loss: 0.7851  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5655/10240  Batch Loss: 2.3300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5656/10240  Batch Loss: 0.7155  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5657/10240  Batch Loss: 1.3775  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5658/10240  Batch Loss: 1.2497  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5659/10240  Batch Loss: 1.2360  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5660/10240  Batch Loss: 0.9961  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5661/10240  Batch Loss: 1.4891  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5662/10240  Batch Loss: 1.1231  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5663/10240  Batch Loss: 1.0500  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5664/10240  Batch Loss: 1.7348  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5665/10240  Batch Loss: 1.1930  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5666/10240  Batch Loss: 2.0602  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5667/10240  Batch Loss: 0.8548  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5920\n",
      "Epoch 001  Batch 5668/10240  Batch Loss: 2.2208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5669/10240  Batch Loss: 0.7435  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5670/10240  Batch Loss: 1.0907  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5671/10240  Batch Loss: 1.9139  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5672/10240  Batch Loss: 1.6617  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5673/10240  Batch Loss: 2.4074  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5674/10240  Batch Loss: 1.1994  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5675/10240  Batch Loss: 1.8569  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5676/10240  Batch Loss: 2.2177  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5677/10240  Batch Loss: 1.3404  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5678/10240  Batch Loss: 1.5438  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5679/10240  Batch Loss: 0.7195  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5680/10240  Batch Loss: 1.4219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5681/10240  Batch Loss: 1.5752  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5682/10240  Batch Loss: 1.2136  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5683/10240  Batch Loss: 0.7138  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5919\n",
      "Epoch 001  Batch 5684/10240  Batch Loss: 1.8044  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5685/10240  Batch Loss: 1.6587  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5686/10240  Batch Loss: 1.8765  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5687/10240  Batch Loss: 1.2829  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5688/10240  Batch Loss: 2.7509  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5689/10240  Batch Loss: 1.1827  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5690/10240  Batch Loss: 1.8256  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5691/10240  Batch Loss: 0.9174  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5692/10240  Batch Loss: 1.1094  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5693/10240  Batch Loss: 1.4317  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5694/10240  Batch Loss: 0.7155  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5695/10240  Batch Loss: 0.7153  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5696/10240  Batch Loss: 0.7203  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5697/10240  Batch Loss: 1.7061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5698/10240  Batch Loss: 0.7801  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5699/10240  Batch Loss: 1.0154  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5700/10240  Batch Loss: 1.2244  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5701/10240  Batch Loss: 1.4477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5702/10240  Batch Loss: 1.8236  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5703/10240  Batch Loss: 1.1637  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5704/10240  Batch Loss: 1.4407  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5705/10240  Batch Loss: 2.4225  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5706/10240  Batch Loss: 1.1580  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5707/10240  Batch Loss: 2.3089  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5708/10240  Batch Loss: 1.0442  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5709/10240  Batch Loss: 0.9060  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5710/10240  Batch Loss: 2.1674  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5711/10240  Batch Loss: 2.2453  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5712/10240  Batch Loss: 1.6236  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5713/10240  Batch Loss: 0.9928  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5714/10240  Batch Loss: 0.7267  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5715/10240  Batch Loss: 2.0212  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5716/10240  Batch Loss: 1.5578  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5717/10240  Batch Loss: 1.5275  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5718/10240  Batch Loss: 1.6336  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5719/10240  Batch Loss: 0.7218  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5720/10240  Batch Loss: 1.5429  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5721/10240  Batch Loss: 2.4253  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5722/10240  Batch Loss: 2.3465  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5723/10240  Batch Loss: 1.9722  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5724/10240  Batch Loss: 2.5329  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5725/10240  Batch Loss: 1.7665  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5726/10240  Batch Loss: 1.9305  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5727/10240  Batch Loss: 1.1420  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5728/10240  Batch Loss: 1.5990  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5729/10240  Batch Loss: 1.2319  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5730/10240  Batch Loss: 1.0300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5731/10240  Batch Loss: 1.7540  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5732/10240  Batch Loss: 1.9089  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5733/10240  Batch Loss: 0.7503  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5734/10240  Batch Loss: 1.2029  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5735/10240  Batch Loss: 1.1487  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5736/10240  Batch Loss: 1.1364  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5737/10240  Batch Loss: 2.2983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5738/10240  Batch Loss: 1.1895  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5739/10240  Batch Loss: 1.6176  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5740/10240  Batch Loss: 1.9894  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5741/10240  Batch Loss: 1.1765  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5742/10240  Batch Loss: 0.7443  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5743/10240  Batch Loss: 1.6963  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5744/10240  Batch Loss: 1.6233  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5745/10240  Batch Loss: 0.9395  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5746/10240  Batch Loss: 1.0983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5747/10240  Batch Loss: 1.0613  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5748/10240  Batch Loss: 1.4292  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5918\n",
      "Epoch 001  Batch 5749/10240  Batch Loss: 1.9818  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5750/10240  Batch Loss: 1.4423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5751/10240  Batch Loss: 2.3815  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5752/10240  Batch Loss: 1.2967  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5753/10240  Batch Loss: 0.7259  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5754/10240  Batch Loss: 1.7365  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5917\n",
      "Epoch 001  Batch 5755/10240  Batch Loss: 1.3435  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5756/10240  Batch Loss: 1.2068  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5757/10240  Batch Loss: 1.1643  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5758/10240  Batch Loss: 0.7210  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5759/10240  Batch Loss: 2.7453  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5760/10240  Batch Loss: 0.8110  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5916\n",
      "Epoch 001  Batch 5761/10240  Batch Loss: 1.5742  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5915\n",
      "Epoch 001  Batch 5762/10240  Batch Loss: 1.9129  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5915\n",
      "Epoch 001  Batch 5763/10240  Batch Loss: 1.1851  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5915\n",
      "Epoch 001  Batch 5764/10240  Batch Loss: 1.4346  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5765/10240  Batch Loss: 0.8219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5766/10240  Batch Loss: 1.3908  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5767/10240  Batch Loss: 1.2968  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5915\n",
      "Epoch 001  Batch 5768/10240  Batch Loss: 1.1413  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5769/10240  Batch Loss: 2.3168  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5770/10240  Batch Loss: 1.3489  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5771/10240  Batch Loss: 1.7675  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5772/10240  Batch Loss: 1.1451  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5773/10240  Batch Loss: 0.7687  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5774/10240  Batch Loss: 1.2035  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5775/10240  Batch Loss: 1.8152  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5914\n",
      "Epoch 001  Batch 5776/10240  Batch Loss: 2.5918  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5913\n",
      "Epoch 001  Batch 5777/10240  Batch Loss: 1.7612  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5913\n",
      "Epoch 001  Batch 5778/10240  Batch Loss: 2.0535  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5913\n",
      "Epoch 001  Batch 5779/10240  Batch Loss: 1.8127  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5780/10240  Batch Loss: 1.6728  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5781/10240  Batch Loss: 0.9891  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5782/10240  Batch Loss: 1.0791  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5783/10240  Batch Loss: 0.7341  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5784/10240  Batch Loss: 1.6853  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5785/10240  Batch Loss: 1.4867  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5786/10240  Batch Loss: 1.6694  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5787/10240  Batch Loss: 1.8832  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5788/10240  Batch Loss: 0.8497  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5789/10240  Batch Loss: 1.1191  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5790/10240  Batch Loss: 1.1583  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5791/10240  Batch Loss: 3.3339  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5792/10240  Batch Loss: 2.2505  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5793/10240  Batch Loss: 1.5571  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5794/10240  Batch Loss: 1.7004  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5795/10240  Batch Loss: 1.6742  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5796/10240  Batch Loss: 1.4143  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5797/10240  Batch Loss: 0.7916  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5798/10240  Batch Loss: 0.7386  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5799/10240  Batch Loss: 1.6698  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5800/10240  Batch Loss: 1.8414  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5912\n",
      "Epoch 001  Batch 5801/10240  Batch Loss: 1.3687  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5802/10240  Batch Loss: 0.9095  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5803/10240  Batch Loss: 1.8238  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5804/10240  Batch Loss: 0.9405  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5805/10240  Batch Loss: 0.9785  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5806/10240  Batch Loss: 1.5949  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5807/10240  Batch Loss: 1.2376  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5808/10240  Batch Loss: 1.1543  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5809/10240  Batch Loss: 0.9478  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5810/10240  Batch Loss: 0.7379  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5811/10240  Batch Loss: 1.1268  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5812/10240  Batch Loss: 1.6273  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5813/10240  Batch Loss: 1.5607  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5814/10240  Batch Loss: 0.9646  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5815/10240  Batch Loss: 1.1315  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5816/10240  Batch Loss: 1.1288  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5817/10240  Batch Loss: 0.8183  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5818/10240  Batch Loss: 0.9751  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5819/10240  Batch Loss: 1.5933  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5820/10240  Batch Loss: 2.1295  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5821/10240  Batch Loss: 1.0176  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5822/10240  Batch Loss: 1.3219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5823/10240  Batch Loss: 2.0607  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5824/10240  Batch Loss: 0.8349  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5825/10240  Batch Loss: 1.0716  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5826/10240  Batch Loss: 2.0122  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5827/10240  Batch Loss: 1.7637  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5828/10240  Batch Loss: 1.1389  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5829/10240  Batch Loss: 1.3693  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5830/10240  Batch Loss: 1.9927  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5831/10240  Batch Loss: 1.4020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5832/10240  Batch Loss: 2.0482  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5833/10240  Batch Loss: 2.6898  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5834/10240  Batch Loss: 0.9981  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5835/10240  Batch Loss: 2.0457  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5836/10240  Batch Loss: 1.3315  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5837/10240  Batch Loss: 1.1158  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5838/10240  Batch Loss: 1.5384  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5839/10240  Batch Loss: 1.4908  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5840/10240  Batch Loss: 1.9964  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5841/10240  Batch Loss: 1.5717  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5842/10240  Batch Loss: 1.2692  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5843/10240  Batch Loss: 0.9661  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5844/10240  Batch Loss: 2.4305  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5845/10240  Batch Loss: 1.3295  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5846/10240  Batch Loss: 1.2122  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5847/10240  Batch Loss: 1.1775  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5848/10240  Batch Loss: 1.0983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5849/10240  Batch Loss: 1.9949  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5850/10240  Batch Loss: 1.4287  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5851/10240  Batch Loss: 1.1359  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5852/10240  Batch Loss: 1.2563  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5853/10240  Batch Loss: 1.1071  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5854/10240  Batch Loss: 1.4539  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5855/10240  Batch Loss: 1.5056  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5856/10240  Batch Loss: 1.7318  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5857/10240  Batch Loss: 0.9725  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5858/10240  Batch Loss: 1.8968  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5859/10240  Batch Loss: 1.3059  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5860/10240  Batch Loss: 0.7350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5861/10240  Batch Loss: 1.7352  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5862/10240  Batch Loss: 1.1115  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5863/10240  Batch Loss: 0.7408  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5864/10240  Batch Loss: 1.8362  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5865/10240  Batch Loss: 1.4907  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5866/10240  Batch Loss: 1.6236  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5867/10240  Batch Loss: 0.8527  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5868/10240  Batch Loss: 2.5728  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5869/10240  Batch Loss: 0.7434  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5870/10240  Batch Loss: 1.1608  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5871/10240  Batch Loss: 1.0390  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5872/10240  Batch Loss: 1.1793  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5873/10240  Batch Loss: 3.2013  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5874/10240  Batch Loss: 1.2377  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5875/10240  Batch Loss: 1.3406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5876/10240  Batch Loss: 1.1415  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5877/10240  Batch Loss: 0.7523  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5878/10240  Batch Loss: 1.0793  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5879/10240  Batch Loss: 1.8325  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5880/10240  Batch Loss: 0.9900  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5881/10240  Batch Loss: 1.4185  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5882/10240  Batch Loss: 0.7310  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5883/10240  Batch Loss: 1.1427  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5884/10240  Batch Loss: 0.8572  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5885/10240  Batch Loss: 1.4423  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5886/10240  Batch Loss: 1.1263  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5887/10240  Batch Loss: 1.6063  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5888/10240  Batch Loss: 1.1978  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5889/10240  Batch Loss: 1.0074  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5890/10240  Batch Loss: 2.4816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5891/10240  Batch Loss: 1.9219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5911\n",
      "Epoch 001  Batch 5892/10240  Batch Loss: 1.1851  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5893/10240  Batch Loss: 1.3406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5894/10240  Batch Loss: 0.7297  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5895/10240  Batch Loss: 0.8354  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5896/10240  Batch Loss: 1.3234  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5897/10240  Batch Loss: 1.0365  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5898/10240  Batch Loss: 0.8753  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5899/10240  Batch Loss: 1.2986  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5900/10240  Batch Loss: 1.1854  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5901/10240  Batch Loss: 1.3329  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5902/10240  Batch Loss: 1.5312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5903/10240  Batch Loss: 0.7526  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5904/10240  Batch Loss: 1.9959  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5905/10240  Batch Loss: 1.6442  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5906/10240  Batch Loss: 1.7524  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5907/10240  Batch Loss: 1.1732  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5908/10240  Batch Loss: 0.8456  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5909/10240  Batch Loss: 2.5655  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5910/10240  Batch Loss: 1.1684  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5911/10240  Batch Loss: 0.9603  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5912/10240  Batch Loss: 1.4690  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5913/10240  Batch Loss: 1.7701  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5914/10240  Batch Loss: 1.6276  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5915/10240  Batch Loss: 1.1308  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5916/10240  Batch Loss: 2.2145  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5917/10240  Batch Loss: 1.7909  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5910\n",
      "Epoch 001  Batch 5918/10240  Batch Loss: 1.9259  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5919/10240  Batch Loss: 1.1494  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5920/10240  Batch Loss: 1.1076  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5921/10240  Batch Loss: 1.3907  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5922/10240  Batch Loss: 1.1874  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5923/10240  Batch Loss: 2.4052  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5924/10240  Batch Loss: 0.9543  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5925/10240  Batch Loss: 1.2570  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5926/10240  Batch Loss: 0.8684  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5927/10240  Batch Loss: 0.7477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5928/10240  Batch Loss: 1.6115  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5929/10240  Batch Loss: 0.8551  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5930/10240  Batch Loss: 1.4773  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5931/10240  Batch Loss: 1.4838  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5932/10240  Batch Loss: 1.1802  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5933/10240  Batch Loss: 0.7187  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5934/10240  Batch Loss: 1.0990  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5935/10240  Batch Loss: 1.6640  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5909\n",
      "Epoch 001  Batch 5936/10240  Batch Loss: 1.7922  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5937/10240  Batch Loss: 1.0942  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5938/10240  Batch Loss: 0.7067  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5939/10240  Batch Loss: 1.7814  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5940/10240  Batch Loss: 0.7869  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5941/10240  Batch Loss: 1.3669  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5942/10240  Batch Loss: 0.8719  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5943/10240  Batch Loss: 0.7197  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5944/10240  Batch Loss: 3.9663  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5945/10240  Batch Loss: 1.8635  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5946/10240  Batch Loss: 1.1012  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5947/10240  Batch Loss: 2.0049  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5948/10240  Batch Loss: 0.7377  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5949/10240  Batch Loss: 1.5357  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5950/10240  Batch Loss: 2.8708  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5951/10240  Batch Loss: 0.7587  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5952/10240  Batch Loss: 1.6786  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5953/10240  Batch Loss: 0.9677  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5954/10240  Batch Loss: 2.0061  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5955/10240  Batch Loss: 0.9402  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5956/10240  Batch Loss: 0.7016  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5957/10240  Batch Loss: 2.1031  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5958/10240  Batch Loss: 1.3242  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5959/10240  Batch Loss: 1.1406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5960/10240  Batch Loss: 1.1113  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5961/10240  Batch Loss: 2.2593  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5962/10240  Batch Loss: 1.1308  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5963/10240  Batch Loss: 1.4477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5964/10240  Batch Loss: 0.9351  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5965/10240  Batch Loss: 1.0638  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5966/10240  Batch Loss: 1.0632  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5908\n",
      "Epoch 001  Batch 5967/10240  Batch Loss: 0.8832  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5968/10240  Batch Loss: 1.0383  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5969/10240  Batch Loss: 1.1617  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5970/10240  Batch Loss: 2.1376  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5971/10240  Batch Loss: 2.3969  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5972/10240  Batch Loss: 1.3670  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5973/10240  Batch Loss: 1.3414  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5974/10240  Batch Loss: 1.3839  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5975/10240  Batch Loss: 1.3736  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5976/10240  Batch Loss: 1.6208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5977/10240  Batch Loss: 1.8323  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5978/10240  Batch Loss: 0.7885  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5979/10240  Batch Loss: 0.7233  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5980/10240  Batch Loss: 1.1932  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5981/10240  Batch Loss: 0.7251  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5982/10240  Batch Loss: 0.7074  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 5983/10240  Batch Loss: 1.4219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5984/10240  Batch Loss: 1.2303  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5985/10240  Batch Loss: 1.3179  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5986/10240  Batch Loss: 0.7205  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5987/10240  Batch Loss: 2.2215  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5988/10240  Batch Loss: 1.8270  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5989/10240  Batch Loss: 1.3495  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5990/10240  Batch Loss: 2.3367  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5991/10240  Batch Loss: 1.2114  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5992/10240  Batch Loss: 1.4035  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5993/10240  Batch Loss: 1.3190  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5994/10240  Batch Loss: 1.0048  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5995/10240  Batch Loss: 1.6995  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5996/10240  Batch Loss: 1.3409  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5997/10240  Batch Loss: 1.6595  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5998/10240  Batch Loss: 0.7197  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 5999/10240  Batch Loss: 1.2737  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 6000/10240  Batch Loss: 1.4530  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 6001/10240  Batch Loss: 1.7116  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 6002/10240  Batch Loss: 1.3017  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 6003/10240  Batch Loss: 0.7184  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 6004/10240  Batch Loss: 1.5928  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6005/10240  Batch Loss: 1.6122  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6006/10240  Batch Loss: 2.0853  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5907\n",
      "Epoch 001  Batch 6007/10240  Batch Loss: 1.7767  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6008/10240  Batch Loss: 1.2615  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6009/10240  Batch Loss: 1.0851  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6010/10240  Batch Loss: 1.8001  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6011/10240  Batch Loss: 0.7177  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6012/10240  Batch Loss: 1.7028  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6013/10240  Batch Loss: 1.2617  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6014/10240  Batch Loss: 1.1849  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6015/10240  Batch Loss: 0.9264  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6016/10240  Batch Loss: 0.7381  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6017/10240  Batch Loss: 0.9779  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5906\n",
      "Epoch 001  Batch 6018/10240  Batch Loss: 2.5175  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6019/10240  Batch Loss: 1.9510  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6020/10240  Batch Loss: 1.4514  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6021/10240  Batch Loss: 0.7862  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6022/10240  Batch Loss: 1.6407  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6023/10240  Batch Loss: 1.5701  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6024/10240  Batch Loss: 1.2369  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6025/10240  Batch Loss: 2.0644  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6026/10240  Batch Loss: 1.7501  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6027/10240  Batch Loss: 0.7033  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6028/10240  Batch Loss: 2.0439  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6029/10240  Batch Loss: 0.9581  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6030/10240  Batch Loss: 0.8739  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6031/10240  Batch Loss: 1.7747  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6032/10240  Batch Loss: 0.7295  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6033/10240  Batch Loss: 0.9055  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6034/10240  Batch Loss: 0.9731  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6035/10240  Batch Loss: 2.1002  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6036/10240  Batch Loss: 1.5324  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6037/10240  Batch Loss: 0.7158  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5905\n",
      "Epoch 001  Batch 6038/10240  Batch Loss: 1.4018  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6039/10240  Batch Loss: 1.4761  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6040/10240  Batch Loss: 1.7613  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6041/10240  Batch Loss: 1.2452  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6042/10240  Batch Loss: 1.6419  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6043/10240  Batch Loss: 0.7896  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6044/10240  Batch Loss: 1.5815  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6045/10240  Batch Loss: 1.0223  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6046/10240  Batch Loss: 2.3595  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6047/10240  Batch Loss: 1.1755  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5904\n",
      "Epoch 001  Batch 6048/10240  Batch Loss: 2.7441  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6049/10240  Batch Loss: 0.7054  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6050/10240  Batch Loss: 0.9261  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6051/10240  Batch Loss: 1.0220  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6052/10240  Batch Loss: 1.4933  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6053/10240  Batch Loss: 2.3499  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6054/10240  Batch Loss: 1.7112  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6055/10240  Batch Loss: 1.5947  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6056/10240  Batch Loss: 2.2648  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6057/10240  Batch Loss: 1.0808  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6058/10240  Batch Loss: 1.4322  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6059/10240  Batch Loss: 0.9141  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6060/10240  Batch Loss: 2.2010  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6061/10240  Batch Loss: 1.8403  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6062/10240  Batch Loss: 0.8852  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6063/10240  Batch Loss: 0.9312  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6064/10240  Batch Loss: 2.2480  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6065/10240  Batch Loss: 1.4107  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5903\n",
      "Epoch 001  Batch 6066/10240  Batch Loss: 1.7555  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6067/10240  Batch Loss: 0.9150  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6068/10240  Batch Loss: 1.3873  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6069/10240  Batch Loss: 2.6555  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6070/10240  Batch Loss: 2.4116  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6071/10240  Batch Loss: 0.7217  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6072/10240  Batch Loss: 1.3058  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6073/10240  Batch Loss: 1.3553  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6074/10240  Batch Loss: 0.8006  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6075/10240  Batch Loss: 1.4243  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6076/10240  Batch Loss: 1.2350  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6077/10240  Batch Loss: 1.2001  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5902\n",
      "Epoch 001  Batch 6078/10240  Batch Loss: 1.2380  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6079/10240  Batch Loss: 1.7134  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6080/10240  Batch Loss: 1.5190  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6081/10240  Batch Loss: 1.5805  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6082/10240  Batch Loss: 0.9087  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6083/10240  Batch Loss: 1.6178  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5901\n",
      "Epoch 001  Batch 6084/10240  Batch Loss: 2.2546  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6085/10240  Batch Loss: 1.1858  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6086/10240  Batch Loss: 1.6845  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6087/10240  Batch Loss: 0.7168  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6088/10240  Batch Loss: 1.2052  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6089/10240  Batch Loss: 1.1221  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6090/10240  Batch Loss: 1.3310  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6091/10240  Batch Loss: 1.5837  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6092/10240  Batch Loss: 2.1348  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6093/10240  Batch Loss: 2.5656  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6094/10240  Batch Loss: 1.6862  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5900\n",
      "Epoch 001  Batch 6095/10240  Batch Loss: 3.0838  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6096/10240  Batch Loss: 0.7732  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6097/10240  Batch Loss: 1.8889  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6098/10240  Batch Loss: 1.2136  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6099/10240  Batch Loss: 1.7934  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6100/10240  Batch Loss: 2.0147  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6101/10240  Batch Loss: 1.2380  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6102/10240  Batch Loss: 0.7031  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6103/10240  Batch Loss: 1.6005  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6104/10240  Batch Loss: 1.8159  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6105/10240  Batch Loss: 1.5531  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6106/10240  Batch Loss: 1.2161  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6107/10240  Batch Loss: 1.2491  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6108/10240  Batch Loss: 1.5624  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6109/10240  Batch Loss: 1.5114  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6110/10240  Batch Loss: 1.7645  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6111/10240  Batch Loss: 0.7137  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6112/10240  Batch Loss: 0.7223  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6113/10240  Batch Loss: 1.3988  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6114/10240  Batch Loss: 0.8017  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6115/10240  Batch Loss: 1.9778  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6116/10240  Batch Loss: 1.6560  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6117/10240  Batch Loss: 1.5367  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6118/10240  Batch Loss: 1.1994  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6119/10240  Batch Loss: 1.5118  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6120/10240  Batch Loss: 0.9034  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6121/10240  Batch Loss: 1.3021  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6122/10240  Batch Loss: 0.7214  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6123/10240  Batch Loss: 1.2574  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6124/10240  Batch Loss: 2.0439  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6125/10240  Batch Loss: 1.0332  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5899\n",
      "Epoch 001  Batch 6126/10240  Batch Loss: 1.8357  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6127/10240  Batch Loss: 1.5359  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6128/10240  Batch Loss: 1.4643  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6129/10240  Batch Loss: 1.7390  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6130/10240  Batch Loss: 1.8123  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6131/10240  Batch Loss: 1.5033  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6132/10240  Batch Loss: 0.9581  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6133/10240  Batch Loss: 0.9705  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6134/10240  Batch Loss: 0.7272  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6135/10240  Batch Loss: 1.1463  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5898\n",
      "Epoch 001  Batch 6136/10240  Batch Loss: 1.4663  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5897\n",
      "Epoch 001  Batch 6137/10240  Batch Loss: 1.5183  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5897\n",
      "Epoch 001  Batch 6138/10240  Batch Loss: 2.1417  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6139/10240  Batch Loss: 2.5232  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6140/10240  Batch Loss: 1.8573  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6141/10240  Batch Loss: 1.4038  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6142/10240  Batch Loss: 0.7104  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6143/10240  Batch Loss: 2.0453  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6144/10240  Batch Loss: 2.4137  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6145/10240  Batch Loss: 1.4661  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6146/10240  Batch Loss: 3.0497  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6147/10240  Batch Loss: 1.0185  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6148/10240  Batch Loss: 1.5179  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6149/10240  Batch Loss: 0.9992  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6150/10240  Batch Loss: 1.4860  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6151/10240  Batch Loss: 0.7418  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6152/10240  Batch Loss: 0.7300  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6153/10240  Batch Loss: 1.2319  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6154/10240  Batch Loss: 2.1601  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6155/10240  Batch Loss: 1.6224  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6156/10240  Batch Loss: 0.7328  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6157/10240  Batch Loss: 1.8996  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6158/10240  Batch Loss: 1.9566  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6159/10240  Batch Loss: 1.9690  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6160/10240  Batch Loss: 0.8085  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5896\n",
      "Epoch 001  Batch 6161/10240  Batch Loss: 2.2829  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6162/10240  Batch Loss: 1.2345  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6163/10240  Batch Loss: 0.9971  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6164/10240  Batch Loss: 0.9060  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6165/10240  Batch Loss: 1.4430  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6166/10240  Batch Loss: 1.4792  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6167/10240  Batch Loss: 0.9513  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6168/10240  Batch Loss: 0.7431  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6169/10240  Batch Loss: 1.5167  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6170/10240  Batch Loss: 1.1894  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6171/10240  Batch Loss: 1.5021  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6172/10240  Batch Loss: 1.3521  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6173/10240  Batch Loss: 1.4159  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6174/10240  Batch Loss: 1.1508  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6175/10240  Batch Loss: 0.7214  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6176/10240  Batch Loss: 1.4929  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6177/10240  Batch Loss: 1.4937  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6178/10240  Batch Loss: 1.2924  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6179/10240  Batch Loss: 0.8195  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6180/10240  Batch Loss: 0.8410  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6181/10240  Batch Loss: 1.2924  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6182/10240  Batch Loss: 1.9450  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5895\n",
      "Epoch 001  Batch 6183/10240  Batch Loss: 1.3636  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6184/10240  Batch Loss: 1.7827  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6185/10240  Batch Loss: 1.6377  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6186/10240  Batch Loss: 1.8459  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6187/10240  Batch Loss: 1.7474  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6188/10240  Batch Loss: 0.8279  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6189/10240  Batch Loss: 1.4163  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6190/10240  Batch Loss: 1.4199  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6191/10240  Batch Loss: 1.3437  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6192/10240  Batch Loss: 1.8728  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6193/10240  Batch Loss: 1.4199  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6194/10240  Batch Loss: 1.5691  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6195/10240  Batch Loss: 1.2075  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6196/10240  Batch Loss: 1.3720  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6197/10240  Batch Loss: 1.3941  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6198/10240  Batch Loss: 1.7430  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6199/10240  Batch Loss: 0.8460  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6200/10240  Batch Loss: 1.2038  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6201/10240  Batch Loss: 0.8695  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5894\n",
      "Epoch 001  Batch 6202/10240  Batch Loss: 1.7927  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6203/10240  Batch Loss: 2.8411  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6204/10240  Batch Loss: 1.0379  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6205/10240  Batch Loss: 1.3053  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6206/10240  Batch Loss: 1.7133  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6207/10240  Batch Loss: 2.1317  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6208/10240  Batch Loss: 1.2844  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6209/10240  Batch Loss: 1.7264  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6210/10240  Batch Loss: 1.2909  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6211/10240  Batch Loss: 2.2829  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6212/10240  Batch Loss: 0.7207  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6213/10240  Batch Loss: 1.6508  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6214/10240  Batch Loss: 1.6073  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6215/10240  Batch Loss: 1.3375  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6216/10240  Batch Loss: 1.3900  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6217/10240  Batch Loss: 1.0208  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6218/10240  Batch Loss: 0.8563  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6219/10240  Batch Loss: 1.0329  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5893\n",
      "Epoch 001  Batch 6220/10240  Batch Loss: 1.7018  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6221/10240  Batch Loss: 1.1227  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6222/10240  Batch Loss: 1.1125  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6223/10240  Batch Loss: 0.8082  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6224/10240  Batch Loss: 0.8816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6225/10240  Batch Loss: 1.2076  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6226/10240  Batch Loss: 1.8015  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6227/10240  Batch Loss: 2.6530  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6228/10240  Batch Loss: 1.9275  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6229/10240  Batch Loss: 2.2555  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6230/10240  Batch Loss: 1.7126  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6231/10240  Batch Loss: 1.1335  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6232/10240  Batch Loss: 1.3941  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6233/10240  Batch Loss: 1.1533  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6234/10240  Batch Loss: 1.2713  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6235/10240  Batch Loss: 0.7242  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6236/10240  Batch Loss: 1.8404  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6237/10240  Batch Loss: 1.6570  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6238/10240  Batch Loss: 0.8307  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6239/10240  Batch Loss: 1.2596  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6240/10240  Batch Loss: 0.7355  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6241/10240  Batch Loss: 1.1044  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6242/10240  Batch Loss: 1.6019  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6243/10240  Batch Loss: 1.1981  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6244/10240  Batch Loss: 1.7442  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6245/10240  Batch Loss: 0.9776  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6246/10240  Batch Loss: 0.7948  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6247/10240  Batch Loss: 1.6457  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6248/10240  Batch Loss: 0.9904  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6249/10240  Batch Loss: 1.0803  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6250/10240  Batch Loss: 1.0093  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6251/10240  Batch Loss: 1.2394  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6252/10240  Batch Loss: 0.8444  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6253/10240  Batch Loss: 1.5278  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5892\n",
      "Epoch 001  Batch 6254/10240  Batch Loss: 2.6177  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6255/10240  Batch Loss: 1.0967  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6256/10240  Batch Loss: 1.2983  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6257/10240  Batch Loss: 1.2236  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6258/10240  Batch Loss: 1.6649  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5891\n",
      "Epoch 001  Batch 6259/10240  Batch Loss: 2.6498  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6260/10240  Batch Loss: 1.1137  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6261/10240  Batch Loss: 1.2657  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6262/10240  Batch Loss: 1.5639  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6263/10240  Batch Loss: 1.6794  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6264/10240  Batch Loss: 1.7010  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6265/10240  Batch Loss: 0.8999  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6266/10240  Batch Loss: 2.4462  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6267/10240  Batch Loss: 1.7253  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6268/10240  Batch Loss: 0.7218  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6269/10240  Batch Loss: 1.0621  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6270/10240  Batch Loss: 0.7531  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6271/10240  Batch Loss: 0.8747  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6272/10240  Batch Loss: 1.5327  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5890\n",
      "Epoch 001  Batch 6273/10240  Batch Loss: 2.0126  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6274/10240  Batch Loss: 0.9762  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6275/10240  Batch Loss: 0.7810  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6276/10240  Batch Loss: 1.2656  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6277/10240  Batch Loss: 0.7665  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6278/10240  Batch Loss: 3.2617  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6279/10240  Batch Loss: 1.7730  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6280/10240  Batch Loss: 1.5247  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5889\n",
      "Epoch 001  Batch 6281/10240  Batch Loss: 0.9804  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6282/10240  Batch Loss: 0.8736  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6283/10240  Batch Loss: 1.0997  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6284/10240  Batch Loss: 0.8672  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6285/10240  Batch Loss: 1.3612  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6286/10240  Batch Loss: 2.0923  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6287/10240  Batch Loss: 2.5779  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6288/10240  Batch Loss: 1.2124  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6289/10240  Batch Loss: 1.6340  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6290/10240  Batch Loss: 0.7162  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6291/10240  Batch Loss: 1.1041  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6292/10240  Batch Loss: 1.8108  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6293/10240  Batch Loss: 0.7690  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6294/10240  Batch Loss: 1.8146  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6295/10240  Batch Loss: 2.1531  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6296/10240  Batch Loss: 0.9890  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6297/10240  Batch Loss: 1.1808  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5888\n",
      "Epoch 001  Batch 6298/10240  Batch Loss: 1.2088  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6299/10240  Batch Loss: 1.5219  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6300/10240  Batch Loss: 0.7507  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6301/10240  Batch Loss: 0.9031  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6302/10240  Batch Loss: 1.0662  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6303/10240  Batch Loss: 0.7143  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6304/10240  Batch Loss: 1.4146  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6305/10240  Batch Loss: 0.9071  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6306/10240  Batch Loss: 2.0476  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6307/10240  Batch Loss: 3.4181  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6308/10240  Batch Loss: 1.3410  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6309/10240  Batch Loss: 1.2000  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6310/10240  Batch Loss: 2.0816  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6311/10240  Batch Loss: 2.9564  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5887\n",
      "Epoch 001  Batch 6312/10240  Batch Loss: 1.9696  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6313/10240  Batch Loss: 0.7603  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6314/10240  Batch Loss: 0.7413  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6315/10240  Batch Loss: 0.7272  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6316/10240  Batch Loss: 1.7346  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6317/10240  Batch Loss: 1.2568  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6318/10240  Batch Loss: 2.1770  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6319/10240  Batch Loss: 1.4678  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6320/10240  Batch Loss: 2.2968  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6321/10240  Batch Loss: 1.6668  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6322/10240  Batch Loss: 1.3231  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6323/10240  Batch Loss: 1.3445  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6324/10240  Batch Loss: 2.1253  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6325/10240  Batch Loss: 1.0360  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6326/10240  Batch Loss: 1.5366  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6327/10240  Batch Loss: 2.0225  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6328/10240  Batch Loss: 1.3863  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6329/10240  Batch Loss: 1.4594  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6330/10240  Batch Loss: 0.8613  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6331/10240  Batch Loss: 1.9370  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6332/10240  Batch Loss: 0.7354  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6333/10240  Batch Loss: 1.9549  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6334/10240  Batch Loss: 1.7773  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6335/10240  Batch Loss: 1.1684  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6336/10240  Batch Loss: 1.3119  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6337/10240  Batch Loss: 0.7224  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6338/10240  Batch Loss: 1.3322  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6339/10240  Batch Loss: 1.9477  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6340/10240  Batch Loss: 2.1554  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6341/10240  Batch Loss: 3.4030  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6342/10240  Batch Loss: 1.7530  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6343/10240  Batch Loss: 0.7155  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6344/10240  Batch Loss: 0.9163  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6345/10240  Batch Loss: 1.3912  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6346/10240  Batch Loss: 2.1041  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6347/10240  Batch Loss: 2.2546  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6348/10240  Batch Loss: 1.4902  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6349/10240  Batch Loss: 1.4669  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6350/10240  Batch Loss: 2.3001  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6351/10240  Batch Loss: 1.2879  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6352/10240  Batch Loss: 0.7363  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6353/10240  Batch Loss: 1.8152  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6354/10240  Batch Loss: 1.6184  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6355/10240  Batch Loss: 2.0293  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6356/10240  Batch Loss: 2.7597  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6357/10240  Batch Loss: 1.8007  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6358/10240  Batch Loss: 2.3973  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6359/10240  Batch Loss: 1.2406  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6360/10240  Batch Loss: 0.7114  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6361/10240  Batch Loss: 1.5303  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6362/10240  Batch Loss: 2.0524  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6363/10240  Batch Loss: 1.5858  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6364/10240  Batch Loss: 1.0901  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6365/10240  Batch Loss: 0.7298  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6366/10240  Batch Loss: 1.8487  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6367/10240  Batch Loss: 1.0267  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6368/10240  Batch Loss: 1.0721  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6369/10240  Batch Loss: 2.6559  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6370/10240  Batch Loss: 1.4171  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6371/10240  Batch Loss: 0.7154  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6372/10240  Batch Loss: 0.8181  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6373/10240  Batch Loss: 0.7668  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6374/10240  Batch Loss: 1.0020  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6375/10240  Batch Loss: 1.7425  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6376/10240  Batch Loss: 2.6588  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6377/10240  Batch Loss: 1.9199  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6378/10240  Batch Loss: 0.7328  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6379/10240  Batch Loss: 1.2100  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6380/10240  Batch Loss: 1.8868  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6381/10240  Batch Loss: 1.6294  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6382/10240  Batch Loss: 1.3058  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6383/10240  Batch Loss: 1.0881  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6384/10240  Batch Loss: 1.4447  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6385/10240  Batch Loss: 0.7353  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6386/10240  Batch Loss: 1.7101  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6387/10240  Batch Loss: 1.3093  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6388/10240  Batch Loss: 1.1750  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6389/10240  Batch Loss: 1.7446  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6390/10240  Batch Loss: 2.5648  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6391/10240  Batch Loss: 1.5181  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6392/10240  Batch Loss: 1.0089  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6393/10240  Batch Loss: 2.3761  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6394/10240  Batch Loss: 1.8841  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6395/10240  Batch Loss: 1.2633  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6396/10240  Batch Loss: 1.3040  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6397/10240  Batch Loss: 1.7950  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6398/10240  Batch Loss: 1.1833  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6399/10240  Batch Loss: 0.7531  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6400/10240  Batch Loss: 1.4859  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6401/10240  Batch Loss: 1.8760  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6402/10240  Batch Loss: 1.1514  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6403/10240  Batch Loss: 2.1319  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6404/10240  Batch Loss: 1.0464  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6405/10240  Batch Loss: 1.6873  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6406/10240  Batch Loss: 1.5563  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6407/10240  Batch Loss: 0.9684  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6408/10240  Batch Loss: 0.8076  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6409/10240  Batch Loss: 1.4109  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6410/10240  Batch Loss: 1.7483  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6411/10240  Batch Loss: 1.2923  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6412/10240  Batch Loss: 0.9885  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6413/10240  Batch Loss: 2.2724  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6414/10240  Batch Loss: 2.0829  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6415/10240  Batch Loss: 1.3246  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6416/10240  Batch Loss: 1.9429  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5882\n",
      "Epoch 001  Batch 6417/10240  Batch Loss: 1.2649  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6418/10240  Batch Loss: 2.0640  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6419/10240  Batch Loss: 1.9191  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6420/10240  Batch Loss: 1.9053  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6421/10240  Batch Loss: 1.4925  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6422/10240  Batch Loss: 1.0254  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6423/10240  Batch Loss: 1.2450  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6424/10240  Batch Loss: 1.5327  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6425/10240  Batch Loss: 1.3104  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5883\n",
      "Epoch 001  Batch 6426/10240  Batch Loss: 2.4222  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6427/10240  Batch Loss: 0.8514  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6428/10240  Batch Loss: 1.2523  | train F1: 0.0072  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6429/10240  Batch Loss: 2.2310  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6430/10240  Batch Loss: 1.9338  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6431/10240  Batch Loss: 0.7198  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6432/10240  Batch Loss: 3.1348  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6433/10240  Batch Loss: 1.4949  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6434/10240  Batch Loss: 1.4628  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6435/10240  Batch Loss: 1.1355  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6436/10240  Batch Loss: 1.9514  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6437/10240  Batch Loss: 1.0539  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6438/10240  Batch Loss: 1.0964  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6439/10240  Batch Loss: 2.0785  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5884\n",
      "Epoch 001  Batch 6440/10240  Batch Loss: 1.1378  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5884\n",
      "Epoch 001  Batch 6441/10240  Batch Loss: 0.7242  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6442/10240  Batch Loss: 0.7374  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6443/10240  Batch Loss: 1.6343  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6444/10240  Batch Loss: 0.8612  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5884\n",
      "Epoch 001  Batch 6445/10240  Batch Loss: 1.2051  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6446/10240  Batch Loss: 1.4474  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6447/10240  Batch Loss: 1.2659  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6448/10240  Batch Loss: 1.5068  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6449/10240  Batch Loss: 1.0471  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6450/10240  Batch Loss: 1.3383  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6451/10240  Batch Loss: 1.9099  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6452/10240  Batch Loss: 0.8198  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6453/10240  Batch Loss: 0.7883  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6454/10240  Batch Loss: 3.1899  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6455/10240  Batch Loss: 1.4397  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6456/10240  Batch Loss: 1.4843  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6457/10240  Batch Loss: 1.2613  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6458/10240  Batch Loss: 1.9131  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6459/10240  Batch Loss: 2.4776  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6460/10240  Batch Loss: 1.8495  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6461/10240  Batch Loss: 0.9437  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6462/10240  Batch Loss: 1.0511  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6463/10240  Batch Loss: 2.5774  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6464/10240  Batch Loss: 2.0206  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5884\n",
      "Epoch 001  Batch 6465/10240  Batch Loss: 1.1205  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5884\n",
      "Epoch 001  Batch 6466/10240  Batch Loss: 1.0099  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5884\n",
      "Epoch 001  Batch 6467/10240  Batch Loss: 1.3202  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6468/10240  Batch Loss: 0.7902  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6469/10240  Batch Loss: 1.7681  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5884\n",
      "Epoch 001  Batch 6470/10240  Batch Loss: 1.4649  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6471/10240  Batch Loss: 1.1644  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6472/10240  Batch Loss: 0.7798  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6473/10240  Batch Loss: 2.4145  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6474/10240  Batch Loss: 1.5462  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6475/10240  Batch Loss: 1.5465  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6476/10240  Batch Loss: 1.5841  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6477/10240  Batch Loss: 2.0825  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6478/10240  Batch Loss: 1.0288  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6479/10240  Batch Loss: 1.2930  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6480/10240  Batch Loss: 0.9574  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6481/10240  Batch Loss: 1.3683  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6482/10240  Batch Loss: 0.8248  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6483/10240  Batch Loss: 1.1481  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6484/10240  Batch Loss: 0.7298  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5885\n",
      "Epoch 001  Batch 6485/10240  Batch Loss: 0.7272  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5885\n",
      "Epoch 001  Batch 6486/10240  Batch Loss: 1.3878  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6487/10240  Batch Loss: 1.3942  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6488/10240  Batch Loss: 1.6320  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6489/10240  Batch Loss: 1.3805  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6490/10240  Batch Loss: 1.2002  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6491/10240  Batch Loss: 1.5045  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6492/10240  Batch Loss: 1.8714  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6493/10240  Batch Loss: 1.1613  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6494/10240  Batch Loss: 2.0831  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6495/10240  Batch Loss: 0.9435  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6496/10240  Batch Loss: 0.9583  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6497/10240  Batch Loss: 1.3293  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6498/10240  Batch Loss: 1.2563  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6499/10240  Batch Loss: 1.4760  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6500/10240  Batch Loss: 1.8614  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6501/10240  Batch Loss: 1.0592  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6502/10240  Batch Loss: 0.7296  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6503/10240  Batch Loss: 1.6253  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6504/10240  Batch Loss: 2.1315  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6505/10240  Batch Loss: 2.0957  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6506/10240  Batch Loss: 0.7768  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6507/10240  Batch Loss: 1.8001  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6508/10240  Batch Loss: 0.7754  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6509/10240  Batch Loss: 1.1877  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6510/10240  Batch Loss: 1.4016  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6511/10240  Batch Loss: 0.7380  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6512/10240  Batch Loss: 2.1619  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6513/10240  Batch Loss: 1.4936  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6514/10240  Batch Loss: 1.1493  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6515/10240  Batch Loss: 0.7692  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6516/10240  Batch Loss: 1.0831  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6517/10240  Batch Loss: 1.3206  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6518/10240  Batch Loss: 0.7897  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6519/10240  Batch Loss: 0.9866  | train F1: 0.0073  | train precision: 0.0036  | train recall: 0.5886\n",
      "Epoch 001  Batch 6520/10240  Batch Loss: 1.5056  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6521/10240  Batch Loss: 1.4491  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6522/10240  Batch Loss: 1.7824  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6523/10240  Batch Loss: 1.1185  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6524/10240  Batch Loss: 1.6932  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6525/10240  Batch Loss: 2.1290  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6526/10240  Batch Loss: 0.9962  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6527/10240  Batch Loss: 1.6301  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6528/10240  Batch Loss: 2.3207  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6529/10240  Batch Loss: 1.9933  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6530/10240  Batch Loss: 1.5333  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6531/10240  Batch Loss: 1.0912  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6532/10240  Batch Loss: 2.4220  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6533/10240  Batch Loss: 1.1254  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6534/10240  Batch Loss: 2.2710  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6535/10240  Batch Loss: 2.1825  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6536/10240  Batch Loss: 1.5647  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6537/10240  Batch Loss: 0.7507  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6538/10240  Batch Loss: 0.9704  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6539/10240  Batch Loss: 1.0645  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6540/10240  Batch Loss: 0.7189  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6541/10240  Batch Loss: 1.3725  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6542/10240  Batch Loss: 2.2548  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6543/10240  Batch Loss: 1.7343  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6544/10240  Batch Loss: 0.8679  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6545/10240  Batch Loss: 0.7417  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6546/10240  Batch Loss: 1.1203  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6547/10240  Batch Loss: 0.9838  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6548/10240  Batch Loss: 2.5107  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6549/10240  Batch Loss: 1.2533  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6550/10240  Batch Loss: 1.1796  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6551/10240  Batch Loss: 0.7298  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6552/10240  Batch Loss: 1.4239  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6553/10240  Batch Loss: 1.4179  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6554/10240  Batch Loss: 1.1367  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6555/10240  Batch Loss: 0.7277  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6556/10240  Batch Loss: 2.0137  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6557/10240  Batch Loss: 1.0404  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6558/10240  Batch Loss: 1.5668  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6559/10240  Batch Loss: 0.9383  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6560/10240  Batch Loss: 1.4617  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6561/10240  Batch Loss: 1.3454  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6562/10240  Batch Loss: 2.0170  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6563/10240  Batch Loss: 0.7334  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6564/10240  Batch Loss: 2.6820  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6565/10240  Batch Loss: 2.3975  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6566/10240  Batch Loss: 1.8897  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6567/10240  Batch Loss: 1.1849  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6568/10240  Batch Loss: 1.2540  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6569/10240  Batch Loss: 1.9344  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6570/10240  Batch Loss: 1.1796  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6571/10240  Batch Loss: 1.2215  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6572/10240  Batch Loss: 0.9234  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6573/10240  Batch Loss: 1.5095  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6574/10240  Batch Loss: 0.7515  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6575/10240  Batch Loss: 1.4193  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6576/10240  Batch Loss: 2.4743  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6577/10240  Batch Loss: 1.4061  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6578/10240  Batch Loss: 0.8026  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6579/10240  Batch Loss: 2.2213  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6580/10240  Batch Loss: 1.5856  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6581/10240  Batch Loss: 1.3591  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6582/10240  Batch Loss: 1.6577  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6583/10240  Batch Loss: 1.8410  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6584/10240  Batch Loss: 1.8172  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6585/10240  Batch Loss: 1.7041  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6586/10240  Batch Loss: 1.5073  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6587/10240  Batch Loss: 2.1201  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6588/10240  Batch Loss: 1.3903  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6589/10240  Batch Loss: 2.7197  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6590/10240  Batch Loss: 0.9284  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6591/10240  Batch Loss: 1.3974  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6592/10240  Batch Loss: 1.3154  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6593/10240  Batch Loss: 2.5733  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6594/10240  Batch Loss: 2.4542  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6595/10240  Batch Loss: 1.6863  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6596/10240  Batch Loss: 1.7235  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6597/10240  Batch Loss: 1.2375  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6598/10240  Batch Loss: 1.2975  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6599/10240  Batch Loss: 1.4118  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6600/10240  Batch Loss: 1.1306  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6601/10240  Batch Loss: 1.1885  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6602/10240  Batch Loss: 2.6243  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6603/10240  Batch Loss: 0.7239  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6604/10240  Batch Loss: 2.4784  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6605/10240  Batch Loss: 1.5431  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6606/10240  Batch Loss: 2.7979  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6607/10240  Batch Loss: 1.2624  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6608/10240  Batch Loss: 2.0832  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6609/10240  Batch Loss: 0.9749  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6610/10240  Batch Loss: 1.5847  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6611/10240  Batch Loss: 0.8661  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6612/10240  Batch Loss: 2.6621  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6613/10240  Batch Loss: 1.9952  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6614/10240  Batch Loss: 1.2723  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6615/10240  Batch Loss: 0.7502  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6616/10240  Batch Loss: 0.7172  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6617/10240  Batch Loss: 0.8358  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6618/10240  Batch Loss: 1.9536  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6619/10240  Batch Loss: 1.1423  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6620/10240  Batch Loss: 1.9179  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6621/10240  Batch Loss: 1.3904  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6622/10240  Batch Loss: 1.0129  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6623/10240  Batch Loss: 1.3356  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6624/10240  Batch Loss: 1.0198  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6625/10240  Batch Loss: 1.4007  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6626/10240  Batch Loss: 0.7749  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6627/10240  Batch Loss: 0.7353  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6628/10240  Batch Loss: 1.7819  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6629/10240  Batch Loss: 1.2259  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6630/10240  Batch Loss: 2.9856  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6631/10240  Batch Loss: 1.0794  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6632/10240  Batch Loss: 1.1641  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6633/10240  Batch Loss: 0.7275  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6634/10240  Batch Loss: 1.5809  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6635/10240  Batch Loss: 0.7378  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6636/10240  Batch Loss: 1.0593  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6637/10240  Batch Loss: 0.9078  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6638/10240  Batch Loss: 1.2044  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6639/10240  Batch Loss: 1.4079  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6640/10240  Batch Loss: 1.6063  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6641/10240  Batch Loss: 1.2595  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6642/10240  Batch Loss: 0.8395  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6643/10240  Batch Loss: 1.4213  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6644/10240  Batch Loss: 1.6751  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6645/10240  Batch Loss: 1.1329  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6646/10240  Batch Loss: 0.7389  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6647/10240  Batch Loss: 1.5855  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6648/10240  Batch Loss: 1.9871  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6649/10240  Batch Loss: 1.4555  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6650/10240  Batch Loss: 1.3941  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6651/10240  Batch Loss: 1.1935  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6652/10240  Batch Loss: 1.3074  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6653/10240  Batch Loss: 0.7295  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6654/10240  Batch Loss: 0.9177  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6655/10240  Batch Loss: 1.6500  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6656/10240  Batch Loss: 1.0757  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6657/10240  Batch Loss: 1.2489  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6658/10240  Batch Loss: 2.0221  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6659/10240  Batch Loss: 1.0206  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6660/10240  Batch Loss: 1.0865  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6661/10240  Batch Loss: 1.6905  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6662/10240  Batch Loss: 0.7316  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6663/10240  Batch Loss: 1.9537  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6664/10240  Batch Loss: 1.1590  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6665/10240  Batch Loss: 1.6723  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6666/10240  Batch Loss: 0.9880  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6667/10240  Batch Loss: 0.8000  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6668/10240  Batch Loss: 2.0038  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6669/10240  Batch Loss: 0.8937  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6670/10240  Batch Loss: 0.8641  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6671/10240  Batch Loss: 1.3786  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5886\n",
      "Epoch 001  Batch 6672/10240  Batch Loss: 2.3849  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6673/10240  Batch Loss: 2.1201  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6674/10240  Batch Loss: 1.6491  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6675/10240  Batch Loss: 1.3724  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6676/10240  Batch Loss: 0.7215  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6677/10240  Batch Loss: 1.7118  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6678/10240  Batch Loss: 2.0987  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6679/10240  Batch Loss: 0.8553  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6680/10240  Batch Loss: 1.6238  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6681/10240  Batch Loss: 0.7633  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6682/10240  Batch Loss: 1.9562  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6683/10240  Batch Loss: 1.6270  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6684/10240  Batch Loss: 1.1350  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6685/10240  Batch Loss: 1.1252  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6686/10240  Batch Loss: 1.8354  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6687/10240  Batch Loss: 0.8388  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5887\n",
      "Epoch 001  Batch 6688/10240  Batch Loss: 1.4639  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6689/10240  Batch Loss: 2.8200  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6690/10240  Batch Loss: 2.3038  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6691/10240  Batch Loss: 1.2277  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6692/10240  Batch Loss: 1.4163  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6693/10240  Batch Loss: 1.5634  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6694/10240  Batch Loss: 1.0829  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6695/10240  Batch Loss: 1.3559  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6696/10240  Batch Loss: 1.2274  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6697/10240  Batch Loss: 1.1228  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6698/10240  Batch Loss: 1.4430  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6699/10240  Batch Loss: 1.9257  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6700/10240  Batch Loss: 1.2538  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6701/10240  Batch Loss: 0.8826  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6702/10240  Batch Loss: 0.8747  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6703/10240  Batch Loss: 1.0592  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6704/10240  Batch Loss: 1.8166  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6705/10240  Batch Loss: 1.3653  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6706/10240  Batch Loss: 1.1466  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6707/10240  Batch Loss: 0.8526  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6708/10240  Batch Loss: 1.7513  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6709/10240  Batch Loss: 2.7398  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6710/10240  Batch Loss: 0.8089  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6711/10240  Batch Loss: 0.7083  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6712/10240  Batch Loss: 2.6454  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6713/10240  Batch Loss: 2.2707  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6714/10240  Batch Loss: 1.9999  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6715/10240  Batch Loss: 1.7391  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6716/10240  Batch Loss: 1.3044  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6717/10240  Batch Loss: 0.7160  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6718/10240  Batch Loss: 0.9344  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6719/10240  Batch Loss: 1.3339  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6720/10240  Batch Loss: 1.2809  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6721/10240  Batch Loss: 0.7379  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6722/10240  Batch Loss: 1.4764  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6723/10240  Batch Loss: 0.8223  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6724/10240  Batch Loss: 1.5922  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6725/10240  Batch Loss: 0.8002  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6726/10240  Batch Loss: 1.4027  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6727/10240  Batch Loss: 1.1993  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6728/10240  Batch Loss: 2.0536  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6729/10240  Batch Loss: 0.8246  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6730/10240  Batch Loss: 0.7309  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6731/10240  Batch Loss: 0.8827  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6732/10240  Batch Loss: 2.5102  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6733/10240  Batch Loss: 2.0733  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6734/10240  Batch Loss: 1.8374  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6735/10240  Batch Loss: 1.3083  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6736/10240  Batch Loss: 1.7865  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6737/10240  Batch Loss: 2.1642  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6738/10240  Batch Loss: 0.7024  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6739/10240  Batch Loss: 1.0721  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6740/10240  Batch Loss: 0.7006  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6741/10240  Batch Loss: 0.7118  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6742/10240  Batch Loss: 2.0212  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6743/10240  Batch Loss: 1.2065  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6744/10240  Batch Loss: 0.7648  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6745/10240  Batch Loss: 1.5540  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6746/10240  Batch Loss: 0.7151  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6747/10240  Batch Loss: 0.7857  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6748/10240  Batch Loss: 0.8524  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6749/10240  Batch Loss: 0.8524  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6750/10240  Batch Loss: 2.2303  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6751/10240  Batch Loss: 2.4084  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6752/10240  Batch Loss: 1.2763  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6753/10240  Batch Loss: 0.8288  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6754/10240  Batch Loss: 1.6527  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6755/10240  Batch Loss: 0.7223  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6756/10240  Batch Loss: 1.3608  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6757/10240  Batch Loss: 1.8076  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6758/10240  Batch Loss: 0.7608  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6759/10240  Batch Loss: 1.3399  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6760/10240  Batch Loss: 1.8140  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6761/10240  Batch Loss: 1.3898  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6762/10240  Batch Loss: 0.8985  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6763/10240  Batch Loss: 0.8351  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6764/10240  Batch Loss: 2.3572  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6765/10240  Batch Loss: 1.5376  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6766/10240  Batch Loss: 1.2045  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6767/10240  Batch Loss: 1.0122  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6768/10240  Batch Loss: 1.8275  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6769/10240  Batch Loss: 0.7131  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6770/10240  Batch Loss: 1.7232  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6771/10240  Batch Loss: 1.1982  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6772/10240  Batch Loss: 1.3756  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6773/10240  Batch Loss: 0.7146  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6774/10240  Batch Loss: 1.2511  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6775/10240  Batch Loss: 1.5209  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6776/10240  Batch Loss: 1.7538  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6777/10240  Batch Loss: 2.3676  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6778/10240  Batch Loss: 1.9498  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5888\n",
      "Epoch 001  Batch 6779/10240  Batch Loss: 2.0325  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6780/10240  Batch Loss: 1.6895  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6781/10240  Batch Loss: 1.9821  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6782/10240  Batch Loss: 2.0587  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6783/10240  Batch Loss: 0.7955  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6784/10240  Batch Loss: 1.0531  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6785/10240  Batch Loss: 1.0942  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6786/10240  Batch Loss: 1.0620  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6787/10240  Batch Loss: 0.7391  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6788/10240  Batch Loss: 1.7782  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6789/10240  Batch Loss: 1.3502  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6790/10240  Batch Loss: 0.8189  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6791/10240  Batch Loss: 2.0004  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6792/10240  Batch Loss: 1.5952  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6793/10240  Batch Loss: 1.3892  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6794/10240  Batch Loss: 1.6237  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6795/10240  Batch Loss: 0.7147  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6796/10240  Batch Loss: 1.3680  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6797/10240  Batch Loss: 0.8034  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6798/10240  Batch Loss: 1.9779  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6799/10240  Batch Loss: 1.4811  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6800/10240  Batch Loss: 0.7466  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n",
      "Epoch 001  Batch 6801/10240  Batch Loss: 0.8718  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6802/10240  Batch Loss: 1.0326  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6803/10240  Batch Loss: 1.9101  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6804/10240  Batch Loss: 0.8827  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6805/10240  Batch Loss: 1.0324  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6806/10240  Batch Loss: 0.7781  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6807/10240  Batch Loss: 1.0372  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6808/10240  Batch Loss: 1.1465  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6809/10240  Batch Loss: 0.7140  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6810/10240  Batch Loss: 1.3993  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6811/10240  Batch Loss: 1.5215  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6812/10240  Batch Loss: 1.3628  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6813/10240  Batch Loss: 1.9623  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6814/10240  Batch Loss: 0.8665  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6815/10240  Batch Loss: 0.9378  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6816/10240  Batch Loss: 1.4139  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5890\n",
      "Epoch 001  Batch 6817/10240  Batch Loss: 2.0575  | train F1: 0.0073  | train precision: 0.0037  | train recall: 0.5889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 35\u001B[39m\n\u001B[32m     30\u001B[39m n_va = n - n_tr\n\u001B[32m     32\u001B[39m train_ds, val_ds = random_split(full_ds, [n_tr, n_va],\n\u001B[32m     33\u001B[39m                                 generator=torch.Generator().manual_seed(\u001B[32m42\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m trained_model = train_model(\n\u001B[32m     36\u001B[39m     model,\n\u001B[32m     37\u001B[39m     train_ds, val_ds,\n\u001B[32m     38\u001B[39m     epochs=\u001B[32m150\u001B[39m,\n\u001B[32m     39\u001B[39m     batch_size=\u001B[32m64\u001B[39m,          \u001B[38;5;66;03m# start smaller to reduce I/O pressure\u001B[39;00m\n\u001B[32m     40\u001B[39m     lr=\u001B[32m1.5e-4\u001B[39m,\n\u001B[32m     41\u001B[39m     loss=loss,              \u001B[38;5;66;03m# or use Focal-Tversky combo (recommended)\u001B[39;00m\n\u001B[32m     42\u001B[39m     num_workers=\u001B[32m32\u001B[39m,          \u001B[38;5;66;03m# ✅ critical: no multiprocessing\u001B[39;00m\n\u001B[32m     43\u001B[39m     pin_memory=\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     44\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 136\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_ds, val_ds, epochs, batch_size, lr, loss, alpha, gamma, device, num_workers, pin_memory, prefetch_factor, persistent_workers)\u001B[39m\n\u001B[32m    134\u001B[39m running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m    135\u001B[39m tp = fp = fn = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_num, (imgs, masks) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[32m    137\u001B[39m     imgs = imgs.to(device)  \u001B[38;5;66;03m# (B,1,128,128)\u001B[39;00m\n\u001B[32m    138\u001B[39m     \u001B[38;5;66;03m#print(imgs.shape)\u001B[39;00m\n\u001B[32m    139\u001B[39m \n\u001B[32m    140\u001B[39m     \u001B[38;5;66;03m# Resize the ground‐truth masks to output_size (e.g. (32,32))\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28mself\u001B[39m._next_data()\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1488\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_data(data, worker_id)\n\u001B[32m   1490\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tasks_outstanding > \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1491\u001B[39m idx, data = \u001B[38;5;28mself\u001B[39m._get_data()\n\u001B[32m   1492\u001B[39m \u001B[38;5;28mself\u001B[39m._tasks_outstanding -= \u001B[32m1\u001B[39m\n\u001B[32m   1493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable:\n\u001B[32m   1494\u001B[39m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1453\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._get_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1449\u001B[39m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[32m   1450\u001B[39m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[32m   1451\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1452\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1453\u001B[39m         success, data = \u001B[38;5;28mself\u001B[39m._try_get_data()\n\u001B[32m   1454\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[32m   1455\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001B[32m   1272\u001B[39m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[32m   1273\u001B[39m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1281\u001B[39m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[32m   1282\u001B[39m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[32m   1283\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1284\u001B[39m         data = \u001B[38;5;28mself\u001B[39m._data_queue.get(timeout=timeout)\n\u001B[32m   1285\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[32m   1286\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1287\u001B[39m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[32m   1288\u001B[39m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[32m   1289\u001B[39m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/multiprocessing/queues.py:113\u001B[39m, in \u001B[36mQueue.get\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m block:\n\u001B[32m    112\u001B[39m     timeout = deadline - time.monotonic()\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._poll(timeout):\n\u001B[32m    114\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._poll():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/multiprocessing/connection.py:257\u001B[39m, in \u001B[36m_ConnectionBase.poll\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    255\u001B[39m \u001B[38;5;28mself\u001B[39m._check_closed()\n\u001B[32m    256\u001B[39m \u001B[38;5;28mself\u001B[39m._check_readable()\n\u001B[32m--> \u001B[39m\u001B[32m257\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._poll(timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/multiprocessing/connection.py:440\u001B[39m, in \u001B[36mConnection._poll\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    439\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_poll\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout):\n\u001B[32m--> \u001B[39m\u001B[32m440\u001B[39m     r = wait([\u001B[38;5;28mself\u001B[39m], timeout)\n\u001B[32m    441\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(r)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/multiprocessing/connection.py:1136\u001B[39m, in \u001B[36mwait\u001B[39m\u001B[34m(object_list, timeout)\u001B[39m\n\u001B[32m   1133\u001B[39m     deadline = time.monotonic() + timeout\n\u001B[32m   1135\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1136\u001B[39m     ready = selector.select(timeout)\n\u001B[32m   1137\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ready:\n\u001B[32m   1138\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m [key.fileobj \u001B[38;5;28;01mfor\u001B[39;00m (key, events) \u001B[38;5;129;01min\u001B[39;00m ready]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/selectors.py:415\u001B[39m, in \u001B[36m_PollLikeSelector.select\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    413\u001B[39m ready = []\n\u001B[32m    414\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m415\u001B[39m     fd_event_list = \u001B[38;5;28mself\u001B[39m._selector.poll(timeout)\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mInterruptedError\u001B[39;00m:\n\u001B[32m    417\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ready\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:48:28.508748Z",
     "start_time": "2025-10-08T10:36:13.471719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#down_filters     = [32, 64, 128, 256, 512]\n",
    "down_filters =  [32, 32, 64, 128, 256, 512, 1024]\n",
    "down_activations = ['relu', 'selu', 'selu', 'selu', 'selu', 'selu', 'selu']\n",
    "\n",
    "up_filters       = [1024, 512, 256, 128, 64]\n",
    "up_activations   = ['selu', 'selu', 'selu', 'selu', 'relu']\n",
    "# BCE loss with logits with\n",
    "#x_tiles, y_tiles = build_datasets(\"../DATA/test.h5\", tile_size=128)\n",
    "\n",
    "#print(\"Positive/negative ratio:\", (y_tiles==1).sum().item() / (y_tiles==0).sum().item())\n",
    "\n",
    "#neg = (y_tiles==0).sum()\n",
    "#pos = (y_tiles==1).sum()\n",
    "#pos_weight = (neg / pos).float()\n",
    "#loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1 / 0.003329051612807811))\n",
    "model = UNet(\n",
    "        down_filters=down_filters,\n",
    "        down_activations=down_activations,\n",
    "        up_filters=up_filters,\n",
    "        up_activations=up_activations,\n",
    "        bottleneck_transformer=False,\n",
    "        ASPP_blocks=False,\n",
    "        output_sigmoid=False)\n",
    "\n",
    "\n",
    "trained_model = train_model(model,\n",
    "                            train_ds, val_ds,\n",
    "                            epochs=150,\n",
    "                            batch_size=128,\n",
    "                            lr=0.00015,\n",
    "                            alpha=0.95, gamma=3.1, loss=loss)"
   ],
   "id": "cf5bb10922db71d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001  Batch 160/5120  Batch Loss: 0.8639  | train F1: 0.0077  | train precision: 0.0039  | train recall: 0.5039\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     16\u001B[39m loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(\u001B[32m0.003329051612807811\u001B[39m))\n\u001B[32m     17\u001B[39m model = UNet(\n\u001B[32m     18\u001B[39m         down_filters=down_filters,\n\u001B[32m     19\u001B[39m         down_activations=down_activations,\n\u001B[32m   (...)\u001B[39m\u001B[32m     23\u001B[39m         ASPP_blocks=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     24\u001B[39m         output_sigmoid=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m trained_model = train_model(model,\n\u001B[32m     28\u001B[39m                             train_ds, val_ds,\n\u001B[32m     29\u001B[39m                             epochs=\u001B[32m150\u001B[39m,\n\u001B[32m     30\u001B[39m                             batch_size=\u001B[32m128\u001B[39m,\n\u001B[32m     31\u001B[39m                             lr=\u001B[32m0.00015\u001B[39m,\n\u001B[32m     32\u001B[39m                             alpha=\u001B[32m0.95\u001B[39m, gamma=\u001B[32m3.1\u001B[39m, loss=loss)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 39\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_ds, val_ds, epochs, batch_size, lr, loss, alpha, gamma, device)\u001B[39m\n\u001B[32m     37\u001B[39m running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m     38\u001B[39m tp = fp = fn = \u001B[32m0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_num, (imgs, masks) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[32m     40\u001B[39m     imgs = imgs.to(device)  \u001B[38;5;66;03m# (B,1,128,128)\u001B[39;00m\n\u001B[32m     42\u001B[39m     \u001B[38;5;66;03m# Resize the ground‐truth masks to output_size (e.g. (32,32))\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28mself\u001B[39m._next_data()\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1488\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_data(data, worker_id)\n\u001B[32m   1490\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tasks_outstanding > \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1491\u001B[39m idx, data = \u001B[38;5;28mself\u001B[39m._get_data()\n\u001B[32m   1492\u001B[39m \u001B[38;5;28mself\u001B[39m._tasks_outstanding -= \u001B[32m1\u001B[39m\n\u001B[32m   1493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable:\n\u001B[32m   1494\u001B[39m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._get_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1441\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m   1442\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory_thread.is_alive():\n\u001B[32m-> \u001B[39m\u001B[32m1443\u001B[39m         success, data = \u001B[38;5;28mself\u001B[39m._try_get_data()\n\u001B[32m   1444\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[32m   1445\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001B[32m   1272\u001B[39m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[32m   1273\u001B[39m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1281\u001B[39m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[32m   1282\u001B[39m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[32m   1283\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1284\u001B[39m         data = \u001B[38;5;28mself\u001B[39m._data_queue.get(timeout=timeout)\n\u001B[32m   1285\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[32m   1286\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1287\u001B[39m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[32m   1288\u001B[39m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[32m   1289\u001B[39m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/queue.py:180\u001B[39m, in \u001B[36mQueue.get\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m    178\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m remaining <= \u001B[32m0.0\u001B[39m:\n\u001B[32m    179\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m         \u001B[38;5;28mself\u001B[39m.not_empty.wait(remaining)\n\u001B[32m    181\u001B[39m item = \u001B[38;5;28mself\u001B[39m._get()\n\u001B[32m    182\u001B[39m \u001B[38;5;28mself\u001B[39m.not_full.notify()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/threading.py:359\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    357\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m359\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mTrue\u001B[39;00m, timeout)\n\u001B[32m    360\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    361\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:33:52.807670Z",
     "start_time": "2025-10-08T10:33:52.773008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from time import perf_counter\n",
    "ds = H5TiledDataset(\"../DATA/train.h5\", tile_size=128)\n",
    "t0 = perf_counter()\n",
    "x,y = ds[0]\n",
    "print(\"one __getitem__ took:\", perf_counter()-t0, \"sec\", x.shape, y.shape)\n"
   ],
   "id": "3d549ca51a72b39e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ HDF5 datasets are contiguous (no chunks). Random tile reads will be slow. Consider h5repack with CHUNK=1x128x128.\n",
      "one __getitem__ took: 0.0007629280000855942 sec torch.Size([1, 128, 128]) torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:30:30.364458Z",
     "start_time": "2025-10-08T10:30:28.161189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "for i,(xb,yb) in enumerate(train_loader):\n",
    "    print(\"first batch OK:\", xb.shape); break\n"
   ],
   "id": "a6ab48570d2bf4cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch OK: torch.Size([64, 1, 128, 128])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:35:36.505357Z",
     "start_time": "2025-10-08T10:35:36.499373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py, numpy as np, torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "CLIP_MIN, CLIP_MAX = -166.43, 169.96  # match TF\n",
    "\n",
    "def _tiles_for_shape(H, W, tile):\n",
    "    Hb = (H + tile - 1) // tile\n",
    "    Wb = (W + tile - 1) // tile\n",
    "    return Hb, Wb\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazily streams tiles from an HDF5 file. No mask scanning in __init__.\n",
    "    Each worker opens its own file handle.\n",
    "    \"\"\"\n",
    "    def __init__(self, h5_path, tile_size=128):\n",
    "        self.h5_path = h5_path\n",
    "        self.tile    = tile_size\n",
    "        self.h5      = None  # opened lazily per worker\n",
    "\n",
    "        # Probe shapes without loading data\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            ds_x = f[\"images\"]; ds_y = f[\"masks\"]\n",
    "            self.N, self.H, self.W = ds_x.shape\n",
    "            assert ds_y.shape == (self.N, self.H, self.W)\n",
    "\n",
    "            # (Optional) warn about unfriendly chunking\n",
    "            try:\n",
    "                chunks_x = ds_x.chunks\n",
    "                chunks_y = ds_y.chunks\n",
    "            except Exception:\n",
    "                chunks_x = chunks_y = None\n",
    "            #if chunks_x is None or chunks_y is None:\n",
    "                #print(\"⚠️ HDF5 datasets are contiguous (no chunks). Random tile reads will be slow. Consider h5repack with CHUNK=1x128x128.\")\n",
    "\n",
    "        # Precompute tile indices (grouped by image for locality)\n",
    "        Hb, Wb = _tiles_for_shape(self.H, self.W, self.tile)\n",
    "        self.indices = []\n",
    "        for i in range(self.N):\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    self.indices.append((i, r, c))\n",
    "\n",
    "    def _ensure_open(self):\n",
    "        if self.h5 is None:\n",
    "            self.h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self.ds_x = self.h5[\"images\"]\n",
    "            self.ds_y = self.h5[\"masks\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure_open()\n",
    "        i, r, c = self.indices[idx]\n",
    "        t = self.tile\n",
    "        r0, c0 = r * t, c * t\n",
    "        r1, c1 = min(r0 + t, self.H), min(c0 + t, self.W)\n",
    "\n",
    "        # read exact window\n",
    "        x = self.ds_x[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        y = self.ds_y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "\n",
    "        # pad edges\n",
    "        if x.shape[0] != t or x.shape[1] != t:\n",
    "            xp = np.zeros((t, t), dtype=np.float32)\n",
    "            yp = np.zeros((t, t), dtype=np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x\n",
    "            yp[:y.shape[0], :y.shape[1]] = y\n",
    "            x, y = xp, yp\n",
    "\n",
    "        # preprocessing to match TF\n",
    "        x = np.clip(x, CLIP_MIN, CLIP_MAX)\n",
    "        return torch.from_numpy(x[None, ...]), torch.from_numpy(y[None, ...])\n"
   ],
   "id": "157f1332d7766dd6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T10:33:20.929066Z",
     "start_time": "2025-10-08T10:33:20.882969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "full_ds = H5TiledDataset(\"../DATA/train.h5\", tile_size=128)\n",
    "\n",
    "# quick timing check (should be ~milliseconds, not 20+ seconds)\n",
    "from time import perf_counter\n",
    "t0 = perf_counter(); _ = full_ds[0]; print(\"getitem:\", perf_counter()-t0, \"s\")\n",
    "\n",
    "# split\n",
    "n = len(full_ds); n_tr = int(0.8*n); n_va = n - n_tr\n",
    "train_ds, val_ds = random_split(full_ds, [n_tr, n_va],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# start with num_workers=0 to sidestep h5py/fork issues\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False,\n",
    "                          num_workers=0, pin_memory=False)\n"
   ],
   "id": "dfa0af4fb0a8c240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ HDF5 datasets are contiguous (no chunks). Random tile reads will be slow. Consider h5repack with CHUNK=1x128x128.\n",
      "getitem: 0.0008007400001588394 s\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4264d3696fd612ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
