{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399d7c56",
   "metadata": {},
   "source": [
    "# LSST Asteroid Trail — Evaluation Notebook\n",
    "This notebook loads a trained model, runs inference on an HDF5 **test set**, aligns detections with a `test.csv` catalog of injected trails, and produces the histograms you requested:\n",
    "\n",
    "- **Histogram of LSST stack detections vs. magnitude** and **trail length**  \n",
    "- **Histogram of Neural Network detections vs. magnitude** and **trail length**  \n",
    "- Summary counts for NN, stack, and combined detections\n",
    "\n",
    "> Tip: Adjust the **paths** and **threshold** in the first code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration (edit these) ---\n",
    "MODEL_CKPT = \"./best_unet_tf_parity.pt\"   # path to your trained checkpoint\n",
    "TEST_H5    = \"../DATA/test.h5\"            # path to HDF5 test set with datasets: images, masks\n",
    "CATALOG_CSV= \"../DATA/test.csv\"           # path to catalog (injected trails with positions & metadata)\n",
    "\n",
    "# Detection parameters\n",
    "THRESHOLD  = 0.80      # binarization threshold for NN probability map\n",
    "RADIUS_PX  = 3         # pixel radius around (x,y) to accept NN hit\n",
    "\n",
    "# Tiling parameters (should match training/eval input tiling)\n",
    "TILE       = 128\n",
    "\n",
    "# Histogram saving\n",
    "OUTDIR     = \"./eval_outputs\"\n",
    "import os\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "print(\"Configured. Edit paths above if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db677739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, time, numpy as np, torch, h5py, pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "def robust_stats_mad(arr):\n",
    "    \"\"\"Return (median, sigma) using MAD (robust to outliers).\"\"\"\n",
    "    med = np.median(arr)\n",
    "    mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)  # 1.4826*MAD ~ std for normal\n",
    "    return np.float32(med), np.float32(sigma)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"Streams tiles from HDF5 and applies robust per-image standardization + k-sigma clipping.\n",
    "    Expected datasets in H5: images (N,H,W), masks (N,H,W)\"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, image_crop_for_stats=(512,512)):\n",
    "        self.h5_path = h5_path\n",
    "        self.tile = int(tile)\n",
    "        self.k_sigma = float(k_sigma)\n",
    "        self.image_crop_for_stats = image_crop_for_stats\n",
    "        self._h5 = None\n",
    "        self._stats_cache = {}\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "        Hb = math.ceil(self.H / self.tile)\n",
    "        Wb = math.ceil(self.W / self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "\n",
    "    def _ensure_open(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self.x = self._h5[\"images\"]\n",
    "            self.y = self._h5[\"masks\"]\n",
    "\n",
    "    def _get_image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        H, W = self.H, self.W\n",
    "        if self.image_crop_for_stats is None:\n",
    "            s = min(512, H, W)\n",
    "            h0 = (H - s)//2; w0 = (W - s)//2; h1, w1 = h0+s, w0+s\n",
    "        else:\n",
    "            sH, sW = self.image_crop_for_stats\n",
    "            sH = min(sH, H); sW = min(sW, W)\n",
    "            h0 = (H - sH)//2; w0 = (W - sW)//2; h1, w1 = h0+sH, w0+sW\n",
    "        crop = self.x[i, h0:h1, w0:w1].astype(\"float32\")\n",
    "        med, sigma = robust_stats_mad(crop)\n",
    "        if not np.isfinite(sigma) or sigma <= 0: sigma = np.float32(1.0)\n",
    "        self._stats_cache[i] = (med, sigma)\n",
    "        return self._stats_cache[i]\n",
    "\n",
    "    def _normalize_and_clip(self, tile_arr, med, sigma):\n",
    "        x = (tile_arr - med) / sigma\n",
    "        if self.k_sigma > 0: x = np.clip(x, -self.k_sigma, +self.k_sigma)\n",
    "        return x\n",
    "\n",
    "    def __len__(self): return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure_open()\n",
    "        i, r, c = self.indices[idx]\n",
    "        t = self.tile\n",
    "        r0, c0 = r*t, c*t\n",
    "        r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "        x = self.x[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        y = self.y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        if x.shape[0] != t or x.shape[1] != t:\n",
    "            xp = np.zeros((t, t), np.float32)\n",
    "            yp = np.zeros((t, t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x\n",
    "            yp[:y.shape[0], :y.shape[1]] = y\n",
    "            x, y = xp, yp\n",
    "        med, sigma = self._get_image_stats(i)\n",
    "        x = self._normalize_and_clip(x, med, sigma)\n",
    "        return torch.from_numpy(x[None, ...]), torch.from_numpy(y[None, ...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1, bias=False), nn.BatchNorm2d(F_int))\n",
    "        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1, bias=False), nn.BatchNorm2d(F_int))\n",
    "        self.psi = nn.Sequential(nn.Conv2d(F_int, F_l, 1, bias=False), nn.BatchNorm2d(F_l), nn.Sigmoid())\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g); x1 = self.W_x(x)\n",
    "        if g1.shape[-2:] != x1.shape[-2:]:\n",
    "            g1 = F.interpolate(g1, size=x1.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "def conv_bn_act(in_ch, out_ch, k=3, act='relu'):\n",
    "    layers = [nn.Conv2d(in_ch, out_ch, k, padding=k//2, bias=False), nn.BatchNorm2d(out_ch)]\n",
    "    if act.lower() == 'relu': layers += [nn.ReLU(inplace=True)]\n",
    "    elif act.lower() == 'selu': layers += [nn.SELU(inplace=True)]\n",
    "    elif act.lower() == 'elu': layers += [nn.ELU(inplace=True)]\n",
    "    else: layers += [nn.ReLU(inplace=True)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, act='relu', dropout=0.0, max_pool=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_bn_act(in_ch, out_ch, 3, act)\n",
    "        self.conv2 = conv_bn_act(out_ch, out_ch, 3, act)\n",
    "        self.drop = nn.Dropout2d(dropout) if dropout>0 else nn.Identity()\n",
    "        self.pool = nn.MaxPool2d(2) if max_pool else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x); x = self.conv2(x); x = self.drop(x)\n",
    "        skip = x\n",
    "        x = self.pool(x)\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, act='relu', dropout=0.0, do_up=True, use_attn=True):\n",
    "        super().__init__()\n",
    "        self.skip_ch = skip_ch\n",
    "        up_out_ch = in_ch if (skip_ch == 0) else skip_ch\n",
    "        self.up = (nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, up_out_ch, 3, stride=2, padding=1, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(up_out_ch),\n",
    "            nn.ReLU(inplace=True) if act=='relu' else nn.SELU(inplace=True)\n",
    "        ) if do_up else nn.Identity())\n",
    "        self.attn = (AttentionGate(up_out_ch, skip_ch, max(1, skip_ch//2)) if (use_attn and skip_ch > 0) else nn.Identity())\n",
    "        conv1_in = up_out_ch if skip_ch == 0 else (up_out_ch + skip_ch)\n",
    "        self.conv1 = conv_bn_act(conv1_in, out_ch, 3, act)\n",
    "        self.conv2 = conv_bn_act(out_ch, out_ch, 3, act)\n",
    "        self.drop = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.up(x)\n",
    "        if self.skip_ch > 0 and skip is not None:\n",
    "            if x.shape[-2:] != skip.shape[-2:]:\n",
    "                x = F.interpolate(x, size=skip.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            skip = self.attn(x, skip)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x); x = self.conv2(x); x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class UNetTFParity(nn.Module):\n",
    "    def __init__(self, in_ch=1, arch=None, kernel_size=5):\n",
    "        super().__init__()\n",
    "        assert arch is not None, \"Provide architecture dict with keys like 'downFilters', 'downActivation', 'downDropout', 'downMaxPool', 'upFilters', 'upActivation', 'upDropout'.\"\n",
    "        self.input_bn = nn.BatchNorm2d(in_ch)\n",
    "        self.enc = nn.ModuleList()\n",
    "        prev = in_ch\n",
    "        for nf, act, drop, mp in zip(arch[\"downFilters\"], arch[\"downActivation\"], arch[\"downDropout\"], arch[\"downMaxPool\"]):\n",
    "            self.enc.append(EncoderBlock(prev, nf, act, drop, max_pool=mp))\n",
    "            prev = nf\n",
    "        self.dec = nn.ModuleList()\n",
    "        skip_chs = arch[\"downFilters\"][:]\n",
    "        for i, (nf, act, drop) in enumerate(zip(arch[\"upFilters\"], arch[\"upActivation\"], arch[\"upDropout\"])):\n",
    "            in_ch_dec = skip_chs[-1] if i == 0 else arch[\"upFilters\"][i-1]\n",
    "            skip_ch = 0 if i == 0 else skip_chs[-1-i]\n",
    "            self.dec.append(DecoderBlock(in_ch_dec, skip_ch, nf, act, drop, do_up=True, use_attn=True))\n",
    "        self.out_conv = nn.Conv2d(arch[\"upFilters\"][-1], 1, kernel_size, padding=kernel_size//2)\n",
    "        self.out_act  = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        skips = []\n",
    "        for i, blk in enumerate(self.enc):\n",
    "            x, skip = blk(x)\n",
    "            skips.append(skip if i < len(self.enc)-1 else None)\n",
    "        x = self.dec[0](x, skips[-1])\n",
    "        for i in range(1, len(self.dec)):\n",
    "            x = self.dec[i](x, skips[-1 - i])\n",
    "        x = self.out_conv(x)\n",
    "        return self.out_act(x)\n",
    "\n",
    "def load_model(ckpt_path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    arch = ckpt.get(\"arch\", None)\n",
    "    if arch is None:\n",
    "        raise ValueError(\"Checkpoint missing 'arch'. Re-save your model with {'state_dict':..., 'arch':...}.\")\n",
    "    model = UNetTFParity(in_ch=1, arch=arch, kernel_size=5).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"])\n",
    "    model.eval()\n",
    "    print(\"Loaded model with architecture:\", arch)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def predict_tiles_to_full(h5_path, loader, model, tile=128):\n",
    "    \"\"\"Assemble full-size per-panel predictions from tile predictions.\"\"\"\n",
    "    model.eval()\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        N, H, W = f[\"images\"].shape\n",
    "        Hb, Wb = math.ceil(H / tile), math.ceil(W / tile)\n",
    "        tiles_per_panel = Hb * Wb\n",
    "    xb0, _ = next(iter(loader))\n",
    "    xb0 = xb0.to(next(model.parameters()).device)\n",
    "    out0 = model(xb0[:1])\n",
    "    oh, ow = out0.shape[-2], out0.shape[-1]\n",
    "    full_preds = np.zeros((N, H, W), dtype=np.float32)\n",
    "    tile_buf = []\n",
    "    ptr = 0\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(next(model.parameters()).device)\n",
    "        out = model(xb)\n",
    "        probs = out.detach()[:, 0]\n",
    "        if (oh, ow) != (tile, tile):\n",
    "            probs = F.interpolate(probs.unsqueeze(1), size=(tile, tile), mode='bilinear', align_corners=False).squeeze(1)\n",
    "        probs = probs.cpu().numpy()\n",
    "        tile_buf.extend(list(probs))\n",
    "        while len(tile_buf) >= tiles_per_panel:\n",
    "            p = ptr // tiles_per_panel\n",
    "            if p >= full_preds.shape[0]: break\n",
    "            panel = np.zeros((Hb * tile, Wb * tile), dtype=np.float32)\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    t_idx = r * Wb + c\n",
    "                    tile_img = tile_buf[t_idx]\n",
    "                    r0, c0 = r * tile, c * tile\n",
    "                    panel[r0:r0 + tile, c0:c0 + tile] = tile_img\n",
    "            full_preds[p] = panel[:H, :W]\n",
    "            tile_buf = tile_buf[tiles_per_panel:]\n",
    "            ptr += tiles_per_panel\n",
    "    return full_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mark_nn_and_stack(csv_path, p_full, radius=3, thr=0.5):\n",
    "    \"\"\"Add detection flags to catalog: stack_detected (from CSV) and nn_detected (from p_full).\"\"\"\n",
    "    cat = pd.read_csv(csv_path).copy()\n",
    "    need = {\"image_id\",\"x\",\"y\"}\n",
    "    missing = need - set(cat.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing columns: {missing}\")\n",
    "    if \"stack_detection\" in cat.columns:\n",
    "        cat[\"stack_detected\"] = cat[\"stack_detection\"].astype(bool)\n",
    "    elif \"stack_mag\" in cat.columns:\n",
    "        cat[\"stack_detected\"] = ~cat[\"stack_mag\"].isna()\n",
    "    else:\n",
    "        cat[\"stack_detected\"] = False\n",
    "    H, W = p_full.shape[1:]\n",
    "    pred_bin = (p_full >= thr).astype(np.uint8)\n",
    "    nn = np.zeros(len(cat), dtype=bool)\n",
    "    for pid, grp in cat.groupby(\"image_id\"):\n",
    "        pid = int(pid)\n",
    "        if pid < 0 or pid >= pred_bin.shape[0]: continue\n",
    "        mask = pred_bin[pid]\n",
    "        xs = grp[\"x\"].to_numpy().astype(int)\n",
    "        ys = grp[\"y\"].to_numpy().astype(int)\n",
    "        xs = np.clip(xs, 0, W-1)\n",
    "        ys = np.clip(ys, 0, H-1)\n",
    "        for idx_row, (x, y) in zip(grp.index.to_numpy(), zip(xs, ys)):\n",
    "            y0, y1 = max(0, y-radius), min(H, y+radius+1)\n",
    "            x0, x1 = max(0, x-radius), min(W, x+radius+1)\n",
    "            nn[idx_row] = (mask[y0:y1, x0:x1].max() > 0)\n",
    "    cat[\"nn_detected\"] = nn\n",
    "    return cat\n",
    "\n",
    "def _choose_mag_field(df):\n",
    "    for c in [\"PSF_mag\", \"integrated_mag\", \"mag\"]:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def plot_detect_hist(cat, field, bins=12, title=None, savepath=None):\n",
    "    nn_det = cat[cat[\"nn_detected\"]]\n",
    "    stk_det = cat[cat[\"stack_detected\"]]\n",
    "    cum_det = cat[cat[\"nn_detected\"] | cat[\"stack_detected\"]]\n",
    "    vals = cat[field].to_numpy()\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    edges = np.histogram_bin_edges(vals, bins=bins)\n",
    "    fig, ax = plt.subplots(figsize=(6.5,4.5))\n",
    "    ax.hist(cat[field],      bins=edges, histtype=\"step\", label=\"All injected\", alpha=0.7)\n",
    "    ax.hist(cum_det[field],  bins=edges, histtype=\"step\", label=\"Cumulative (NN ∪ LSST)\")\n",
    "    ax.hist(nn_det[field],   bins=edges, histtype=\"step\", label=\"NN detected\")\n",
    "    ax.hist(stk_det[field],  bins=edges, histtype=\"step\", label=\"LSST stack detected\")\n",
    "    ax.set_xlabel(field.replace(\"_\",\" \")); ax.set_ylabel(\"Count\")\n",
    "    if title: ax.set_title(title)\n",
    "    ax.legend(); ax.grid(True, alpha=0.3)\n",
    "    if savepath:\n",
    "        fig.savefig(savepath, dpi=150, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", savepath)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build dataset/loader, load model, and run predictions\n",
    "test_ds = H5TiledDataset(TEST_H5, tile=TILE, k_sigma=5.0, image_crop_for_stats=(512,512))\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=False)\n",
    "print(\"Tiles:\", len(test_ds), \"| images:\", test_ds.N, f\"| panel size: {test_ds.H}x{test_ds.W}\")\n",
    "model = load_model(MODEL_CKPT)\n",
    "p_full = predict_tiles_to_full(TEST_H5, test_loader, model, tile=TILE)\n",
    "np.save(os.path.join(OUTDIR, \"p_full.npy\"), p_full)\n",
    "print(\"Predictions shape:\", p_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Catalog matching + histograms\n",
    "cat = mark_nn_and_stack(CATALOG_CSV, p_full, radius=RADIUS_PX, thr=THRESHOLD)\n",
    "\n",
    "mag_field = _choose_mag_field(cat)\n",
    "if mag_field is None:\n",
    "    print(\"No magnitude column found in CSV. Skipping magnitude histogram.\")\n",
    "else:\n",
    "    plot_detect_hist(cat, mag_field, bins=12, title=f\"Detections vs {mag_field}\",\n",
    "                     savepath=os.path.join(OUTDIR, f\"hist_vs_{mag_field}.png\"))\n",
    "\n",
    "if \"trail_length\" in cat.columns:\n",
    "    plot_detect_hist(cat, \"trail_length\", bins=12, title=\"Detections vs trail length\",\n",
    "                     savepath=os.path.join(OUTDIR, \"hist_vs_trail_length.png\"))\n",
    "else:\n",
    "    print(\"CSV missing 'trail_length' column. Skipping trail-length histogram.\")\n",
    "\n",
    "out_csv = os.path.join(OUTDIR, \"test_with_detections.csv\")\n",
    "cat.to_csv(out_csv, index=False)\n",
    "print(\"Saved catalog with flags:\", out_csv)\n",
    "\n",
    "tot = len(cat)\n",
    "nn  = int(cat[\"nn_detected\"].sum())\n",
    "stk = int(cat[\"stack_detected\"].sum())\n",
    "cum = int((cat[\"nn_detected\"] | cat[\"stack_detected\"]).sum())\n",
    "print(f\"NN: {nn}/{tot} | LSST stack: {stk}/{tot} | Cumulative: {cum}/{tot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: quick pixel-level PRF1 at chosen threshold\n",
    "with h5py.File(TEST_H5, \"r\") as f:\n",
    "    gt_full = f[\"masks\"][:].astype(np.uint8)\n",
    "bin_full = (p_full >= THRESHOLD).astype(np.uint8)\n",
    "tp = int((bin_full & gt_full).sum())\n",
    "fp = int((bin_full & (1-gt_full)).sum())\n",
    "fn = int(((1-bin_full) & gt_full).sum())\n",
    "precision = tp / max(tp+fp, 1)\n",
    "recall    = tp / max(tp+fn, 1)\n",
    "f1        = 2*precision*recall / max(precision+recall, 1e-8)\n",
    "print(f\"Pixel-level P={precision:.4f}, R={recall:.4f}, F1={f1:.4f} (thr={THRESHOLD})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
