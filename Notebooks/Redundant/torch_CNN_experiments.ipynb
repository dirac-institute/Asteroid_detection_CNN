{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-23T17:19:15.183668Z",
     "start_time": "2025-05-23T17:19:14.594164Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# Squeeze-and-Excitation Block\n",
    "# -------------------------\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x * self.fc(x)\n",
    "\n",
    "# -------------------------\n",
    "# Attention Gate for Skip Connections\n",
    "# -------------------------\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Conv2d(F_g, F_int, kernel_size=1, bias=False)\n",
    "        self.W_x = nn.Conv2d(F_l, F_int, kernel_size=1, bias=False)\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, g, x):\n",
    "        # g: gating signal, x: skip connection\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# -------------------------\n",
    "# Depthwise Separable Conv (for ASPP)\n",
    "# -------------------------\n",
    "class SepConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, padding, dilation=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size, padding=padding,\n",
    "                                   dilation=dilation, groups=in_ch, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "        self.norm = nn.GroupNorm(8, out_ch)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return self.act(self.norm(x))\n",
    "\n",
    "# -------------------------\n",
    "# Multi-Scale Fusion\n",
    "# -------------------------\n",
    "class MultiScaleFusion(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, scales=(1.0,0.5,0.25)):\n",
    "        super().__init__()\n",
    "        self.scales = scales\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.GroupNorm(8, out_ch), nn.ReLU(),\n",
    "                SEBlock(out_ch)\n",
    "            ) for _ in scales\n",
    "        ])\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(scales)*out_ch, out_ch, 1, bias=False),\n",
    "            nn.GroupNorm(8, out_ch), nn.ReLU(),\n",
    "            SEBlock(out_ch)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        bs,c,h,w = x.shape\n",
    "        outs = []\n",
    "        for s,conv in zip(self.scales,self.convs):\n",
    "            xi = F.interpolate(x, scale_factor=s, mode='bilinear', align_corners=False) if s!=1.0 else x\n",
    "            if s!=1.0:\n",
    "                xi = F.interpolate(xi, size=(h,w), mode='bilinear', align_corners=False)\n",
    "            outs.append(conv(xi))\n",
    "        return self.merge(torch.cat(outs, dim=1))\n",
    "\n",
    "# -------------------------\n",
    "# ASPP Module with Mixed Kernels\n",
    "# -------------------------\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        dilations = [1,6,12,18]\n",
    "        kernels   = [3,3,5,7]\n",
    "        self.branches = nn.ModuleList()\n",
    "        for d,k in zip(dilations,kernels):\n",
    "            pad = (k//2)*d\n",
    "            self.branches.append(SepConv(in_ch, out_ch, k, pad, d))\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv2d(len(dilations)*out_ch, out_ch, 1, bias=False),\n",
    "            nn.GroupNorm(8, out_ch), nn.ReLU(),\n",
    "            SEBlock(out_ch)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.merge(torch.cat([b(x) for b in self.branches], dim=1))\n",
    "\n",
    "# -------------------------\n",
    "# Residual DoubleConv + Strided Downsampling\n",
    "# -------------------------\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 5, padding=2, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.act1  = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 5, padding=2, bias=False)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.act2  = nn.ReLU()\n",
    "        self.res   = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
    "        self.se    = SEBlock(out_ch)\n",
    "    def forward(self, x):\n",
    "        residual = self.res(x)\n",
    "        x = self.act1(self.norm1(self.conv1(x)))\n",
    "        x = self.norm2(self.conv2(x))\n",
    "        x = self.act2(x + residual)\n",
    "        return self.se(x)\n",
    "\n",
    "class PoolConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 5, stride=2, padding=2, bias=False),\n",
    "            nn.GroupNorm(8, out_ch), nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# -------------------------\n",
    "# Bottleneck Transformer\n",
    "# -------------------------\n",
    "class BottleneckTransformer(nn.Module):\n",
    "    def __init__(self, dim, heads=8, depth=6, mlp_dim=None):\n",
    "        super().__init__()\n",
    "        mlp_dim = mlp_dim or dim*4\n",
    "        layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads,\n",
    "                                           dim_feedforward=mlp_dim,\n",
    "                                           activation='relu', norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(layer, depth)\n",
    "        self.norm    = nn.LayerNorm(dim)\n",
    "    def forward(self,x):\n",
    "        bs,c,h,w = x.shape\n",
    "        x = x.flatten(2).permute(2,0,1)\n",
    "        x = self.encoder(x)\n",
    "        x = x.permute(1,2,0).view(bs,c,h,w)\n",
    "        return self.norm(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "\n",
    "# -------------------------\n",
    "# Encoder/Decoder Blocks\n",
    "# -------------------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,dropout=0.0,pool=True):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "        self.down = PoolConv(out_ch, out_ch) if pool else None\n",
    "        self.drop = nn.Dropout2d(dropout) if dropout>0 else None\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.drop: x = self.drop(x)\n",
    "        skip = x\n",
    "        x = self.down(x) if self.down else x\n",
    "        return x, skip\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.up   = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
    "        self.attn = AttentionGate(F_g=out_ch, F_l=out_ch, F_int=out_ch//2)\n",
    "        self.conv = DoubleConv(out_ch*2, out_ch)\n",
    "        self.drop = nn.Dropout2d(dropout) if dropout>0 else None\n",
    "    def forward(self,x,skip):\n",
    "        x = self.up(x)\n",
    "        skip = self.attn(g=x, x=skip)\n",
    "        x = self.conv(torch.cat([x,skip],dim=1))\n",
    "        return self.drop(x) if self.drop else x\n",
    "\n",
    "# -------------------------\n",
    "# Full UNet with Attention and SE\n",
    "# -------------------------\n",
    "class UNetEnhanced(nn.Module):\n",
    "    def __init__(self,in_ch=1,base=64,depth=4,dropout=0.0,ds=True):\n",
    "        super().__init__()\n",
    "        self.encs = nn.ModuleList()\n",
    "        ch = in_ch\n",
    "        for i in range(depth):\n",
    "            out = base*(2**i)\n",
    "            self.encs.append(EncoderBlock(ch, out, dropout, pool=(i<depth-1)))\n",
    "            ch = out\n",
    "        self.ms   = MultiScaleFusion(ch, ch)\n",
    "        self.aspp = ASPP(ch, ch)\n",
    "        self.trans= BottleneckTransformer(ch)\n",
    "        self.decs = nn.ModuleList()\n",
    "        ds_chs   = []\n",
    "        for i in reversed(range(depth-1)):\n",
    "            out = base*(2**i)\n",
    "            self.decs.append(DecoderBlock(ch, out, dropout))\n",
    "            ds_chs.append(out)\n",
    "            ch = out\n",
    "        self.final_seg = nn.Conv2d(ch, 1, kernel_size=1)\n",
    "        self.ds = ds\n",
    "        if ds:\n",
    "            self.ds_heads = nn.ModuleList([\n",
    "                nn.Conv2d(c, 1, kernel_size=1) for c in ds_chs\n",
    "            ])\n",
    "    def forward(self,x):\n",
    "        skips = []\n",
    "        for enc in self.encs:\n",
    "            x, skip = enc(x)\n",
    "            skips.append(skip)\n",
    "        x = self.ms(x)\n",
    "        x = self.aspp(x)\n",
    "        x = self.trans(x)\n",
    "        ds_out = []\n",
    "        for idx, dec in enumerate(self.decs):\n",
    "            x = dec(x, skips[-2-idx])\n",
    "            if self.ds:\n",
    "                ds_out.append(self.ds_heads[idx](x))\n",
    "        seg_logits = self.final_seg(x)\n",
    "        if self.ds:\n",
    "            ds_out = [F.interpolate(o, size=seg_logits.shape[2:],\n",
    "                         mode='bilinear', align_corners=False)\n",
    "                      for o in ds_out]\n",
    "            return seg_logits, ds_out\n",
    "        return seg_logits, None\n",
    "\n",
    "class TverskyF2Loss(nn.Module):\n",
    "    def forward(self, preds, targets):\n",
    "        TP = (preds*targets).sum()\n",
    "        FN = ((1-preds)*targets).sum()\n",
    "        FP = (preds*(1-targets)).sum()\n",
    "        return (FN + 1e-6) / (2*TP + FN + 1e-6)\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, beta=0.1, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.gamma, self.eps = alpha, beta, gamma, eps\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        TP = (preds * targets).sum()\n",
    "        FP = (preds * (1 - targets)).sum()\n",
    "        FN = ((1 - preds) * targets).sum()\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return torch.pow((1 - tversky), self.gamma)\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=500.0, bce_weight=0.2, dice_weight=0.4, f2_weight=0.4):\n",
    "        super().__init__()\n",
    "        self.bce  = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        self.ft   = FocalTverskyLoss(alpha=0.9, beta=0.3, gamma=2.0)\n",
    "        self.tf2 = TverskyF2Loss()\n",
    "        self.bw, self.dw, self.fw= bce_weight, dice_weight, f2_weight\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss  = self.bce(logits, targets)\n",
    "        dice_loss = self.ft(torch.sigmoid(logits), targets)\n",
    "        f2_loss   = self.tf2(torch.sigmoid(logits), targets)\n",
    "        return self.bw*bce_loss + self.dw*dice_loss + self.fw*f2_loss\n",
    "\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self,path): self.path=path; self.f=None\n",
    "    def __len__(self): return h5py.File(self.path,'r')['x'].shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        if self.f is None:\n",
    "            self.f = h5py.File(self.path,'r', swmr=True)\n",
    "        x = torch.from_numpy(self.f['x'][idx]).float()\n",
    "        y = torch.from_numpy(self.f['y'][idx]).float()\n",
    "        return x, y"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T12:45:25.757793Z",
     "start_time": "2025-05-20T12:45:08.790114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    bs, max_batches, epochs = 5, 500, 200\n",
    "    train_path = 'train.h5'\n",
    "    val_path   = 'val.h5'\n",
    "\n",
    "    # ─── datasets ───\n",
    "    train_ds = H5Dataset(train_path)\n",
    "    val_ds   = H5Dataset(val_path)\n",
    "\n",
    "    # ─── train loader ───\n",
    "    # pick exactly bs*max_batches random samples from train set\n",
    "    small_idx = torch.randperm(len(train_ds))[:10]\n",
    "    small_loader = DataLoader(train_ds, batch_size=5,\n",
    "                          sampler=SubsetRandomSampler(small_idx))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UNetEnhanced(in_ch=1, base=32, depth=4, dropout=0.0, ds=True).to(device)\n",
    "    crit  = ComboLoss(pos_weight=500, bce_weight=0.5, dice_weight=0.5, f2_weight=0.0)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.ConstantLR(opt, factor=1.0, total_iters=epochs)\n",
    "\n",
    "    ds_weights = [1, 1, 1]\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # ——— train ———\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_num, (imgs, msk) in enumerate(pbar:=tqdm(small_loader, desc=f\"Epoch {epoch}\")):\n",
    "            #imgs[msk==1] = imgs[msk==1] * 3\n",
    "            imgs = imgs/50\n",
    "            imgs = imgs.to(device)\n",
    "            msk  = msk.to(device)\n",
    "\n",
    "            seg_logits, ds_out = model(imgs)\n",
    "            loss = crit(seg_logits, msk)\n",
    "\n",
    "            # deep supervision\n",
    "            for aux, w in zip(ds_out, ds_weights):\n",
    "                loss = loss + w * crit(aux, msk)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(avg_train_loss=total_loss/(batch_num+1), batch_loss=loss.item(), lr=sched.get_last_lr()[0])\n",
    "        sched.step()\n",
    "\n",
    "        # ——— validate ———\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tp=fp=fn=0\n",
    "            loss = 0\n",
    "            for imgs, msk in small_loader:\n",
    "                #imgs[msk==1] = imgs[msk==1] * 3\n",
    "                imgs = imgs/50\n",
    "                imgs = imgs.to(device)\n",
    "                msk  = msk.to(device)\n",
    "\n",
    "                seg_logits, ds_out = model(imgs)\n",
    "                loss += crit(seg_logits, msk)\n",
    "\n",
    "                # deep supervision\n",
    "                for aux, w in zip(ds_out, ds_weights):\n",
    "                    loss = loss + w * crit(aux, msk)\n",
    "                preds = (torch.sigmoid(seg_logits)>0.5).float().view(-1)\n",
    "                t     = msk.view(-1)\n",
    "\n",
    "                tp += (preds * t).sum().item()\n",
    "                fp += (preds * (1-t)).sum().item()\n",
    "                fn += ((1-preds)*t).sum().item()\n",
    "\n",
    "            prec = tp/(tp+fp+1e-8)\n",
    "            rec  = tp/(tp+fn+1e-8)\n",
    "            f1   = 2*prec*rec/(prec+rec+1e-8)\n",
    "            print(f\"Val loss: {loss:.4f}  Val F1: {f1:.4f}  (P={prec:.4f}, R={rec:.4f})\")\n"
   ],
   "id": "2a2544e0928eda32",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:00<00:00, 10.68it/s, avg_train_loss=5.53, batch_loss=6.02, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 10.5824  Val F1: 0.0125  (P=0.0063, R=0.6698)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [00:00<00:00, 43.90it/s, avg_train_loss=5.26, batch_loss=4.91, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 10.0373  Val F1: 0.0122  (P=0.0062, R=0.8862)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2/2 [00:00<00:00, 45.18it/s, avg_train_loss=4.99, batch_loss=5.26, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 9.5142  Val F1: 0.0135  (P=0.0068, R=0.9198)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2/2 [00:00<00:00, 43.72it/s, avg_train_loss=4.71, batch_loss=5.01, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 9.0855  Val F1: 0.0161  (P=0.0081, R=0.9254)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2/2 [00:00<00:00, 44.66it/s, avg_train_loss=4.53, batch_loss=4.1, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.6253  Val F1: 0.0183  (P=0.0092, R=0.9459)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2/2 [00:00<00:00, 44.43it/s, avg_train_loss=4.29, batch_loss=3.88, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.1945  Val F1: 0.0207  (P=0.0105, R=0.9590)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2/2 [00:00<00:00, 44.27it/s, avg_train_loss=4.09, batch_loss=3.74, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.6854  Val F1: 0.0251  (P=0.0127, R=0.9832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2/2 [00:00<00:00, 44.63it/s, avg_train_loss=3.78, batch_loss=3.49, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.2002  Val F1: 0.0245  (P=0.0124, R=0.9851)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2/2 [00:00<00:00, 44.28it/s, avg_train_loss=3.58, batch_loss=3.74, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.7906  Val F1: 0.0351  (P=0.0179, R=0.9907)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2/2 [00:00<00:00, 45.94it/s, avg_train_loss=3.43, batch_loss=3.51, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.5642  Val F1: 0.0422  (P=0.0215, R=0.9925)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2/2 [00:00<00:00, 38.42it/s, avg_train_loss=3.2, batch_loss=3.01, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.7223  Val F1: 0.0582  (P=0.0300, R=0.9963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2/2 [00:00<00:00, 38.36it/s, avg_train_loss=3.26, batch_loss=2.98, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.0594  Val F1: 0.0771  (P=0.0401, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2/2 [00:00<00:00, 44.72it/s, avg_train_loss=3.05, batch_loss=3.09, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.0841  Val F1: 0.0853  (P=0.0446, R=0.9963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2/2 [00:00<00:00, 42.90it/s, avg_train_loss=3.01, batch_loss=2.97, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.7926  Val F1: 0.1507  (P=0.0815, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2/2 [00:00<00:00, 31.37it/s, avg_train_loss=2.92, batch_loss=2.94, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.7393  Val F1: 0.2114  (P=0.1182, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2/2 [00:00<00:00, 40.02it/s, avg_train_loss=2.84, batch_loss=2.9, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.5594  Val F1: 0.2061  (P=0.1149, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2/2 [00:00<00:00, 40.29it/s, avg_train_loss=2.78, batch_loss=2.79, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.5179  Val F1: 0.1969  (P=0.1092, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 2/2 [00:00<00:00, 40.19it/s, avg_train_loss=2.75, batch_loss=2.77, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.4171  Val F1: 0.2365  (P=0.1341, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:00<00:00, 30.13it/s, avg_train_loss=2.7, batch_loss=2.68, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.3660  Val F1: 0.2806  (P=0.1632, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 2/2 [00:00<00:00, 33.12it/s, avg_train_loss=2.68, batch_loss=2.64, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.2905  Val F1: 0.2803  (P=0.1630, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 2/2 [00:00<00:00, 38.62it/s, avg_train_loss=2.64, batch_loss=2.62, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.2428  Val F1: 0.2733  (P=0.1583, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 2/2 [00:00<00:00, 31.38it/s, avg_train_loss=2.62, batch_loss=2.63, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.2129  Val F1: 0.2793  (P=0.1623, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 2/2 [00:00<00:00, 30.11it/s, avg_train_loss=2.6, batch_loss=2.6, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.1559  Val F1: 0.3040  (P=0.1793, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 2/2 [00:00<00:00, 31.35it/s, avg_train_loss=2.58, batch_loss=2.57, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.1472  Val F1: 0.3291  (P=0.1970, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 2/2 [00:00<00:00, 37.58it/s, avg_train_loss=2.56, batch_loss=2.56, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.0873  Val F1: 0.3255  (P=0.1944, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 2/2 [00:00<00:00, 37.44it/s, avg_train_loss=2.54, batch_loss=2.54, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.0719  Val F1: 0.3212  (P=0.1913, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 2/2 [00:00<00:00, 36.58it/s, avg_train_loss=2.54, batch_loss=2.53, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.0320  Val F1: 0.3355  (P=0.2016, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 2/2 [00:00<00:00, 37.58it/s, avg_train_loss=2.52, batch_loss=2.51, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.0060  Val F1: 0.3455  (P=0.2088, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 2/2 [00:00<00:00, 34.46it/s, avg_train_loss=2.52, batch_loss=2.5, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.9881  Val F1: 0.3433  (P=0.2072, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 2/2 [00:00<00:00, 37.91it/s, avg_train_loss=2.49, batch_loss=2.48, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.9482  Val F1: 0.3519  (P=0.2135, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 2/2 [00:00<00:00, 34.83it/s, avg_train_loss=2.47, batch_loss=2.47, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.9206  Val F1: 0.3612  (P=0.2204, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 2/2 [00:00<00:00, 32.63it/s, avg_train_loss=2.46, batch_loss=2.44, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.9225  Val F1: 0.3614  (P=0.2206, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 2/2 [00:00<00:00, 32.23it/s, avg_train_loss=2.44, batch_loss=2.44, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.8666  Val F1: 0.3654  (P=0.2235, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 2/2 [00:00<00:00, 37.67it/s, avg_train_loss=2.43, batch_loss=2.43, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.8865  Val F1: 0.3747  (P=0.2305, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 2/2 [00:00<00:00, 31.50it/s, avg_train_loss=2.42, batch_loss=2.44, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.8105  Val F1: 0.3746  (P=0.2304, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 2/2 [00:00<00:00, 32.24it/s, avg_train_loss=2.4, batch_loss=2.38, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.7947  Val F1: 0.3860  (P=0.2392, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 2/2 [00:00<00:00, 37.83it/s, avg_train_loss=2.39, batch_loss=2.41, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.7526  Val F1: 0.3860  (P=0.2392, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 2/2 [00:00<00:00, 30.26it/s, avg_train_loss=2.37, batch_loss=2.38, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.7084  Val F1: 0.3910  (P=0.2430, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 2/2 [00:00<00:00, 30.87it/s, avg_train_loss=2.35, batch_loss=2.36, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.6744  Val F1: 0.3938  (P=0.2452, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 2/2 [00:00<00:00, 37.49it/s, avg_train_loss=2.35, batch_loss=2.4, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.6493  Val F1: 0.3931  (P=0.2446, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 2/2 [00:00<00:00, 30.93it/s, avg_train_loss=2.32, batch_loss=2.29, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.6461  Val F1: 0.4158  (P=0.2625, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 2/2 [00:00<00:00, 31.57it/s, avg_train_loss=2.3, batch_loss=2.31, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.5585  Val F1: 0.3917  (P=0.2435, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 2/2 [00:00<00:00, 37.21it/s, avg_train_loss=2.29, batch_loss=2.3, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.5749  Val F1: 0.3757  (P=0.2313, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 2/2 [00:00<00:00, 34.16it/s, avg_train_loss=2.29, batch_loss=2.29, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.5222  Val F1: 0.4257  (P=0.2704, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 2/2 [00:00<00:00, 31.94it/s, avg_train_loss=2.26, batch_loss=2.26, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.5410  Val F1: 0.3573  (P=0.2175, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 2/2 [00:00<00:00, 29.70it/s, avg_train_loss=2.28, batch_loss=2.21, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.4926  Val F1: 0.3852  (P=0.2385, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 2/2 [00:00<00:00, 31.45it/s, avg_train_loss=2.2, batch_loss=2.18, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.4334  Val F1: 0.4485  (P=0.2891, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 2/2 [00:00<00:00, 31.19it/s, avg_train_loss=2.19, batch_loss=2.2, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.3765  Val F1: 0.3700  (P=0.2270, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 2/2 [00:00<00:00, 30.41it/s, avg_train_loss=2.19, batch_loss=2.18, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.3117  Val F1: 0.3764  (P=0.2318, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 2/2 [00:00<00:00, 36.56it/s, avg_train_loss=2.17, batch_loss=2.19, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.4273  Val F1: 0.3801  (P=0.2347, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|██████████| 2/2 [00:00<00:00, 35.24it/s, avg_train_loss=2.11, batch_loss=2.09, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.1767  Val F1: 0.4367  (P=0.2793, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|██████████| 2/2 [00:00<00:00, 34.91it/s, avg_train_loss=2.09, batch_loss=2.08, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.2390  Val F1: 0.4061  (P=0.2548, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 2/2 [00:00<00:00, 36.16it/s, avg_train_loss=2.08, batch_loss=2.04, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.0875  Val F1: 0.4413  (P=0.2831, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 2/2 [00:00<00:00, 34.23it/s, avg_train_loss=2.04, batch_loss=2.07, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.0728  Val F1: 0.4044  (P=0.2534, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|██████████| 2/2 [00:00<00:00, 35.84it/s, avg_train_loss=2.03, batch_loss=2.06, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.0276  Val F1: 0.4271  (P=0.2715, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|██████████| 2/2 [00:00<00:00, 31.27it/s, avg_train_loss=1.99, batch_loss=1.99, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.9605  Val F1: 0.4115  (P=0.2591, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|██████████| 2/2 [00:00<00:00, 34.68it/s, avg_train_loss=1.99, batch_loss=2, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.9010  Val F1: 0.4402  (P=0.2823, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 2/2 [00:00<00:00, 34.29it/s, avg_train_loss=2.07, batch_loss=2.11, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.2564  Val F1: 0.3012  (P=0.1773, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 2/2 [00:00<00:00, 34.90it/s, avg_train_loss=2.22, batch_loss=2.26, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.5171  Val F1: 0.2594  (P=0.1491, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 2/2 [00:00<00:00, 34.85it/s, avg_train_loss=2.14, batch_loss=2.07, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.8966  Val F1: 0.5546  (P=0.3946, R=0.9328)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 2/2 [00:00<00:00, 35.85it/s, avg_train_loss=2.28, batch_loss=2, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.1865  Val F1: 0.2979  (P=0.1750, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 2/2 [00:00<00:00, 34.30it/s, avg_train_loss=2.13, batch_loss=2.19, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.2238  Val F1: 0.3283  (P=0.1964, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 2/2 [00:00<00:00, 35.69it/s, avg_train_loss=2.1, batch_loss=2, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.2204  Val F1: 0.4131  (P=0.2603, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 2/2 [00:00<00:00, 34.20it/s, avg_train_loss=2.13, batch_loss=2.24, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.2165  Val F1: 0.3664  (P=0.2243, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 2/2 [00:00<00:00, 36.22it/s, avg_train_loss=2.22, batch_loss=2.41, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.5183  Val F1: 0.2233  (P=0.1257, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 2/2 [00:00<00:00, 31.88it/s, avg_train_loss=2.24, batch_loss=2.28, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.1469  Val F1: 0.4101  (P=0.2579, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 2/2 [00:00<00:00, 33.43it/s, avg_train_loss=2.05, batch_loss=2.03, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 4.1205  Val F1: 0.5146  (P=0.3465, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 2/2 [00:00<00:00, 30.60it/s, avg_train_loss=2.01, batch_loss=1.96, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.8471  Val F1: 0.4035  (P=0.2527, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 2/2 [00:00<00:00, 34.96it/s, avg_train_loss=1.92, batch_loss=1.95, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.8554  Val F1: 0.3577  (P=0.2178, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 2/2 [00:00<00:00, 30.15it/s, avg_train_loss=1.93, batch_loss=1.88, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.8323  Val F1: 0.3589  (P=0.2187, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 2/2 [00:00<00:00, 31.12it/s, avg_train_loss=1.92, batch_loss=1.91, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.7479  Val F1: 0.3651  (P=0.2233, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 2/2 [00:00<00:00, 29.93it/s, avg_train_loss=1.92, batch_loss=1.83, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.6842  Val F1: 0.3838  (P=0.2375, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 2/2 [00:00<00:00, 35.96it/s, avg_train_loss=1.83, batch_loss=1.84, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.6161  Val F1: 0.4219  (P=0.2673, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 2/2 [00:00<00:00, 31.56it/s, avg_train_loss=1.81, batch_loss=1.8, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.7247  Val F1: 0.4444  (P=0.2857, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 2/2 [00:00<00:00, 29.96it/s, avg_train_loss=1.79, batch_loss=1.75, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.5272  Val F1: 0.4424  (P=0.2840, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 2/2 [00:00<00:00, 30.31it/s, avg_train_loss=1.77, batch_loss=1.78, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4777  Val F1: 0.4450  (P=0.2862, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 2/2 [00:00<00:00, 34.72it/s, avg_train_loss=1.74, batch_loss=1.76, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4446  Val F1: 0.4552  (P=0.2947, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 2/2 [00:00<00:00, 32.47it/s, avg_train_loss=1.72, batch_loss=1.78, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.4040  Val F1: 0.4639  (P=0.3020, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 2/2 [00:00<00:00, 38.49it/s, avg_train_loss=1.69, batch_loss=1.74, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.2878  Val F1: 0.4419  (P=0.2836, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 2/2 [00:00<00:00, 32.10it/s, avg_train_loss=1.64, batch_loss=1.63, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.3091  Val F1: 0.4448  (P=0.2860, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 2/2 [00:00<00:00, 34.78it/s, avg_train_loss=1.62, batch_loss=1.65, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.1725  Val F1: 0.4685  (P=0.3059, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 2/2 [00:00<00:00, 35.05it/s, avg_train_loss=1.58, batch_loss=1.57, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.0908  Val F1: 0.4599  (P=0.2986, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 2/2 [00:00<00:00, 35.51it/s, avg_train_loss=1.55, batch_loss=1.53, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.0833  Val F1: 0.4700  (P=0.3072, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 2/2 [00:00<00:00, 30.44it/s, avg_train_loss=1.51, batch_loss=1.5, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 3.0741  Val F1: 0.5184  (P=0.3499, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 2/2 [00:00<00:00, 29.91it/s, avg_train_loss=1.48, batch_loss=1.45, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.9611  Val F1: 0.4855  (P=0.3206, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 2/2 [00:00<00:00, 30.57it/s, avg_train_loss=1.45, batch_loss=1.45, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.8430  Val F1: 0.4681  (P=0.3056, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 2/2 [00:00<00:00, 37.44it/s, avg_train_loss=1.44, batch_loss=1.48, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.7677  Val F1: 0.5149  (P=0.3467, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 2/2 [00:00<00:00, 31.36it/s, avg_train_loss=1.38, batch_loss=1.36, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.6870  Val F1: 0.4936  (P=0.3276, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 2/2 [00:00<00:00, 30.30it/s, avg_train_loss=1.35, batch_loss=1.36, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.6834  Val F1: 0.4857  (P=0.3208, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 2/2 [00:00<00:00, 30.07it/s, avg_train_loss=1.34, batch_loss=1.32, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.5897  Val F1: 0.5242  (P=0.3552, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|██████████| 2/2 [00:00<00:00, 35.79it/s, avg_train_loss=1.32, batch_loss=1.31, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.5702  Val F1: 0.5414  (P=0.3712, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|██████████| 2/2 [00:00<00:00, 37.41it/s, avg_train_loss=1.3, batch_loss=1.33, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.5371  Val F1: 0.4888  (P=0.3235, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|██████████| 2/2 [00:00<00:00, 35.46it/s, avg_train_loss=1.27, batch_loss=1.25, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.5045  Val F1: 0.5033  (P=0.3363, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|██████████| 2/2 [00:00<00:00, 29.71it/s, avg_train_loss=1.25, batch_loss=1.28, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.4244  Val F1: 0.5245  (P=0.3554, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|██████████| 2/2 [00:00<00:00, 33.96it/s, avg_train_loss=1.21, batch_loss=1.18, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3944  Val F1: 0.5333  (P=0.3636, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|██████████| 2/2 [00:00<00:00, 31.55it/s, avg_train_loss=1.21, batch_loss=1.25, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3696  Val F1: 0.5616  (P=0.3904, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|██████████| 2/2 [00:00<00:00, 29.76it/s, avg_train_loss=1.18, batch_loss=1.18, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3538  Val F1: 0.5660  (P=0.3947, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|██████████| 2/2 [00:00<00:00, 35.57it/s, avg_train_loss=1.16, batch_loss=1.14, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3953  Val F1: 0.5439  (P=0.3735, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 2/2 [00:00<00:00, 34.58it/s, avg_train_loss=1.14, batch_loss=1.14, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.2501  Val F1: 0.5384  (P=0.3684, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|██████████| 2/2 [00:00<00:00, 30.10it/s, avg_train_loss=1.12, batch_loss=1.12, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.2056  Val F1: 0.5514  (P=0.3807, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101: 100%|██████████| 2/2 [00:00<00:00, 30.38it/s, avg_train_loss=1.1, batch_loss=1.08, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.1688  Val F1: 0.5678  (P=0.3964, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102: 100%|██████████| 2/2 [00:00<00:00, 32.21it/s, avg_train_loss=1.08, batch_loss=1.1, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.2213  Val F1: 0.5788  (P=0.4073, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103: 100%|██████████| 2/2 [00:00<00:00, 35.99it/s, avg_train_loss=1.07, batch_loss=1.12, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.0836  Val F1: 0.5969  (P=0.4254, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104: 100%|██████████| 2/2 [00:00<00:00, 36.40it/s, avg_train_loss=1.04, batch_loss=1.06, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.0671  Val F1: 0.5975  (P=0.4261, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105: 100%|██████████| 2/2 [00:00<00:00, 36.37it/s, avg_train_loss=1.01, batch_loss=1, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.9888  Val F1: 0.5785  (P=0.4070, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106: 100%|██████████| 2/2 [00:00<00:00, 31.03it/s, avg_train_loss=1.01, batch_loss=1.06, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.9733  Val F1: 0.6006  (P=0.4291, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107: 100%|██████████| 2/2 [00:00<00:00, 30.36it/s, avg_train_loss=0.973, batch_loss=0.971, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.9070  Val F1: 0.5791  (P=0.4076, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108: 100%|██████████| 2/2 [00:00<00:00, 31.37it/s, avg_train_loss=0.964, batch_loss=0.924, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8802  Val F1: 0.5817  (P=0.4101, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109: 100%|██████████| 2/2 [00:00<00:00, 32.00it/s, avg_train_loss=0.938, batch_loss=0.973, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8539  Val F1: 0.6347  (P=0.4649, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110: 100%|██████████| 2/2 [00:00<00:00, 33.57it/s, avg_train_loss=0.963, batch_loss=1.09, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8058  Val F1: 0.6646  (P=0.4977, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111: 100%|██████████| 2/2 [00:00<00:00, 31.22it/s, avg_train_loss=0.99, batch_loss=1.11, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8594  Val F1: 0.5214  (P=0.3526, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112: 100%|██████████| 2/2 [00:00<00:00, 31.94it/s, avg_train_loss=0.943, batch_loss=0.967, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8167  Val F1: 0.5083  (P=0.3408, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113: 100%|██████████| 2/2 [00:00<00:00, 37.60it/s, avg_train_loss=0.998, batch_loss=1.14, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.6204  Val F1: 0.6122  (P=0.4412, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114: 100%|██████████| 2/2 [00:00<00:00, 36.20it/s, avg_train_loss=0.877, batch_loss=0.681, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.4739  Val F1: 0.6164  (P=0.4456, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115: 100%|██████████| 2/2 [00:00<00:00, 31.12it/s, avg_train_loss=0.742, batch_loss=0.665, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.3851  Val F1: 0.6370  (P=0.4673, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116: 100%|██████████| 2/2 [00:00<00:00, 33.50it/s, avg_train_loss=0.688, batch_loss=0.721, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.2079  Val F1: 0.6358  (P=0.4661, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117: 100%|██████████| 2/2 [00:00<00:00, 31.89it/s, avg_train_loss=0.646, batch_loss=0.548, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.0697  Val F1: 0.6218  (P=0.4512, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118: 100%|██████████| 2/2 [00:00<00:00, 33.34it/s, avg_train_loss=0.517, batch_loss=0.497, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9373  Val F1: 0.6084  (P=0.4372, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 2/2 [00:00<00:00, 34.00it/s, avg_train_loss=0.505, batch_loss=0.591, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9296  Val F1: 0.6339  (P=0.4641, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120: 100%|██████████| 2/2 [00:00<00:00, 32.43it/s, avg_train_loss=0.4, batch_loss=0.391, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8075  Val F1: 0.6225  (P=0.4519, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121: 100%|██████████| 2/2 [00:00<00:00, 32.17it/s, avg_train_loss=0.348, batch_loss=0.385, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6107  Val F1: 0.6287  (P=0.4585, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122: 100%|██████████| 2/2 [00:00<00:00, 32.31it/s, avg_train_loss=0.292, batch_loss=0.314, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6241  Val F1: 0.6939  (P=0.5312, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123: 100%|██████████| 2/2 [00:00<00:00, 31.41it/s, avg_train_loss=0.249, batch_loss=0.248, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4199  Val F1: 0.6679  (P=0.5014, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124: 100%|██████████| 2/2 [00:00<00:00, 32.29it/s, avg_train_loss=0.24, batch_loss=0.262, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5233  Val F1: 0.6136  (P=0.4426, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125: 100%|██████████| 2/2 [00:00<00:00, 36.92it/s, avg_train_loss=0.226, batch_loss=0.252, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3929  Val F1: 0.6396  (P=0.4702, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126: 100%|██████████| 2/2 [00:00<00:00, 31.47it/s, avg_train_loss=0.187, batch_loss=0.157, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3520  Val F1: 0.6916  (P=0.5286, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127: 100%|██████████| 2/2 [00:00<00:00, 30.50it/s, avg_train_loss=0.157, batch_loss=0.143, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2749  Val F1: 0.7002  (P=0.5387, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128: 100%|██████████| 2/2 [00:00<00:00, 33.61it/s, avg_train_loss=0.137, batch_loss=0.128, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3249  Val F1: 0.7415  (P=0.5899, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129: 100%|██████████| 2/2 [00:00<00:00, 37.33it/s, avg_train_loss=0.145, batch_loss=0.118, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2308  Val F1: 0.7109  (P=0.5514, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130: 100%|██████████| 2/2 [00:00<00:00, 36.15it/s, avg_train_loss=0.132, batch_loss=0.146, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2182  Val F1: 0.7229  (P=0.5660, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131: 100%|██████████| 2/2 [00:00<00:00, 37.08it/s, avg_train_loss=0.118, batch_loss=0.132, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2115  Val F1: 0.7195  (P=0.5618, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132: 100%|██████████| 2/2 [00:00<00:00, 37.95it/s, avg_train_loss=0.122, batch_loss=0.137, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2080  Val F1: 0.7011  (P=0.5398, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133: 100%|██████████| 2/2 [00:00<00:00, 35.85it/s, avg_train_loss=0.136, batch_loss=0.169, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2353  Val F1: 0.6948  (P=0.5323, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134: 100%|██████████| 2/2 [00:00<00:00, 36.43it/s, avg_train_loss=0.132, batch_loss=0.102, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2037  Val F1: 0.7614  (P=0.6147, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135: 100%|██████████| 2/2 [00:00<00:00, 39.59it/s, avg_train_loss=0.135, batch_loss=0.163, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1933  Val F1: 0.7679  (P=0.6233, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136: 100%|██████████| 2/2 [00:00<00:00, 35.36it/s, avg_train_loss=0.102, batch_loss=0.106, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1868  Val F1: 0.7403  (P=0.5877, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137: 100%|██████████| 2/2 [00:00<00:00, 30.38it/s, avg_train_loss=0.0984, batch_loss=0.0862, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1777  Val F1: 0.7576  (P=0.6098, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138: 100%|██████████| 2/2 [00:00<00:00, 31.79it/s, avg_train_loss=0.0912, batch_loss=0.0811, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1768  Val F1: 0.7825  (P=0.6427, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139: 100%|██████████| 2/2 [00:00<00:00, 30.82it/s, avg_train_loss=0.0845, batch_loss=0.0706, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1815  Val F1: 0.7842  (P=0.6450, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140: 100%|██████████| 2/2 [00:00<00:00, 38.20it/s, avg_train_loss=0.0793, batch_loss=0.0773, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1769  Val F1: 0.7768  (P=0.6351, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141: 100%|██████████| 2/2 [00:00<00:00, 35.57it/s, avg_train_loss=0.0781, batch_loss=0.0722, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1697  Val F1: 0.7762  (P=0.6343, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142: 100%|██████████| 2/2 [00:00<00:00, 43.03it/s, avg_train_loss=0.0768, batch_loss=0.0729, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1531  Val F1: 0.7911  (P=0.6545, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143: 100%|██████████| 2/2 [00:00<00:00, 29.81it/s, avg_train_loss=0.0733, batch_loss=0.0578, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1437  Val F1: 0.7911  (P=0.6545, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144: 100%|██████████| 2/2 [00:00<00:00, 33.55it/s, avg_train_loss=0.0714, batch_loss=0.0669, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1336  Val F1: 0.7906  (P=0.6537, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145: 100%|██████████| 2/2 [00:00<00:00, 31.12it/s, avg_train_loss=0.0703, batch_loss=0.0731, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1301  Val F1: 0.7947  (P=0.6593, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146: 100%|██████████| 2/2 [00:00<00:00, 37.60it/s, avg_train_loss=0.0659, batch_loss=0.0653, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1267  Val F1: 0.8024  (P=0.6700, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147: 100%|██████████| 2/2 [00:00<00:00, 35.96it/s, avg_train_loss=0.0641, batch_loss=0.0606, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1274  Val F1: 0.8072  (P=0.6768, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148: 100%|██████████| 2/2 [00:00<00:00, 30.19it/s, avg_train_loss=0.0626, batch_loss=0.0648, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1204  Val F1: 0.8158  (P=0.6889, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 2/2 [00:00<00:00, 32.10it/s, avg_train_loss=0.0607, batch_loss=0.0649, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1223  Val F1: 0.8196  (P=0.6943, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150: 100%|██████████| 2/2 [00:00<00:00, 29.87it/s, avg_train_loss=0.0606, batch_loss=0.057, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1131  Val F1: 0.8240  (P=0.7007, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151: 100%|██████████| 2/2 [00:00<00:00, 35.29it/s, avg_train_loss=0.0569, batch_loss=0.0558, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1398  Val F1: 0.8240  (P=0.7007, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152: 100%|██████████| 2/2 [00:00<00:00, 35.61it/s, avg_train_loss=0.064, batch_loss=0.0634, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1121  Val F1: 0.8323  (P=0.7128, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153: 100%|██████████| 2/2 [00:00<00:00, 37.51it/s, avg_train_loss=0.0546, batch_loss=0.0494, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1126  Val F1: 0.8488  (P=0.7373, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154: 100%|██████████| 2/2 [00:00<00:00, 30.96it/s, avg_train_loss=0.0614, batch_loss=0.0557, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1061  Val F1: 0.8323  (P=0.7128, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155: 100%|██████████| 2/2 [00:00<00:00, 38.03it/s, avg_train_loss=0.0535, batch_loss=0.0589, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1058  Val F1: 0.8388  (P=0.7224, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156: 100%|██████████| 2/2 [00:00<00:00, 36.83it/s, avg_train_loss=0.0531, batch_loss=0.059, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1000  Val F1: 0.8474  (P=0.7353, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157: 100%|██████████| 2/2 [00:00<00:00, 36.91it/s, avg_train_loss=0.0499, batch_loss=0.0463, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0996  Val F1: 0.8401  (P=0.7243, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158: 100%|██████████| 2/2 [00:00<00:00, 36.89it/s, avg_train_loss=0.0492, batch_loss=0.0586, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0982  Val F1: 0.8508  (P=0.7403, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159: 100%|██████████| 2/2 [00:00<00:00, 37.94it/s, avg_train_loss=0.0496, batch_loss=0.0408, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0945  Val F1: 0.8576  (P=0.7507, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160: 100%|██████████| 2/2 [00:00<00:00, 31.40it/s, avg_train_loss=0.0476, batch_loss=0.0495, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1184  Val F1: 0.8631  (P=0.7592, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161: 100%|██████████| 2/2 [00:00<00:00, 31.44it/s, avg_train_loss=0.0471, batch_loss=0.0564, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0971  Val F1: 0.8562  (P=0.7486, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162: 100%|██████████| 2/2 [00:00<00:00, 29.87it/s, avg_train_loss=0.0462, batch_loss=0.0517, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0877  Val F1: 0.8549  (P=0.7465, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163: 100%|██████████| 2/2 [00:00<00:00, 31.88it/s, avg_train_loss=0.0454, batch_loss=0.0409, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0893  Val F1: 0.8680  (P=0.7668, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164: 100%|██████████| 2/2 [00:00<00:00, 32.95it/s, avg_train_loss=0.0498, batch_loss=0.0647, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0882  Val F1: 0.8631  (P=0.7592, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165: 100%|██████████| 2/2 [00:00<00:00, 36.48it/s, avg_train_loss=0.0453, batch_loss=0.0391, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0944  Val F1: 0.8349  (P=0.7166, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166: 100%|██████████| 2/2 [00:00<00:00, 38.54it/s, avg_train_loss=0.0487, batch_loss=0.0684, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.0886  Val F1: 0.8576  (P=0.7507, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167: 100%|██████████| 2/2 [00:00<00:00, 31.43it/s, avg_train_loss=0.0478, batch_loss=0.0473, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1056  Val F1: 0.8652  (P=0.7624, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168: 100%|██████████| 2/2 [00:00<00:00, 40.51it/s, avg_train_loss=0.0651, batch_loss=0.0839, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1188  Val F1: 0.8140  (P=0.6863, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169: 100%|██████████| 2/2 [00:00<00:00, 35.25it/s, avg_train_loss=0.137, batch_loss=0.227, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.1801  Val F1: 0.7424  (P=0.5903, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170: 100%|██████████| 2/2 [00:00<00:00, 37.82it/s, avg_train_loss=0.103, batch_loss=0.113, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.8412  Val F1: 0.6329  (P=0.4851, R=0.9104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171: 100%|██████████| 2/2 [00:00<00:00, 32.71it/s, avg_train_loss=3.69, batch_loss=7.24, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5744  Val F1: 0.5826  (P=0.4110, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172: 100%|██████████| 2/2 [00:00<00:00, 32.52it/s, avg_train_loss=0.711, batch_loss=1.19, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.1713  Val F1: 0.4448  (P=0.2886, R=0.9701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173: 100%|██████████| 2/2 [00:00<00:00, 29.97it/s, avg_train_loss=1.37, batch_loss=1.19, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3280  Val F1: 0.2710  (P=0.1568, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174: 100%|██████████| 2/2 [00:00<00:00, 34.33it/s, avg_train_loss=1.09, batch_loss=1.02, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3776  Val F1: 0.4297  (P=0.2736, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175: 100%|██████████| 2/2 [00:00<00:00, 38.80it/s, avg_train_loss=1.21, batch_loss=1.23, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3924  Val F1: 0.2086  (P=0.1165, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176: 100%|██████████| 2/2 [00:00<00:00, 40.10it/s, avg_train_loss=1.05, batch_loss=1.07, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.3673  Val F1: 0.4852  (P=0.3228, R=0.9757)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177: 100%|██████████| 2/2 [00:00<00:00, 38.09it/s, avg_train_loss=1, batch_loss=0.706, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.3290  Val F1: 0.3688  (P=0.2261, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178: 100%|██████████| 2/2 [00:00<00:00, 39.41it/s, avg_train_loss=0.721, batch_loss=0.83, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.0836  Val F1: 0.4230  (P=0.2683, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179: 100%|██████████| 2/2 [00:00<00:00, 39.73it/s, avg_train_loss=0.517, batch_loss=0.479, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8403  Val F1: 0.5682  (P=0.3989, R=0.9869)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180: 100%|██████████| 2/2 [00:00<00:00, 39.21it/s, avg_train_loss=0.791, batch_loss=0.655, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8174  Val F1: 0.2879  (P=0.1682, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181: 100%|██████████| 2/2 [00:00<00:00, 36.93it/s, avg_train_loss=0.912, batch_loss=0.993, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.8221  Val F1: 0.2923  (P=0.1712, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182: 100%|██████████| 2/2 [00:00<00:00, 40.23it/s, avg_train_loss=1.12, batch_loss=0.737, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.1147  Val F1: 0.4192  (P=0.2652, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183: 100%|██████████| 2/2 [00:00<00:00, 32.87it/s, avg_train_loss=0.458, batch_loss=0.42, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.4806  Val F1: 0.6261  (P=0.4823, R=0.8918)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184: 100%|██████████| 2/2 [00:00<00:00, 30.03it/s, avg_train_loss=0.674, batch_loss=0.328, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8376  Val F1: 0.4621  (P=0.3004, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185: 100%|██████████| 2/2 [00:00<00:00, 32.08it/s, avg_train_loss=0.442, batch_loss=0.413, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9676  Val F1: 0.4307  (P=0.2744, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186: 100%|██████████| 2/2 [00:00<00:00, 30.05it/s, avg_train_loss=0.484, batch_loss=0.564, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9284  Val F1: 0.4508  (P=0.2910, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187: 100%|██████████| 2/2 [00:00<00:00, 36.44it/s, avg_train_loss=0.41, batch_loss=0.359, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8435  Val F1: 0.5078  (P=0.3405, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188: 100%|██████████| 2/2 [00:00<00:00, 38.01it/s, avg_train_loss=0.389, batch_loss=0.362, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.0240  Val F1: 0.5743  (P=0.4032, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189: 100%|██████████| 2/2 [00:00<00:00, 41.31it/s, avg_train_loss=0.381, batch_loss=0.223, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8159  Val F1: 0.5390  (P=0.3689, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190: 100%|██████████| 2/2 [00:00<00:00, 30.43it/s, avg_train_loss=0.286, batch_loss=0.207, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5281  Val F1: 0.5453  (P=0.3748, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191: 100%|██████████| 2/2 [00:00<00:00, 30.42it/s, avg_train_loss=0.272, batch_loss=0.196, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4864  Val F1: 0.5779  (P=0.4064, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192: 100%|██████████| 2/2 [00:00<00:00, 35.48it/s, avg_train_loss=0.309, batch_loss=0.265, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4654  Val F1: 0.6077  (P=0.4365, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193: 100%|██████████| 2/2 [00:00<00:00, 36.66it/s, avg_train_loss=0.248, batch_loss=0.215, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4056  Val F1: 0.6091  (P=0.4379, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194: 100%|██████████| 2/2 [00:00<00:00, 38.73it/s, avg_train_loss=0.242, batch_loss=0.183, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3823  Val F1: 0.6236  (P=0.4531, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195: 100%|██████████| 2/2 [00:00<00:00, 38.47it/s, avg_train_loss=0.192, batch_loss=0.161, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3799  Val F1: 0.6361  (P=0.4668, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196: 100%|██████████| 2/2 [00:00<00:00, 35.56it/s, avg_train_loss=0.187, batch_loss=0.14, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4195  Val F1: 0.6407  (P=0.4718, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197: 100%|██████████| 2/2 [00:00<00:00, 33.65it/s, avg_train_loss=0.199, batch_loss=0.228, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3760  Val F1: 0.6477  (P=0.4794, R=0.9981)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198: 100%|██████████| 2/2 [00:00<00:00, 31.45it/s, avg_train_loss=0.182, batch_loss=0.18, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3935  Val F1: 0.6377  (P=0.4681, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 2/2 [00:00<00:00, 39.40it/s, avg_train_loss=0.174, batch_loss=0.182, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3385  Val F1: 0.6446  (P=0.4756, R=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200: 100%|██████████| 2/2 [00:00<00:00, 39.06it/s, avg_train_loss=0.163, batch_loss=0.161, lr=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3146  Val F1: 0.6621  (P=0.4949, R=1.0000)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T14:35:50.104296Z",
     "start_time": "2025-05-20T14:35:50.099374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence of two 3x3 convolutions each followed by ReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "            #nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic U-Net architecture without attention, SE blocks, or deep supervision.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=None):\n",
    "        super(UNet, self).__init__()\n",
    "        if features is None:\n",
    "            features = [64, 128, 256, 512]\n",
    "\n",
    "        # Encoder path\n",
    "        self.downs = nn.ModuleList()\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Decoder path\n",
    "        self.ups = nn.ModuleList()\n",
    "        rev_features = features[::-1]\n",
    "        for feature in rev_features:\n",
    "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=3, stride=2))\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        # Final 1x1 conv\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Downsampling\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Reverse skip connections for decoding\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # Upsampling\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)  # ConvTranspose2d\n",
    "            skip = skip_connections[idx // 2]\n",
    "            # In case the inexact sizes due to pooling\n",
    "            if x.shape != skip.shape:\n",
    "                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "            x = torch.cat((skip, x), dim=1)\n",
    "            x = self.ups[idx + 1](x)  # DoubleConv\n",
    "\n",
    "        return self.final_conv(x)\n"
   ],
   "id": "593da583ed22f335",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T14:35:51.213034Z",
     "start_time": "2025-05-20T14:35:51.207181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        num   = 2 * (probs * targets).sum(dim=(1,2,3))\n",
    "        den   = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + self.eps\n",
    "        dice  = num / den\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bw = bce_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        return self.bw * self.bce(logits, targets) + (1 - self.bw) * self.dice(logits, targets)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma, self.eps = alpha, gamma, eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits).clamp(self.eps, 1 - self.eps)\n",
    "        pt = torch.where(targets==1, probs, 1 - probs)\n",
    "        w  = torch.where(targets==1, self.alpha, 1 - self.alpha)\n",
    "        loss = - w * (1 - pt)**self.gamma * pt.log()\n",
    "        return loss.mean()\n",
    "\n",
    "class TverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.eps = alpha, beta, eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        TP = (probs * targets).sum(dim=(1,2,3))\n",
    "        FP = (probs * (1-targets)).sum(dim=(1,2,3))\n",
    "        FN = ((1-probs) * targets).sum(dim=(1,2,3))\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return 1 - tversky.mean()\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.3, gamma=1.5, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.tversky = TverskyLoss(alpha, beta, eps)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        t = 1 - self.tversky(logits, targets)\n",
    "        return t ** self.gamma\n",
    "\n",
    "class TverskyF2Loss(nn.Module):\n",
    "    def forward(self, preds, targets):\n",
    "        TP = (preds*targets).sum()\n",
    "        FN = ((1-preds)*targets).sum()\n",
    "        FP = (preds*(1-targets)).sum()\n",
    "        return (FN + 1e-6) / (2*TP + FN + 1e-6)\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, beta=0.1, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.gamma, self.eps = alpha, beta, gamma, eps\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        TP = (preds * targets).sum()\n",
    "        FP = (preds * (1 - targets)).sum()\n",
    "        FN = ((1 - preds) * targets).sum()\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return torch.pow((1 - tversky), self.gamma)\n",
    "\n",
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=500.0, bce_weight=0.2, dice_weight=0.4, f2_weight=0.4):\n",
    "        super().__init__()\n",
    "        self.bce  = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight))\n",
    "        self.ft   = FocalTverskyLoss(alpha=0.9, beta=0.3, gamma=2.0)\n",
    "        self.tf2 = TverskyF2Loss()\n",
    "        self.bw, self.dw, self.fw= bce_weight, dice_weight, f2_weight\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss  = self.bce(logits, targets)\n",
    "        dice_loss = self.ft(torch.sigmoid(logits), targets)\n",
    "        f2_loss   = self.tf2(torch.sigmoid(logits), targets)\n",
    "        return self.bw*bce_loss + self.dw*dice_loss + self.fw*f2_loss\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self,path): self.path=path; self.f=None\n",
    "    def __len__(self): return h5py.File(self.path,'r')['x'].shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        if self.f is None:\n",
    "            self.f = h5py.File(self.path,'r', swmr=True)\n",
    "        x = torch.from_numpy(self.f['x'][idx]).float()\n",
    "        y = torch.from_numpy(self.f['y'][idx]).float()\n",
    "        return x, y"
   ],
   "id": "d21ad51dd95a8b10",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-20T14:37:39.529620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "if __name__ == '__main__':\n",
    "    bs, max_batches, epochs = 5, 500, 200\n",
    "    train_path = 'train.h5'\n",
    "    val_path   = 'val.h5'\n",
    "\n",
    "    # ─── datasets ───\n",
    "    train_ds = H5Dataset(train_path)\n",
    "    val_ds   = H5Dataset(val_path)\n",
    "\n",
    "    # ─── train loader ───\n",
    "    # pick exactly bs*max_batches random samples from train set\n",
    "    # total small‐loader size and split\n",
    "    num_samples = 50\n",
    "    num_pos     = num_samples // 2\n",
    "    num_neg     = num_samples - num_pos\n",
    "\n",
    "    # collect all pos/neg indices in order\n",
    "    pos_idx, neg_idx = [], []\n",
    "    for i in range(len(train_ds)):\n",
    "        _, mask = train_ds[i]\n",
    "        if mask.sum() > 0:\n",
    "            pos_idx.append(i)\n",
    "        else:\n",
    "            neg_idx.append(i)\n",
    "\n",
    "    # take the first N from each\n",
    "    selected_pos = pos_idx[:num_pos]\n",
    "    selected_neg = neg_idx[:num_neg]\n",
    "\n",
    "    # combine (optionally keep positives first, or interleave)\n",
    "    balanced_idx = selected_pos + selected_neg\n",
    "\n",
    "    small_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=5,\n",
    "        sampler=SubsetRandomSampler(balanced_idx),\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "    crit  = ComboLoss(pos_weight=100, bce_weight=0.5, dice_weight=0.5, f2_weight=0.0)\n",
    "    #crit = DiceLoss()\n",
    "    #crit = BCEDiceLoss(bce_weight=100)\n",
    "    #crit = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    #crit = FocalTverskyLoss(alpha=0.9, beta=0.1, gamma=1)\n",
    "\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    #sched = torch.optim.lr_scheduler.ConstantLR(opt, factor=1.0, total_iters=epochs)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=5e-4, steps_per_epoch=len(small_loader), epochs=epochs)\n",
    "\n",
    "    ds_weights = [1, 1, 1]\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # ——— train ———\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_num, (imgs, msk) in enumerate(pbar:=tqdm(small_loader, desc=f\"Epoch {epoch}\")):\n",
    "            #imgs[msk==1] = imgs[msk==1]\n",
    "            imgs = imgs/50\n",
    "            imgs = imgs.to(device)\n",
    "            msk  = msk.to(device)\n",
    "            opt.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                seg_logits = model(imgs)\n",
    "                loss = crit(seg_logits, msk)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            #loss.backward()\n",
    "            #opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(avg_train_loss=total_loss/(batch_num+1), batch_loss=loss.item(), lr=sched.get_last_lr()[0])\n",
    "            sched.step()\n",
    "\n",
    "        # ——— validate ———\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tp=fp=fn=0\n",
    "            loss = 0\n",
    "            for imgs, msk in small_loader:\n",
    "                #imgs[msk==1] = imgs[msk==1]\n",
    "                imgs = imgs/50\n",
    "                imgs = imgs.to(device)\n",
    "                msk  = msk.to(device)\n",
    "\n",
    "                seg_logits = model(imgs)\n",
    "                loss += crit(seg_logits, msk)\n",
    "                preds = (torch.sigmoid(seg_logits)>0.5).float().view(-1)\n",
    "                t     = msk.view(-1)\n",
    "\n",
    "                tp += (preds * t).sum().item()\n",
    "                fp += (preds * (1-t)).sum().item()\n",
    "                fn += ((1-preds)*t).sum().item()\n",
    "\n",
    "            prec = tp/(tp+fp+1e-8)\n",
    "            rec  = tp/(tp+fn+1e-8)\n",
    "            f1   = 2*prec*rec/(prec+rec+1e-8)\n",
    "            print(f\"Val loss: {loss:.4f}  Val F1: {f1:.4f}  (P={prec:.4f}, R={rec:.4f})\")\n"
   ],
   "id": "9053d5193df9f066",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [00:00<00:00, 31.33it/s, avg_train_loss=0.925, batch_loss=0.891, lr=2.03e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 9.1768  Val F1: 0.0039  (P=0.0020, R=0.9819)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 49.69it/s, avg_train_loss=0.91, batch_loss=0.894, lr=2.12e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 9.1233  Val F1: 0.0051  (P=0.0025, R=0.7938)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [00:00<00:00, 51.98it/s, avg_train_loss=0.905, batch_loss=0.973, lr=2.28e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 9.0104  Val F1: 0.0067  (P=0.0034, R=0.7402)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:00<00:00, 51.84it/s, avg_train_loss=0.896, batch_loss=0.89, lr=2.5e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.9084  Val F1: 0.0081  (P=0.0041, R=0.7701)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [00:00<00:00, 54.30it/s, avg_train_loss=0.884, batch_loss=0.866, lr=2.79e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.7703  Val F1: 0.0100  (P=0.0050, R=0.8037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [00:00<00:00, 53.87it/s, avg_train_loss=0.875, batch_loss=0.85, lr=3.14e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.6325  Val F1: 0.0129  (P=0.0065, R=0.8579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [00:00<00:00, 52.32it/s, avg_train_loss=0.862, batch_loss=0.887, lr=3.55e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.5514  Val F1: 0.0139  (P=0.0070, R=0.9234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [00:00<00:00, 50.26it/s, avg_train_loss=0.854, batch_loss=0.852, lr=4.03e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.5187  Val F1: 0.0128  (P=0.0064, R=0.9595)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:00<00:00, 48.62it/s, avg_train_loss=0.837, batch_loss=0.848, lr=4.57e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.6146  Val F1: 0.0133  (P=0.0067, R=0.9626)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 10/10 [00:00<00:00, 50.23it/s, avg_train_loss=0.822, batch_loss=0.799, lr=5.16e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 10.7647  Val F1: 0.0085  (P=0.0043, R=0.9838)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 10/10 [00:00<00:00, 55.31it/s, avg_train_loss=0.797, batch_loss=0.77, lr=5.82e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.8315  Val F1: 0.0367  (P=0.0187, R=0.9452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 10/10 [00:00<00:00, 54.95it/s, avg_train_loss=0.771, batch_loss=0.761, lr=6.52e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.3411  Val F1: 0.0132  (P=0.0067, R=0.9857)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 10/10 [00:00<00:00, 49.35it/s, avg_train_loss=0.747, batch_loss=0.746, lr=7.29e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.5249  Val F1: 0.0516  (P=0.0265, R=0.9819)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 10/10 [00:00<00:00, 47.86it/s, avg_train_loss=0.744, batch_loss=0.728, lr=8.1e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.7312  Val F1: 0.0301  (P=0.0153, R=0.9826)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 10/10 [00:00<00:00, 53.05it/s, avg_train_loss=0.723, batch_loss=0.698, lr=8.96e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.3549  Val F1: 0.0449  (P=0.0230, R=0.9838)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 10/10 [00:00<00:00, 53.49it/s, avg_train_loss=0.726, batch_loss=0.733, lr=9.87e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.7018  Val F1: 0.0291  (P=0.0148, R=0.9009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 10/10 [00:00<00:00, 50.57it/s, avg_train_loss=0.715, batch_loss=0.719, lr=0.000108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.4047  Val F1: 0.0348  (P=0.0177, R=0.9819)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 10/10 [00:00<00:00, 50.66it/s, avg_train_loss=0.688, batch_loss=0.706, lr=0.000118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.9004  Val F1: 0.1088  (P=0.0576, R=0.9782)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 10/10 [00:00<00:00, 48.74it/s, avg_train_loss=0.683, batch_loss=0.683, lr=0.000129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.8330  Val F1: 0.1134  (P=0.0601, R=0.9913)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 10/10 [00:00<00:00, 49.18it/s, avg_train_loss=0.683, batch_loss=0.733, lr=0.000139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.9412  Val F1: 0.0645  (P=0.0334, R=0.9595)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 10/10 [00:00<00:00, 48.75it/s, avg_train_loss=0.676, batch_loss=0.675, lr=0.00015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.3615  Val F1: 0.0448  (P=0.0229, R=0.9819)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 10/10 [00:00<00:00, 47.59it/s, avg_train_loss=0.667, batch_loss=0.662, lr=0.000162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.6816  Val F1: 0.1593  (P=0.0869, R=0.9533)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 10/10 [00:00<00:00, 49.54it/s, avg_train_loss=0.659, batch_loss=0.721, lr=0.000173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.5399  Val F1: 0.2071  (P=0.1162, R=0.9495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 10/10 [00:00<00:00, 52.58it/s, avg_train_loss=0.663, batch_loss=0.684, lr=0.000185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.2791  Val F1: 0.0384  (P=0.0197, R=0.8231)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 10/10 [00:00<00:00, 54.30it/s, avg_train_loss=0.687, batch_loss=0.727, lr=0.000197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.7841  Val F1: 0.0769  (P=0.0401, R=0.8997)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 10/10 [00:00<00:00, 52.34it/s, avg_train_loss=0.644, batch_loss=0.622, lr=0.000209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.6985  Val F1: 0.0213  (P=0.0108, R=0.8168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 10/10 [00:00<00:00, 49.57it/s, avg_train_loss=0.643, batch_loss=0.672, lr=0.000222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 19.8921  Val F1: 0.0053  (P=0.0027, R=0.9919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 10/10 [00:00<00:00, 48.01it/s, avg_train_loss=0.635, batch_loss=0.64, lr=0.000234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.4694  Val F1: 0.1206  (P=0.0646, R=0.9134)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 10/10 [00:00<00:00, 47.44it/s, avg_train_loss=0.638, batch_loss=0.639, lr=0.000247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.4009  Val F1: 0.0167  (P=0.0084, R=0.9745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 10/10 [00:00<00:00, 50.01it/s, avg_train_loss=0.629, batch_loss=0.632, lr=0.000259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.0269  Val F1: 0.1956  (P=0.1086, R=0.9888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 10/10 [00:00<00:00, 46.84it/s, avg_train_loss=0.601, batch_loss=0.56, lr=0.000272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.9750  Val F1: 0.4483  (P=0.2900, R=0.9875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 10/10 [00:00<00:00, 51.45it/s, avg_train_loss=0.59, batch_loss=0.588, lr=0.000285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.8761  Val F1: 0.3378  (P=0.2037, R=0.9900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 10/10 [00:00<00:00, 46.84it/s, avg_train_loss=0.596, batch_loss=0.581, lr=0.000297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.4532  Val F1: 0.0802  (P=0.0418, R=0.9626)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 10/10 [00:00<00:00, 52.15it/s, avg_train_loss=0.603, batch_loss=0.638, lr=0.000309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.4373  Val F1: 0.1769  (P=0.1025, R=0.6461)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 10/10 [00:00<00:00, 47.62it/s, avg_train_loss=0.592, batch_loss=0.597, lr=0.000322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.4272  Val F1: 0.0146  (P=0.0074, R=0.9452)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 10/10 [00:00<00:00, 51.66it/s, avg_train_loss=0.58, batch_loss=0.598, lr=0.000334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.8164  Val F1: 0.4561  (P=0.3121, R=0.8474)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 10/10 [00:00<00:00, 49.15it/s, avg_train_loss=0.583, batch_loss=0.592, lr=0.000346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 10.0656  Val F1: 0.0078  (P=0.0039, R=0.9103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 10/10 [00:00<00:00, 51.31it/s, avg_train_loss=0.569, batch_loss=0.573, lr=0.000357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 11.6360  Val F1: 0.0116  (P=0.0059, R=0.9938)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 10/10 [00:00<00:00, 51.69it/s, avg_train_loss=0.592, batch_loss=0.55, lr=0.000369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.9324  Val F1: 0.1050  (P=0.0558, R=0.8847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 10/10 [00:00<00:00, 53.64it/s, avg_train_loss=0.58, batch_loss=0.585, lr=0.00038] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.7674  Val F1: 0.0885  (P=0.0463, R=0.9900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 10/10 [00:00<00:00, 46.12it/s, avg_train_loss=0.573, batch_loss=0.59, lr=0.00039] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 8.5872  Val F1: 0.0146  (P=0.0074, R=0.9539)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 10/10 [00:00<00:00, 46.51it/s, avg_train_loss=0.552, batch_loss=0.546, lr=0.000401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.3670  Val F1: 0.4735  (P=0.3176, R=0.9296)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 10/10 [00:00<00:00, 49.84it/s, avg_train_loss=0.556, batch_loss=0.532, lr=0.000411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 7.4565  Val F1: 0.0231  (P=0.0117, R=0.9869)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 10/10 [00:00<00:00, 52.16it/s, avg_train_loss=0.556, batch_loss=0.511, lr=0.00042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.4109  Val F1: 0.3423  (P=0.2106, R=0.9128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 10/10 [00:00<00:00, 48.13it/s, avg_train_loss=0.512, batch_loss=0.481, lr=0.000429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.3122  Val F1: 0.1909  (P=0.1063, R=0.9364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46:  50%|█████     | 5/10 [00:00<00:00, 46.28it/s, avg_train_loss=0.51, batch_loss=0.553, lr=0.000437] "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T14:30:24.163813Z",
     "start_time": "2025-05-20T14:30:23.394686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    bs, max_batches, epochs = 32, 500, 100\n",
    "    train_path = 'train.h5'\n",
    "    val_path   = 'val.h5'\n",
    "\n",
    "    # ─── datasets ───\n",
    "    train_ds = H5Dataset(train_path)\n",
    "    val_ds   = H5Dataset(val_path)\n",
    "\n",
    "    # ─── train loader ───\n",
    "    # pick exactly bs*max_batches random samples from train set\n",
    "    tr_idx = torch.randperm(len(train_ds))[: bs * max_batches]\n",
    "    tr_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=bs,\n",
    "        sampler=SubsetRandomSampler(tr_idx),\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "\n",
    "    # ─── val loader ───\n",
    "    # just iterate through val.h5 in order (or set shuffle=True if you like)\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=bs,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "    crit  = ComboLoss(pos_weight=100, bce_weight=0.5, dice_weight=0.5, f2_weight=0.0)\n",
    "    #crit = DiceLoss()\n",
    "    #crit = BCEDiceLoss(bce_weight=100)\n",
    "    #crit = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    #crit = FocalTverskyLoss(alpha=0.9, beta=0.1, gamma=1)\n",
    "\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    #sched = torch.optim.lr_scheduler.ConstantLR(opt, factor=1.0, total_iters=epochs)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    opt, max_lr=1e-3,\n",
    "    steps_per_epoch=len(tr_loader), epochs=epochs\n",
    ")\n",
    "    #sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20, T_mult=3)\n",
    "\n",
    "    ds_weights = [1, 1, 1]\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # ——— train ———\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_num, (imgs, msk) in enumerate(pbar:=tqdm(tr_loader, desc=f\"Epoch {epoch}\")):\n",
    "            #imgs[msk==1] = imgs[msk==1] * 3\n",
    "            imgs = imgs/50\n",
    "            imgs = imgs.to(device)\n",
    "            msk  = msk.to(device)\n",
    "            opt.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                seg_logits = model(imgs)\n",
    "                loss = crit(seg_logits, msk)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=4.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            #loss.backward()\n",
    "            #opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(avg_train_loss=total_loss/(batch_num+1), batch_loss=loss.item(), lr=sched.get_last_lr()[0])\n",
    "            sched.step()\n",
    "\n",
    "        # ——— validate ———\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tp=fp=fn=0\n",
    "            loss = 0\n",
    "            for imgs, msk in val_loader:\n",
    "                #imgs[msk==1] = imgs[msk==1] * 3\n",
    "                imgs = imgs/50\n",
    "                imgs = imgs.to(device)\n",
    "                msk  = msk.to(device)\n",
    "\n",
    "                seg_logits = model(imgs)\n",
    "                loss += crit(seg_logits, msk)\n",
    "                preds = (torch.sigmoid(seg_logits)>0.5).float().view(-1)\n",
    "                t     = msk.view(-1)\n",
    "\n",
    "                tp += (preds * t).sum().item()\n",
    "                fp += (preds * (1-t)).sum().item()\n",
    "                fn += ((1-preds)*t).sum().item()\n",
    "\n",
    "            prec = tp/(tp+fp+1e-8)\n",
    "            rec  = tp/(tp+fn+1e-8)\n",
    "            f1   = 2*prec*rec/(prec+rec+1e-8)\n",
    "            print(f\"Val loss: {loss:.4f}  Val F1: {f1:.4f}  (P={prec:.4f}, R={rec:.4f})\")\n"
   ],
   "id": "ef87b2803dd13446",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/469 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 15.48 GiB of which 1.59 GiB is free. Process 2857 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 12.85 GiB memory in use. Of the allocated memory 9.55 GiB is allocated by PyTorch, and 2.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[89]\u001B[39m\u001B[32m, line 64\u001B[39m\n\u001B[32m     62\u001B[39m opt.zero_grad()\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m autocast(\u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     seg_logits = model(imgs)\n\u001B[32m     65\u001B[39m     loss = crit(seg_logits, msk)\n\u001B[32m     66\u001B[39m scaler.scale(loss).backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[82]\u001B[39m\u001B[32m, line 60\u001B[39m, in \u001B[36mUNet.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     58\u001B[39m     x = down(x)\n\u001B[32m     59\u001B[39m     skip_connections.append(x)\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     x = F.max_pool2d(x, kernel_size=\u001B[32m4\u001B[39m, stride=\u001B[32m1\u001B[39m)\n\u001B[32m     62\u001B[39m \u001B[38;5;66;03m# Bottleneck\u001B[39;00m\n\u001B[32m     63\u001B[39m x = \u001B[38;5;28mself\u001B[39m.bottleneck(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/_jit_internal.py:622\u001B[39m, in \u001B[36mboolean_dispatch.<locals>.fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m if_true(*args, **kwargs)\n\u001B[32m    621\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m622\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m if_false(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/functional.py:830\u001B[39m, in \u001B[36m_max_pool2d\u001B[39m\u001B[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001B[39m\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stride \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    829\u001B[39m     stride = torch.jit.annotate(\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m], [])\n\u001B[32m--> \u001B[39m\u001B[32m830\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m torch.max_pool2d(\u001B[38;5;28minput\u001B[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 15.48 GiB of which 1.59 GiB is free. Process 2857 has 258.00 MiB memory in use. Including non-PyTorch memory, this process has 12.85 GiB memory in use. Of the allocated memory 9.55 GiB is allocated by PyTorch, and 2.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4395af39a2ea733f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
