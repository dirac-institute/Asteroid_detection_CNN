{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    1) Split the input (B, 1, H, W) into non-overlapping patches of size (patch_size × patch_size).\n",
    "    2) Flatten each patch to a vector of length (patch_size^2).\n",
    "    3) Project that to an embedding of dimension embed_dim via a learnable linear layer.\n",
    "\n",
    "    Here we implement 2) and 3) as a single Conv2d with stride=patch_size, kernel_size=patch_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1, embed_dim=256, patch_size=16, img_size=128):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size  # number of patches along H (and W)\n",
    "        self.num_patches = self.grid_size * self.grid_size\n",
    "\n",
    "        # Conv2d: (in_ch)→(embed_dim), kernel=patch_size, stride=patch_size\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        # After this conv, an input of shape (B, in_ch, H, W) becomes (B, embed_dim, H/ps, W/ps).\n",
    "        # We’ll flatten that spatial grid to (num_patches, embed_dim) later.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, H, W)\n",
    "        returns: (B, num_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)              # (B, embed_dim, grid_size, grid_size)\n",
    "        x = x.flatten(2)              # (B, embed_dim, grid_size*grid_size)\n",
    "        x = x.transpose(1, 2)         # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"One standard Transformer encoder block with multi-head self-attention and MLP.\"\"\"\n",
    "    def __init__(self, embed_dim=256, num_heads=8, mlp_ratio=4.0, qkv_bias=True, drop=0.1, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            bias=qkv_bias,\n",
    "            dropout=attn_drop,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        mlp_hidden = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden, embed_dim),\n",
    "            nn.Dropout(drop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, N, embed_dim), where N = num_patches\n",
    "        returns: (B, N, embed_dim)\n",
    "        \"\"\"\n",
    "        # Self-attention expects (B, N, E) with batch_first=True\n",
    "        x_norm = self.norm1(x)\n",
    "        # MultiheadAttention returns (B, N, embed_dim)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self.drop1(attn_out)\n",
    "\n",
    "        x_norm2 = self.norm2(x)\n",
    "        mlp_out = self.mlp(x_norm2)\n",
    "        x = x + mlp_out\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"A stack of L TransformerEncoderBlock layers.\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_layers=6,\n",
    "                 embed_dim=256,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=4.0,\n",
    "                 qkv_bias=True,\n",
    "                 drop_rate=0.1,\n",
    "                 attn_drop_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, num_patches, embed_dim)\n",
    "        returns: (B, num_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        for blk in self.layers:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleSegTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A “from-scratch” transformer segmentation network:\n",
    "      1) PatchEmbedding → tokens of shape (B, N, E)\n",
    "      2) Add learnable Positional Embeddings (N, E)\n",
    "      3) TransformerEncoder backbone (L layers)\n",
    "      4) A simple decoder: reshape tokens back to a 2D grid, then apply a few\n",
    "         2D transposed convolutions to upsample to the desired mask size.\n",
    "    No pretrained weights—everything is initialized randomly.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 img_size=128,\n",
    "                 patch_size=16,\n",
    "                 in_ch=1,\n",
    "                 embed_dim=256,\n",
    "                 depth=6,\n",
    "                 num_heads=8,\n",
    "                 mlp_ratio=4.0,\n",
    "                 drop_rate=0.1,\n",
    "                 attn_drop_rate=0.0,\n",
    "                 decoder_channels=[256, 128, 64],\n",
    "                 out_ch=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size: input image height = width (assume square).\n",
    "            patch_size: size of each patch (e.g. 16 → 8×8 patches for 128×128).\n",
    "            in_ch: number of input channels (1 for grayscale).\n",
    "            embed_dim: token dimension inside the Transformer.\n",
    "            depth: number of Transformer layers.\n",
    "            num_heads: number of attention heads.\n",
    "            mlp_ratio: expansion factor for the MLP inside each Transformer block.\n",
    "            drop_rate, attn_drop_rate: dropout rates.\n",
    "            decoder_channels: list of channel sizes in the decoder.\n",
    "                              Should correspond to successive upsampling stages.\n",
    "            out_ch: number of output channels (1 for binary mask).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = img_size // patch_size       # e.g. 128//16 = 8 → 8×8 = 64 patches\n",
    "        self.num_patches = self.grid_size * self.grid_size  # 64\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # 1) Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            in_ch=in_ch,\n",
    "            embed_dim=embed_dim,\n",
    "            patch_size=patch_size,\n",
    "            img_size=img_size\n",
    "        )\n",
    "\n",
    "        # 2) Position embeddings: one per patch\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        # 3) Transformer encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=depth,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate\n",
    "        )\n",
    "\n",
    "        # 4) Decoder: reshape tokens → (B, embed_dim, grid_size, grid_size),\n",
    "        # then do a series of upsampling via ConvTranspose2d blocks.\n",
    "        dec_blocks = []\n",
    "        curr_ch = embed_dim\n",
    "        for out_ch_dec in decoder_channels:\n",
    "            # Each block upsamples spatially by a factor of 2:\n",
    "            dec_blocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(curr_ch, out_ch_dec, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(out_ch_dec),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ))\n",
    "            curr_ch = out_ch_dec\n",
    "        self.decoder_blocks = nn.ModuleList(dec_blocks)\n",
    "\n",
    "        # Final 1×1 conv to map to mask channel:\n",
    "        self.head = nn.Conv2d(curr_ch, out_ch, kernel_size=1)\n",
    "\n",
    "        # Initialize decoder weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize PatchEmbedding and head conv\n",
    "        nn.init.kaiming_normal_(self.patch_embed.proj.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if self.patch_embed.proj.bias is not None:\n",
    "            nn.init.zeros_(self.patch_embed.proj.bias)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "        nn.init.kaiming_normal_(self.head.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        # Decoder blocks are ConvTranspose2d + BatchNorm + ReLU; BatchNorm is initialized by default.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, img_size, img_size), e.g. (B, 1, 128, 128)\n",
    "        returns: (B, out_ch, img_size, img_size) - a mask in [0,1] after sigmoid\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # 1) Patch embedding → (B, num_patches, embed_dim)\n",
    "        x_tokens = self.patch_embed(x)\n",
    "\n",
    "        # 2) Add position embeddings\n",
    "        x_tokens = x_tokens + self.pos_embed   # broadcast pos_embed across batch\n",
    "\n",
    "        # 3) Transformer encoder\n",
    "        x_enc = self.encoder(x_tokens)         # (B, num_patches, embed_dim)\n",
    "\n",
    "        # 4) Reshape tokens → (B, embed_dim, grid_size, grid_size)\n",
    "        H = W = self.grid_size\n",
    "        x_grid = x_enc.transpose(1, 2).reshape(B, self.embed_dim, H, W)\n",
    "\n",
    "        # 5) Decoder: successive upsampling\n",
    "        for dec in self.decoder_blocks:\n",
    "            x_grid = dec(x_grid)\n",
    "            # e.g. from (B, E, H, W)→(B, dec_ch1, 2H, 2W)→(B, dec_ch2, 4H, 4W) etc.\n",
    "\n",
    "        # 6) Now x_grid should have spatial dims = img_size (if decoder_channels were chosen to match).\n",
    "        #    If not exactly img_size, we can interpolate:\n",
    "        x_out = F.interpolate(x_grid, size=(self.patch_size * self.grid_size, self.patch_size * self.grid_size),\n",
    "                              mode=\"bilinear\", align_corners=False)\n",
    "        # Final 1×1 conv → (B, out_ch, img_size, img_size)\n",
    "        mask_logits = self.head(x_out)\n",
    "\n",
    "        # 7) Sigmoid for binary mask\n",
    "        mask = torch.sigmoid(mask_logits)\n",
    "        return mask\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds:   Tensor (B,1,H,W) after Sigmoid\n",
    "        targets: Tensor (B,1,H,W) binary {0,1}\n",
    "        \"\"\"\n",
    "        p_flat = preds.view(-1)\n",
    "        t_flat = targets.view(-1)\n",
    "        intersection = (p_flat * t_flat).sum()\n",
    "        dice_coeff = (2. * intersection + self.smooth) / (p_flat.sum() + t_flat.sum() + self.smooth)\n",
    "        return 1 - dice_coeff\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.9, gamma=2.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.gamma, self.eps = alpha, gamma, eps\n",
    "        self.beta = 1 - alpha  # Ensure alpha + beta = 1\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = preds.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        TP = (preds * targets).sum()\n",
    "        FP = (preds * (1 - targets)).sum()\n",
    "        FN = ((1 - preds) * targets).sum()\n",
    "        tversky = (TP + self.eps) / (TP + self.alpha*FN + self.beta*FP + self.eps)\n",
    "        return torch.pow((1 - tversky), self.gamma)\n",
    "\n",
    "class ComboLossTF(nn.Module):\n",
    "    def __init__(self, bce_weight=0.33, dice_weight=0.33, focal_twersky_weight=0.33):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss(smooth=1e-6)\n",
    "        self.FW = FocalTverskyLoss (alpha = 0.99, gamma=3.1)\n",
    "        self.bw, self.dw, self.fw = bce_weight, dice_weight, focal_twersky_weight\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # preds, targets both (B,1,H,W)\n",
    "        l_bce = self.bce(preds, targets)\n",
    "        l_dice = self.dice(preds, targets)\n",
    "        l_focal_tversky = self.FW(preds, targets)\n",
    "        return self.bw * l_bce + self.dw * l_dice + self.fw * l_focal_tversky"
   ],
   "id": "ce34c9c75e1b8385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigzi(x, axis=None):\n",
    "    \"\"\"\n",
    "Compute the interquartile range (IQR) of x along the specified axis.\n",
    "    Args:\n",
    "        x: array-like, shape (P, H, W) or (H, W) or (N, C, H, W)\n",
    "        axis: axis along which to compute the IQR.\n",
    "              If None, computes over the flattened array.\n",
    "\n",
    "    Returns: float, the IQR of x.\n",
    "\n",
    "    \"\"\"\n",
    "    return 0.741 * (np.percentile(x, 75, axis=axis) - np.percentile(x, 25, axis=axis))\n",
    "\n",
    "def split_stack(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Split a stack of 2D panels into (nrows × ncols) tiles.\n",
    "    arr: ndarray, shape (P, H, W)\n",
    "    Returns: ndarray, shape (P * (H//nrows)*(W//ncols), nrows, ncols)\n",
    "    \"\"\"\n",
    "    P, H, W = arr.shape\n",
    "    pad_h = (-H) % nrows\n",
    "    pad_w = (-W) % ncols\n",
    "    if pad_h or pad_w:\n",
    "        arr = np.pad(arr,\n",
    "                     ((0, 0),\n",
    "                      (0, pad_h),\n",
    "                      (0, pad_w)),\n",
    "                     mode='constant',\n",
    "                     constant_values=0)\n",
    "    H2, W2 = arr.shape[1], arr.shape[2]\n",
    "    blocks = (arr\n",
    "              .reshape(P,\n",
    "                       H2 // nrows, nrows,\n",
    "                       W2 // ncols, ncols)\n",
    "              .swapaxes(2, 3))\n",
    "    P2, Hb, Wb, nr, nc = blocks.shape\n",
    "    out = blocks.reshape(P2 * Hb * Wb, nr, nc)\n",
    "    return out\n",
    "\n",
    "def build_datasets(npz_file, tile_size=128):\n",
    "    \"\"\"\n",
    "    Load data from .npz, clip exactly as TF did, split into tiles, return PyTorch tensors.\n",
    "      - Clips x to [-166.43, 169.96]\n",
    "      - Splits each large image into (tile_size × tile_size) patches\n",
    "      - Adds a channel dimension (→ shape (N, 1, tile_size, tile_size))\n",
    "    \"\"\"\n",
    "    data = np.load(npz_file)\n",
    "    x = data['x']  # shape (P, H, W)\n",
    "    y = data['y']\n",
    "\n",
    "    x = x/sigzi(x)  # normalize by interquartile range\n",
    "    x = np.clip(x, -5, 5) # clip to [-5, 5]\n",
    "\n",
    "    # Split into tiles (tile_size × tile_size)\n",
    "    x_tiles = split_stack(x, tile_size, tile_size)  # (N_tiles, tile_size, tile_size)\n",
    "    y_tiles = split_stack(y, tile_size, tile_size)\n",
    "\n",
    "    # Convert to FloatTensor and add channel dimension\n",
    "    x_tiles = torch.from_numpy(x_tiles).float().unsqueeze(1)  # (N, 1, tile_size, tile_size)\n",
    "    y_tiles = torch.from_numpy(y_tiles).float().unsqueeze(1)  # (N, 1, tile_size, tile_size)\n",
    "\n",
    "    return x_tiles, y_tiles\n",
    "\n",
    "def reshape_masks(masks, new_size):\n",
    "    \"\"\"\n",
    "    Resize binary masks (0/1) to `new_size`:\n",
    "      - Uses bilinear interpolation (same as TF’s tf.image.resize with bilinear)\n",
    "      - Applies torch.ceil(...) to recover {0,1} values exactly.\n",
    "    Input:\n",
    "      - masks: either a Tensor of shape (N, 1, H_orig, W_orig)\n",
    "               or a numpy array of shape (N, H_orig, W_orig)\n",
    "      - new_size: tuple (new_H, new_W)\n",
    "    Returns:\n",
    "      - Tensor of shape (N, 1, new_H, new_W), values in {0,1}\n",
    "    \"\"\"\n",
    "    if isinstance(masks, np.ndarray):\n",
    "        m = torch.from_numpy(masks).float().unsqueeze(1)  # → (N,1,H,W)\n",
    "    else:\n",
    "        m = masks  # assume already FloatTensor (N,1,H,W)\n",
    "    m_resized = F.interpolate(m, size=new_size, mode='bilinear', align_corners=False)\n",
    "    m_resized = torch.ceil(m_resized)\n",
    "    return m_resized.clamp(0, 1)\n",
    "\n",
    "def split_train_val(x_tiles, y_tiles, train_frac=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Shuffle and split x_tiles, y_tiles into two TensorDatasets: train (80%) and val (20%).\n",
    "    \"\"\"\n",
    "    n = x_tiles.shape[0]\n",
    "    idx = torch.randperm(n, generator=torch.Generator().manual_seed(seed))\n",
    "    split = int(train_frac * n)\n",
    "    train_idx = idx[:split]\n",
    "    val_idx   = idx[split:]\n",
    "    x_tr, y_tr = x_tiles[train_idx], y_tiles[train_idx]\n",
    "    x_val, y_val = x_tiles[val_idx], y_tiles[val_idx]\n",
    "    return TensorDataset(x_tr, y_tr), TensorDataset(x_val, y_val)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "def make_balanced_sampler(train_ds: TensorDataset):\n",
    "    \"\"\"\n",
    "    Given train_ds.tensors = (x_tensor, y_tensor), where\n",
    "      - x_tensor:  (N, C, H, W) float\n",
    "      - y_tensor:  (N, 1, H, W) float or (N, H, W) float\n",
    "    Returns a WeightedRandomSampler so that positives (mask sum>0)\n",
    "    and negatives (mask sum==0) are drawn equally often.\n",
    "\n",
    "    Usage:\n",
    "        sampler = make_balanced_sampler(train_ds)\n",
    "        loader  = DataLoader(train_ds, batch_size=32, sampler=sampler, num_workers=4)\n",
    "    \"\"\"\n",
    "    xs, ys = train_ds.tensors\n",
    "\n",
    "    # Ensure ys is shape (N,1,H,W)\n",
    "    if ys.ndim == 3:\n",
    "        ys = ys.unsqueeze(1)\n",
    "    assert ys.ndim == 4 and ys.shape[1] == 1\n",
    "\n",
    "    N = ys.shape[0]\n",
    "    # 1) compute “class” for each example: 1 if ANY pixel>0, else 0\n",
    "    with torch.no_grad():\n",
    "        flat = ys.view(N, -1).sum(dim=1)   # shape (N,)\n",
    "        is_pos = (flat > 0).long()         # 1 if positive, 0 if negative\n",
    "\n",
    "    n_pos = int(is_pos.sum().item())\n",
    "    n_neg = N - n_pos\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        # no balancing possible\n",
    "        print(\"Warning: no positives or no negatives in the training set; sampler will be unbalanced.\")\n",
    "        weights = torch.ones(N)\n",
    "    else:\n",
    "        # 2) assign weight to each index:\n",
    "        #    weight = 1 / (count of examples in that class)\n",
    "        #    so that drawing with replacement evens out pos vs neg\n",
    "        weights = torch.empty(N, dtype=torch.double)\n",
    "        weights[is_pos == 1] = 1.0 / n_pos\n",
    "        weights[is_pos == 0] = 1.0 / n_neg\n",
    "\n",
    "    # 3) create the WeightedRandomSampler:\n",
    "    #    - we set num_samples = N (so each epoch is “N draws with replacement”)\n",
    "    #    - replacement=True means we are allowed to pick the same index multiple times\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=N,\n",
    "        replacement=True,\n",
    "    )\n",
    "    return sampler\n"
   ],
   "id": "42a4a4836fa5e98f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "npz_file = \"../DATA/train.npz\"\n",
    "x_tiles, y_tiles = build_datasets(npz_file, tile_size=256)\n",
    "train_ds, val_ds = split_train_val(x_tiles, y_tiles, train_frac=0.8, seed=42)\n",
    "del x_tiles, y_tiles  # free memory"
   ],
   "id": "4114fba9f242c18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn.metrics\n",
    "\n",
    "# 1) Instantiate the new model (no pretrained weights).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleSegTransformer(\n",
    "    img_size=256,\n",
    "    patch_size=64,\n",
    "    in_ch=1,\n",
    "    embed_dim=32,\n",
    "    depth=3,            # 6 Transformer layers\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    drop_rate=0.5,\n",
    "    attn_drop_rate=0.5,\n",
    "    decoder_channels=[256, 128, 64],\n",
    "    out_ch=1\n",
    ").to(device)\n",
    "\n",
    "# 2) Choose an optimizer + scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# A common schedule for transformers from scratch is CosineAnnealingWarmRestarts:\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# 3) Loss function (your ComboLossTF from before):\n",
    "criterion = ComboLossTF(bce_weight=0.25, dice_weight=0.25, focal_twersky_weight=0.5)\n",
    "\n",
    "# 4) DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 5) Training loop\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs+1):\n",
    "    # ——— Training ———\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    tp = fp = fn = 0\n",
    "    for batch_idx, (imgs, masks) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)             # (B, 1, 128, 128)\n",
    "        masks = masks.to(device)           # (B, 128, 128), dtype float or long\n",
    "\n",
    "        # Forward pass → (B, 1, 128, 128)\n",
    "        preds = model(imgs)\n",
    "\n",
    "        # Make sure masks are shaped (B, 1, 128, 128):\n",
    "        if masks.ndim == 3:\n",
    "            masks = masks.unsqueeze(1)\n",
    "\n",
    "        loss = criterion(preds, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_bin = (preds > 0.5).float()\n",
    "            tp += (pred_bin * masks).sum().item()\n",
    "            fp += (pred_bin * (1 - masks)).sum().item()\n",
    "            fn += ((1 - pred_bin) * masks).sum().item()\n",
    "\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "        print(f\"\\rEpoch {epoch:03d}  Batch {batch_idx+1:03d}/{len(train_loader):03d}  \"\n",
    "              f\"Batch Loss: {loss.item():.4f}  | Train F1: {f1:.4f}  | Prec: {prec:.4f}  | Rec: {rec:.4f}\", end=\"\")\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ——— Validation ———\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    tp = fp = fn = 0\n",
    "    all_masks = []\n",
    "    all_preds  = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            if masks.ndim == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, masks)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            pred_bin = (preds > 0.5).float()\n",
    "            tp += (pred_bin * masks).sum().item()\n",
    "            fp += (pred_bin * (1 - masks)).sum().item()\n",
    "            fn += ((1 - pred_bin) * masks).sum().item()\n",
    "\n",
    "            all_masks.append(masks.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    rec  = tp / (tp + fn + 1e-8)\n",
    "    f1_val = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "\n",
    "    # Compute AUC on flattened probabilities\n",
    "    #all_masks = torch.cat(all_masks, dim=0).numpy().ravel()\n",
    "    #all_preds = torch.cat(all_preds, dim=0).numpy().ravel()\n",
    "    #auc_val = sklearn.metrics.roc_auc_score(all_masks, all_preds)\n",
    "\n",
    "    print(f\"\\rEpoch {epoch:03d}  \"\n",
    "          f\"Train Loss: {train_loss:.4f}  | Val Loss: {val_loss:.4f}  \"\n",
    "          f\"| Val F1: {f1_val:.4f}  | Val Prec: {prec:.4f}  | Val Rec: {rec:.4f} \") #| Val AUC: {auc_val:.4f}\")\n"
   ],
   "id": "8b4f90cb9ee53cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "748c433e4ca66bd3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
