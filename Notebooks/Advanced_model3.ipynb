{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81733fcb-0b86-48b4-8d5a-b923feafafa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Repro & core\n",
    "import os, math, json, time, copy, random, gc, h5py\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Paths (edit to your env) ----\n",
    "DATA = {\n",
    "    \"train_h5\": \"/home/karlo/train_chunked.h5\",\n",
    "    \"test_h5\":  \"../DATA/test.h5\",     # optional for end evaluation\n",
    "    \"train_csv\": \"../DATA/train.csv\",  # optional for plots\n",
    "    \"test_csv\":  \"../DATA/test.csv\",   # optional for plots\n",
    "}\n",
    "\n",
    "# ---- Training knobs ----\n",
    "TILE        = 128\n",
    "BATCH       = 64         # raise if memory allows\n",
    "NUM_WORKERS = 2          # HDF5 prefers small (0–2)\n",
    "EPOCHS      = 4          # 3–6 is a good range for this probe\n",
    "MAX_LR      = 3e-4       # we use a flat LR in this probe\n",
    "SAVE_DIR    = \"./checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Toggle AMP for speed; disable if it destabilizes\n",
    "USE_AMP = (device.type == \"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d7d652-7b4c-468e-9ce8-329c2541a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _free_cuda():\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def resize_masks_to(logits, masks):\n",
    "    \"\"\"Make masks match logits spatially via nearest (keeps 0/1).\"\"\"\n",
    "    H, W = logits.shape[-2:]\n",
    "    if masks.dtype != torch.float32:\n",
    "        masks = masks.float()\n",
    "    if masks.dim() == 3:  # (B,H,W) -> (B,1,H,W)\n",
    "        masks = masks.unsqueeze(1)\n",
    "    if masks.shape[-2:] == (H, W):\n",
    "        return (masks > 0.5).float()\n",
    "    out = F.interpolate(masks, size=(H, W), mode=\"nearest\")\n",
    "    return (out > 0.5).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b474fe5f-a88a-4a30-99a9-850b0ee97cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stats_mad(arr):\n",
    "    med = np.median(arr); mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)\n",
    "    return np.float32(med), np.float32(1.0 if not np.isfinite(sigma) or sigma<=0 else sigma)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"Stream tiles from big (H,W) images, robust-normalize per-image, k-sigma clip, pad edges.\"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, crop_for_stats=512):\n",
    "        self.h5_path, self.tile, self.k_sigma, self.crop_for_stats = h5_path, int(tile), float(k_sigma), int(crop_for_stats)\n",
    "        self._h5 = self._x = self._y = None\n",
    "        self._stats_cache = {}\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "        Hb = math.ceil(self.H/self.tile); Wb = math.ceil(self.W/self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "    def _ensure(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self._x, self._y = self._h5[\"images\"], self._h5[\"masks\"]\n",
    "    def _image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        s = min(self.crop_for_stats, self.H, self.W)\n",
    "        h0, w0 = (self.H-s)//2, (self.W-s)//2\n",
    "        crop = self._x[i, h0:h0+s, w0:w0+s].astype(\"float32\")\n",
    "        med, sig = robust_stats_mad(crop); self._stats_cache[i] = (med, sig); return med, sig\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure()\n",
    "        i, r, c = self.indices[idx]; t = self.tile\n",
    "        r0, c0 = r*t, c*t; r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "        x = self._x[i, r0:r1, c0:c1].astype(\"float32\"); y = self._y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        if x.shape != (t,t):\n",
    "            xp = np.zeros((t,t), np.float32); yp = np.zeros((t,t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x; yp[:y.shape[0], :y.shape[1]] = y; x, y = xp, yp\n",
    "        med, sig = self._image_stats(i); x = np.clip((x-med)/sig, -5, 5)\n",
    "        return torch.from_numpy(x[None,...]), torch.from_numpy(y[None,...])\n",
    "\n",
    "class SubsetDS(Dataset):\n",
    "    \"\"\"Select full panels by id while reusing tiling of base dataset.\"\"\"\n",
    "    def __init__(self, base, panel_ids):\n",
    "        self.base, self.panel_ids = base, np.asarray(panel_ids)\n",
    "        t = base.tile; Hb, Wb = math.ceil(base.H/t), math.ceil(base.W/t)\n",
    "        base_map = {(i,r,c):k for k,(i,r,c) in enumerate(base.indices)}\n",
    "        self.map = [base_map[(i,r,c)] for i in self.panel_ids for r in range(Hb) for c in range(Wb)]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, k): return self.base[self.map[k]]\n",
    "\n",
    "def tile_pos_weights(h5_path, tile=128):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        Y = f[\"masks\"]; N,H,W = Y.shape\n",
    "    Hb, Wb = math.ceil(H/tile), math.ceil(W/tile)\n",
    "    w = []\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        Y = f[\"masks\"]\n",
    "        for i in range(N):\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0,c0=r*tile,c*tile; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    w.append(1.0 + 9.0*(Y[i,r0:r1,c0:c1].any()))\n",
    "    return np.asarray(w, np.float64)\n",
    "\n",
    "\n",
    "def panels_with_positives(h5_path, tile=128, max_panels=None):\n",
    "    ids=[]\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        Y=f['masks']; N,H,W=Y.shape\n",
    "        rng = np.random.default_rng(0)\n",
    "        order = rng.permutation(N) if max_panels else np.arange(N)\n",
    "        for i in order:\n",
    "            yi = Y[i]\n",
    "            if yi.any(): ids.append(i)\n",
    "            if max_panels and len(ids)>=max_panels: break\n",
    "    return np.array(sorted(ids))\n",
    "\n",
    "# per-panel median/MAD normalize, clip like stream_panels_direct\n",
    "def norm_medmad_clip(x, clip=5.0, eps=1e-6):\n",
    "    # x: torch.Tensor [B,1,H,W] or [1,H,W]\n",
    "    if x.ndim == 4:\n",
    "        med = x.median(dim=-1, keepdim=True).values.median(dim=-2, keepdim=True).values\n",
    "    else:  # [1,H,W]\n",
    "        med = x.median()\n",
    "        med = med.view(1,1,1)\n",
    "    mad = (x - med).abs().median()\n",
    "    sigma = 1.4826 * mad + eps\n",
    "    z = (x - med) / sigma\n",
    "    return z.clamp_(-clip, clip)\n",
    "\n",
    "class WithTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.base[i]\n",
    "        x = norm_medmad_clip(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e13d69-40c0-4247-8a8f-7fe75818ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self,c,r=8): super().__init__(); self.fc1=nn.Conv2d(c,c//r,1); self.fc2=nn.Conv2d(c//r,c,1)\n",
    "    def forward(self,x): s=F.adaptive_avg_pool2d(x,1); s=F.silu(self.fc1(s),inplace=True); s=torch.sigmoid(self.fc2(s)); return x*s\n",
    "def _norm(c, groups=8): g=min(groups,c) if c%groups==0 else 1; return nn.GroupNorm(g,c)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); p=1\n",
    "    def __init__(self,c_in,c_out,k=3,act=nn.SiLU,se=True):\n",
    "        super().__init__(); p=k//2\n",
    "        self.proj = nn.Identity() if c_in==c_out else nn.Conv2d(c_in,c_out,1)\n",
    "        self.bn1=_norm(c_in); self.c1=nn.Conv2d(c_in,c_out,k,padding=p,bias=False)\n",
    "        self.bn2=_norm(c_out); self.c2=nn.Conv2d(c_out,c_out,k,padding=p,bias=False)\n",
    "        self.act=act(); self.se=SEBlock(c_out) if se else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        h=self.act(self.bn1(x)); h=self.c1(h)\n",
    "        h=self.act(self.bn2(h)); h=self.c2(h)\n",
    "        h=self.se(h); return h + self.proj(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); self.pool=nn.MaxPool2d(2); self.rb=ResBlock(c_in,c_out)\n",
    "    def forward(self,x): return self.rb(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self,c_in,c_skip,c_out): super().__init__(); self.up=nn.ConvTranspose2d(c_in,c_in,2,stride=2); self.rb1=ResBlock(c_in+c_skip,c_out); self.rb2=ResBlock(c_out,c_out)\n",
    "    def forward(self,x,skip):\n",
    "        x=self.up(x)\n",
    "        dh=skip.size(-2)-x.size(-2); dw=skip.size(-1)-x.size(-1)\n",
    "        if dh or dw: x=F.pad(x,(0,max(0,dw),0,max(0,dh)))\n",
    "        x=torch.cat([x,skip],1); x=self.rb1(x); x=self.rb2(x); return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self,c,r=[1,6,12,18]):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList([nn.Sequential(nn.Conv2d(c,c//4,3,padding=d,dilation=d,bias=False), nn.BatchNorm2d(c//4), nn.SiLU(True)) for d in r])\n",
    "        self.project=nn.Conv2d(c,c,1)\n",
    "    def forward(self,x): return self.project(torch.cat([b(x) for b in self.blocks],1))\n",
    "\n",
    "class UNetResSE(nn.Module):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(); w=widths\n",
    "        self.stem=nn.Sequential(nn.Conv2d(in_ch,w[0],3,padding=1,bias=False), nn.BatchNorm2d(w[0]), nn.SiLU(True), ResBlock(w[0],w[0]))\n",
    "        self.d1=Down(w[0],w[1]); self.d2=Down(w[1],w[2]); self.d3=Down(w[2],w[3]); self.d4=Down(w[3],w[4])\n",
    "        self.u1=Up(w[4],w[3],w[3]); self.u2=Up(w[3],w[2],w[2]); self.u3=Up(w[2],w[1],w[1]); self.u4=Up(w[1],w[0],w[0])\n",
    "        self.head=nn.Conv2d(w[0],out_ch,1)\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x) # logits\n",
    "\n",
    "class UNetResSEASPP(UNetResSE):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(in_ch,out_ch,widths); self.aspp=ASPP(widths[-1]); self.d4=Down(widths[3],widths[4])\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3); b=self.aspp(b)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31bec2a-bda0-423c-a7b5-2bd7614cc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftIoU(nn.Module):\n",
    "    def __init__(self, eps=1e-6): \n",
    "        super().__init__(); self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        t = t.view(t.size(0), -1)\n",
    "        inter = (p*t).sum(1)\n",
    "        union = p.sum(1) + t.sum(1) - inter\n",
    "        iou   = (inter + self.eps) / (union + self.eps)\n",
    "        return (1 - iou).mean()\n",
    "\n",
    "class SoftIoUWithBCE(nn.Module):\n",
    "    \"\"\"\n",
    "    total = lambda_bce * BCE(pos_weight) + (1 - lambda_bce) * SoftIoU\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight=8.0, lambda_bce=0.7):\n",
    "        super().__init__()\n",
    "        self.lambda_bce = float(lambda_bce)\n",
    "        self.pos_weight = float(pos_weight)\n",
    "        self.bce  = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.siou = SoftIoU()\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        loss_bce  = self.bce(logits, t) if self.pos_weight<=0 else \\\n",
    "                    F.binary_cross_entropy_with_logits(logits, t, pos_weight=torch.tensor(self.pos_weight, device=logits.device))\n",
    "        loss_siou = self.siou(logits, t)\n",
    "        return self.lambda_bce*loss_bce + (1.0-self.lambda_bce)*loss_siou\n",
    "\n",
    "\n",
    "# (Optional) More aggressive FP control for later HPO:\n",
    "class AsymFocalTversky(nn.Module):\n",
    "    def __init__(self, alpha=0.35, beta=0.65, gamma=1.2, eps=1e-6):\n",
    "        super().__init__(); self.alpha, self.beta, self.gamma, self.eps = alpha,beta,gamma,eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits).clamp(self.eps, 1-self.eps); t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0), -1); t = t.view(t.size(0), -1)\n",
    "        TP = (p*t).sum(1); FP = ((1-t)*p).sum(1); FN = (t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps)/(TP+self.alpha*FP+self.beta*FN+self.eps)\n",
    "        return torch.pow(1.0 - tv, self.gamma).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f524fc-1b1e-4dde-be03-09c0f3464fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pix_metrics(model, loader, thr=0.5, n_batches=6):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    tp = fp = fn = 0.0\n",
    "    pos_means, neg_means = [], []\n",
    "    t0 = time.time()\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb, yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yb_r   = resize_masks_to(logits, yb)\n",
    "        p      = torch.sigmoid(logits)\n",
    "        if (yb_r>0.5).any(): pos_means.append(float(p[yb_r>0.5].mean()))\n",
    "        neg_means.append(float(p[yb_r<=0.5].mean()))\n",
    "        pv, tv = p.reshape(-1), yb_r.reshape(-1)\n",
    "        pred   = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if i>=n_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "    print(f\"[quick_prob_stats] batches={min(n_batches,i)} | pos≈{np.mean(pos_means) if pos_means else float('nan'):.4f} | \"\n",
    "          f\"neg≈{np.mean(neg_means):.4f} | P {P:.3f} R {R:.3f} F1 {f1:.3f} @ thr={thr:.3f} | {time.time()-t0:.1f}s\")\n",
    "    return dict(P=P,R=R,F1=f1,pos_mean=np.mean(pos_means) if pos_means else float('nan'),neg_mean=np.mean(neg_means))\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_under_min(model, loader, max_batches=40, n_bins=256, beta=2.0):\n",
    "    \"\"\"Histogram-based pixel threshold selection (recall-lean if beta>1).\"\"\"\n",
    "    model.eval(); dev = next(model.parameters()).device\n",
    "    hist_pos = torch.zeros(n_bins, device=dev); hist_neg = torch.zeros(n_bins, device=dev)\n",
    "    edges = torch.linspace(0,1,n_bins+1, device=dev)\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev), yb.to(dev)\n",
    "        p = torch.sigmoid(model(xb))\n",
    "        yb_r = resize_masks_to(p, yb)\n",
    "        pv = p.reshape(-1); tv = (yb_r>0.5).reshape(-1)\n",
    "        hist_pos += torch.histc(pv[tv], bins=n_bins, min=0, max=1)\n",
    "        hist_neg += torch.histc(pv[~tv], bins=n_bins, min=0, max=1)\n",
    "        if i>=max_batches: break\n",
    "    cpos = torch.flip(torch.cumsum(torch.flip(hist_pos, dims=[0]), 0), dims=[0])  # >=t\n",
    "    cneg = torch.flip(torch.cumsum(torch.flip(hist_neg, dims=[0]), 0), dims=[0])\n",
    "    TP = cpos; FP = cneg; FN = (hist_pos.sum() - TP).clamp(min=0)\n",
    "    P = TP / (TP + FP + 1e-8); R = TP / (TP + FN + 1e-8)\n",
    "    fbeta = (1+beta*beta)*P*R / (beta*beta*P + R + 1e-8)\n",
    "    idx = int(torch.argmax(fbeta).item())\n",
    "    thr = float((edges[idx] + edges[idx+1])/2)\n",
    "    return thr, (float(P[idx]), float(R[idx]), float(fbeta[idx])), dict(pos_rate=float((TP[idx]+FP[idx])/(hist_pos.sum()+hist_neg.sum()+1e-8)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_with_floor(model, loader, max_batches=40, n_bins=256, beta=1.0, min_pos_rate=0.05, max_pos_rate=0.10):\n",
    "    thr, (P,R,F), aux = pick_thr_under_min(model, loader, max_batches=max_batches, n_bins=n_bins, beta=beta)\n",
    "    # simple clamp pass using percentile of preds to hit pos_rate band\n",
    "    # (if your earlier “floor” function is available, feel free to swap it in)\n",
    "    return thr, (P,R,F), aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29401cff-1c85-4548-be15-1c2d1c997a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_head_bias_to_prior(model, p0=0.70):\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "            model.head.bias.data.fill_(b)\n",
    "\n",
    "def set_requires_grad(mod, flag: bool):\n",
    "    for p in mod.parameters(): p.requires_grad = flag\n",
    "\n",
    "def freeze_all(model): set_requires_grad(model, False)\n",
    "\n",
    "def unfreeze_head_only(model):\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        set_requires_grad(model.head, True)\n",
    "    else:\n",
    "        raise AttributeError(\"Model has no attribute 'head'.\")\n",
    "\n",
    "def unfreeze_head_and_tail(model):\n",
    "    \"\"\"\n",
    "    Unfreeze head + late upsample blocks + ASPP (assumes UNetResSEASPP)\n",
    "    Adjust attribute names if your class differs.\n",
    "    \"\"\"\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"): set_requires_grad(model.head, True)\n",
    "    for path in [\"u3\", \"u4\", \"aspp\"]:\n",
    "        if hasattr(model, path): set_requires_grad(getattr(model, path), True)\n",
    "\n",
    "def fit_quick_warmup(model, loader, epochs=2, max_batches=800, lr=2e-4, metric_thr=0.20, pos_weight=30.0):\n",
    "    dev = next(model.parameters()).device\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    for ep in range(1, epochs+1):\n",
    "        seen=tp=fp=fn=0.0; loss_sum=0.0\n",
    "        t0=time.time()\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb)\n",
    "            yb_r   = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits); pv, tv = p.view(-1), yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[WARMUP] ep{ep} loss {loss_sum/seen:.4f} | F1 {f1:.3f} P {P:.3f} R {R:.3f}\")\n",
    "\n",
    "def fit_head_only(model, loader, epochs=2, max_batches=600, lr=3e-5, metric_thr=0.15, pos_weight=5.0):\n",
    "    dev = next(model.parameters()).device\n",
    "    unfreeze_head_only(model)\n",
    "    head_params = [p for p in model.head.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.Adam(head_params, lr=lr, weight_decay=0.0)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    for ep in range(1, epochs+1):\n",
    "        seen=tp=fp=fn=0.0; loss_sum=0.0\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits); pv, tv = p.view(-1), yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[HEAD] ep{ep} loss {loss_sum/seen:.4f} | F1 {f1:.3f} P {P:.3f} R {R:.3f}\")\n",
    "\n",
    "def brief_tail_probe(model, loader, criterion, max_batches=400, lr=3e-4, wd=1e-4):\n",
    "    dev = next(model.parameters()).device\n",
    "    unfreeze_head_and_tail(model)\n",
    "    opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)\n",
    "    loss_sum=0.0; seen=0; t0=time.time()\n",
    "    for b,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev), yb.to(dev)\n",
    "        logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "        loss = criterion(logits, yb_r)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "        if b>=max_batches: break\n",
    "    print(f\"[tail-probe] loss≈{loss_sum/max(seen,1):.4f}\")\n",
    "\n",
    "def one_epoch(model, loader, criterion, opt):\n",
    "    t0=time.time()\n",
    "    model.train(); loss_sum=0.0; seen=0\n",
    "    for b, (xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "        loss = criterion(logits, yb_r)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "        print(f\"\\r[TRAIN] batch {b}/{len(loader)} | loss={loss_sum/seen:.4f} | {time.time()-t0:.1f}s\", end='')\n",
    "    print (f\"\\r\")\n",
    "    return loss_sum/max(seen,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_pick_thr(model, vloader, min_pr=0.05, max_pr=0.10):\n",
    "    thr, (P,R,F), aux = pick_thr_with_floor(model, vloader, max_batches=60, n_bins=256, beta=1.0,\n",
    "                                            min_pos_rate=min_pr, max_pos_rate=max_pr)\n",
    "    return float(thr), (P,R,F), aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6365882f-9c5e-4f7f-bd7e-cf91b0fabf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA[\"train_h5\"], \"r\") as f:\n",
    "    N = f[\"images\"].shape[0]\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "split = int(0.9*N)\n",
    "idx_tr, idx_va = np.sort(idx[:split]), np.sort(idx[split:])\n",
    "\n",
    "ds_full = H5TiledDataset(DATA[\"train_h5\"], tile=TILE, k_sigma=5.0)\n",
    "\n",
    "pos_panels = panels_with_positives(DATA[\"train_h5\"], max_panels=2000)\n",
    "sub_tr = np.random.default_rng(SEED).choice(np.intersect1d(idx_tr, pos_panels),\n",
    "                                            size=min(200, len(pos_panels)), replace=False)\n",
    "sub_va = np.random.default_rng(SEED+1).choice(np.intersect1d(idx_va, pos_panels),\n",
    "                                              size=min(80, len(pos_panels)), replace=False)\n",
    "\n",
    "train_loader_small = DataLoader(SubsetDS(ds_full, np.sort(sub_tr)), batch_size=BATCH, shuffle=True,\n",
    "                                num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader_small   = DataLoader(SubsetDS(ds_full, np.sort(sub_va)), batch_size=BATCH, shuffle=False,\n",
    "                                num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "# Full loaders (for end-to-end epochs)\n",
    "train_loader = DataLoader(SubsetDS(ds_full, idx_tr), batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader   = DataLoader(SubsetDS(ds_full, idx_va), batch_size=BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b09bcd2-ca77-451a-9469-44a7e4d8e949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup…\n",
      "[WARMUP] ep1 loss 0.4237 | F1 0.018 P 0.009 R 0.448\n",
      "[thr0] ≈ 0.200\n",
      "[HEAD] ep1 loss 0.1285 | F1 0.026 P 0.014 R 0.371\n",
      "[HEAD] ep2 loss 0.1043 | F1 0.034 P 0.018 R 0.239\n",
      "[quick_prob_stats] batches=6 | pos≈0.1231 | neg≈0.0533 | P 0.020 R 0.192 F1 0.036 @ thr=0.200 | 0.7s\n",
      "[tail-probe] loss≈0.3797\n",
      "[quick_prob_stats] batches=6 | pos≈0.0550 | neg≈0.0270 | P 0.056 R 0.065 F1 0.060 @ thr=0.200 | 0.7s\n"
     ]
    }
   ],
   "source": [
    "probe = UNetResSEASPP(in_ch=1, out_ch=1).to(device)\n",
    "init_head_bias_to_prior(probe, p0=0.80)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Warmup…\")\n",
    "fit_quick_warmup(probe, train_loader_small, epochs=1, max_batches=400, lr=2e-4, metric_thr=0.20, pos_weight=40.0)\n",
    "thr0, *_ = pick_thr_under_min(probe, val_loader_small, max_batches=40, n_bins=256, beta=2.0)\n",
    "thr0 = float(np.clip(thr0, 0.10, 0.20))\n",
    "print(f\"[thr0] ≈ {thr0:.3f}\")\n",
    "\n",
    "fit_head_only(probe, train_loader_small, epochs=2, max_batches=600, lr=3e-5, metric_thr=thr0, pos_weight=5.0)\n",
    "_ = pix_metrics(probe, train_loader_small, thr=thr0, n_batches=6)\n",
    "\n",
    "# Tail probe with SoftIoU+BCE (default training loss)\n",
    "criterion = SoftIoUWithBCE(pos_weight=8.0, lambda_bce=0.7).to(device)\n",
    "brief_tail_probe(probe, train_loader_small, criterion, max_batches=400, lr=3e-4, wd=1e-4)\n",
    "_ = pix_metrics(probe, train_loader_small, thr=thr0, n_batches=6)\n",
    "\n",
    "# Snapshot probe state for full training\n",
    "best_state = copy.deepcopy(probe.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87cde41e-c938-4e97-84e5-9893f5d5dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] batch 11520/11520 | loss=0.3742 | 1367.2s\n",
      "[quick_prob_stats] batches=6 | pos≈0.2307 | neg≈0.0246 | P 0.272 R 0.336 F1 0.301 @ thr=0.200 | 0.7s\n",
      "[EP01] loss 0.3742 | train P 0.272 R 0.336 F 0.301\n",
      "[thr@ep1] thr=0.436 | val P 0.457 R 0.183 F 0.262\n",
      "[VAL ep1] improved: F 0.262 (thr=0.436)\n",
      "[TRAIN] batch 11520/11520 | loss=0.3685 | 1350.1s\n",
      "[quick_prob_stats] batches=6 | pos≈0.0589 | neg≈0.0200 | P 0.000 R 0.000 F1 0.000 @ thr=0.436 | 0.7s\n",
      "[EP02] loss 0.3685 | train P 0.000 R 0.000 F 0.000\n",
      "[thr@ep2] thr=0.373 | val P 0.440 R 0.187 F 0.262\n",
      "[VAL ep2] improved: F 0.262 (thr=0.373)\n",
      "[TRAIN] batch 11520/11520 | loss=0.3668 | 1342.1s\n",
      "[quick_prob_stats] batches=6 | pos≈0.1341 | neg≈0.0192 | P 0.428 R 0.204 F1 0.276 @ thr=0.373 | 0.7s\n",
      "[EP03] loss 0.3668 | train P 0.428 R 0.204 F 0.276\n",
      "[thr@ep3] thr=0.350 | val P 0.303 R 0.187 F 0.231\n",
      "[TRAIN] batch 11520/11520 | loss=0.3640 | 1355.8s\n",
      "[quick_prob_stats] batches=6 | pos≈0.1519 | neg≈0.0184 | P 0.111 R 0.134 F1 0.122 @ thr=0.350 | 0.7s\n",
      "[EP04] loss 0.3640 | train P 0.111 R 0.134 F 0.122\n",
      "[thr@ep4] thr=0.717 | val P 0.461 R 0.228 F 0.305\n",
      "[VAL ep4] improved: F 0.305 (thr=0.717)\n",
      "Best summary: {'F': 0.305, 'thr': 0.717, 'ep': 4}\n"
     ]
    }
   ],
   "source": [
    "# Reset model to snapshot before full train\n",
    "probe.load_state_dict(best_state, strict=True)\n",
    "unfreeze_head_and_tail(probe)\n",
    "\n",
    "criterion = SoftIoUWithBCE(pos_weight=8.0, lambda_bce=0.7).to(device)\n",
    "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, probe.parameters()), lr=MAX_LR, weight_decay=1e-4)\n",
    "\n",
    "best = {\"F\": -1.0, \"thr\": None, \"ep\": 0, \"state\": None}\n",
    "metric_thr = thr0\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    # small LR bump on stagnation\n",
    "    if ep == 3: \n",
    "        for g in opt.param_groups: g[\"lr\"] = 4e-4\n",
    "\n",
    "    ep_loss = one_epoch(probe, train_loader, criterion, opt)\n",
    "    tr = pix_metrics(probe, train_loader_small, thr=metric_thr, n_batches=6)\n",
    "    print(f\"[EP{ep:02d}] loss {ep_loss:.4f} | train P {tr['P']:.3f} R {tr['R']:.3f} F {tr['F1']:.3f}\")\n",
    "\n",
    "    metric_thr, (VP,VR,VF), _ = val_pick_thr(probe, val_loader_small, min_pr=0.05, max_pr=0.10)\n",
    "    print(f\"[thr@ep{ep}] thr={metric_thr:.3f} | val P {VP:.3f} R {VR:.3f} F {VF:.3f}\")\n",
    "\n",
    "    if VF > best[\"F\"]:\n",
    "        best.update(F=VF, thr=metric_thr, ep=ep, state=copy.deepcopy(probe.state_dict()))\n",
    "        print(f\"[VAL ep{ep}] improved: F {VF:.3f} (thr={metric_thr:.3f})\")\n",
    "\n",
    "print(\"Best summary:\", {k: (round(v,3) if isinstance(v,float) else v) for k,v in best.items() if k!='state'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e185cf5-cab7-44eb-a89f-928a1e1d1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"state\": probe.state_dict(), \"thr\": 0.717}, \"baseline_ep4.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05119bb-8bb7-4582-aa8f-81de76c712d3",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f120bc-36d5-402b-b91c-c5bae8838023",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {\n",
    "    \"train_h5\": \"/home/karlo/train_chunked.h5\",\n",
    "    \"test_h5\":  \"../DATA/test.h5\",     # optional for end evaluation\n",
    "    \"train_csv\": \"../DATA/train.csv\",  # optional for plots\n",
    "    \"test_csv\":  \"../DATA/test.csv\",   # optional for plots\n",
    "}\n",
    "TILE        = 128\n",
    "BATCH       = 64         # raise if memory allows\n",
    "NUM_WORKERS = 10          # HDF5 prefers small (0–2)\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3065e330-b33f-47fa-82dd-ffbfab718c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, time, copy, random, gc, h5py\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490ea6c3-811a-41d0-8008-bd3a63f9b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stats_mad(arr):\n",
    "    med = np.median(arr); mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)\n",
    "    return np.float32(med), np.float32(1.0 if not np.isfinite(sigma) or sigma<=0 else sigma)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"Stream tiles from big (H,W) images, robust-normalize per-image, k-sigma clip, pad edges.\"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, crop_for_stats=512):\n",
    "        self.h5_path, self.tile, self.k_sigma, self.crop_for_stats = h5_path, int(tile), float(k_sigma), int(crop_for_stats)\n",
    "        self._h5 = self._x = self._y = None\n",
    "        self._stats_cache = {}\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "        Hb = math.ceil(self.H/self.tile); Wb = math.ceil(self.W/self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "    def _ensure(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self._x, self._y = self._h5[\"images\"], self._h5[\"masks\"]\n",
    "    def _image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        s = min(self.crop_for_stats, self.H, self.W)\n",
    "        h0, w0 = (self.H-s)//2, (self.W-s)//2\n",
    "        crop = self._x[i, h0:h0+s, w0:w0+s].astype(\"float32\")\n",
    "        med, sig = robust_stats_mad(crop); self._stats_cache[i] = (med, sig); return med, sig\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure()\n",
    "        i, r, c = self.indices[idx]; t = self.tile\n",
    "        r0, c0 = r*t, c*t; r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "        x = self._x[i, r0:r1, c0:c1].astype(\"float32\"); y = self._y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        if x.shape != (t,t):\n",
    "            xp = np.zeros((t,t), np.float32); yp = np.zeros((t,t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x; yp[:y.shape[0], :y.shape[1]] = y; x, y = xp, yp\n",
    "        med, sig = self._image_stats(i); x = np.clip((x-med)/sig, -5, 5)\n",
    "        return torch.from_numpy(x[None,...]), torch.from_numpy(y[None,...])\n",
    "\n",
    "class SubsetDS(Dataset):\n",
    "    \"\"\"Select full panels by id while reusing tiling of base dataset.\"\"\"\n",
    "    def __init__(self, base, panel_ids):\n",
    "        self.base, self.panel_ids = base, np.asarray(panel_ids)\n",
    "        t = base.tile; Hb, Wb = math.ceil(base.H/t), math.ceil(base.W/t)\n",
    "        base_map = {(i,r,c):k for k,(i,r,c) in enumerate(base.indices)}\n",
    "        self.map = [base_map[(i,r,c)] for i in self.panel_ids for r in range(Hb) for c in range(Wb)]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, k): return self.base[self.map[k]]\n",
    "\n",
    "def tile_pos_weights(h5_path, tile=128):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        Y = f[\"masks\"]; N,H,W = Y.shape\n",
    "    Hb, Wb = math.ceil(H/tile), math.ceil(W/tile)\n",
    "    w = []\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        Y = f[\"masks\"]\n",
    "        for i in range(N):\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0,c0=r*tile,c*tile; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    w.append(1.0 + 9.0*(Y[i,r0:r1,c0:c1].any()))\n",
    "    return np.asarray(w, np.float64)\n",
    "\n",
    "\n",
    "def panels_with_positives(h5_path, tile=128, max_panels=None):\n",
    "    ids=[]\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        Y=f['masks']; N,H,W=Y.shape\n",
    "        rng = np.random.default_rng(0)\n",
    "        order = rng.permutation(N) if max_panels else np.arange(N)\n",
    "        for i in order:\n",
    "            yi = Y[i]\n",
    "            if yi.any(): ids.append(i)\n",
    "            if max_panels and len(ids)>=max_panels: break\n",
    "    return np.array(sorted(ids))\n",
    "\n",
    "# per-panel median/MAD normalize, clip like stream_panels_direct\n",
    "def norm_medmad_clip(x, clip=5.0, eps=1e-6):\n",
    "    # x: torch.Tensor [B,1,H,W] or [1,H,W]\n",
    "    if x.ndim == 4:\n",
    "        med = x.median(dim=-1, keepdim=True).values.median(dim=-2, keepdim=True).values\n",
    "    else:  # [1,H,W]\n",
    "        med = x.median()\n",
    "        med = med.view(1,1,1)\n",
    "    mad = (x - med).abs().median()\n",
    "    sigma = 1.4826 * mad + eps\n",
    "    z = (x - med) / sigma\n",
    "    return z.clamp_(-clip, clip)\n",
    "\n",
    "class WithTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.base[i]\n",
    "        x = norm_medmad_clip(x)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3c08f6b-6603-4664-9d7e-8fb468f80918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self,c,r=8): super().__init__(); self.fc1=nn.Conv2d(c,c//r,1); self.fc2=nn.Conv2d(c//r,c,1)\n",
    "    def forward(self,x): s=F.adaptive_avg_pool2d(x,1); s=F.silu(self.fc1(s),inplace=True); s=torch.sigmoid(self.fc2(s)); return x*s\n",
    "def _norm(c, groups=8): g=min(groups,c) if c%groups==0 else 1; return nn.GroupNorm(g,c)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); p=1\n",
    "    def __init__(self,c_in,c_out,k=3,act=nn.SiLU,se=True):\n",
    "        super().__init__(); p=k//2\n",
    "        self.proj = nn.Identity() if c_in==c_out else nn.Conv2d(c_in,c_out,1)\n",
    "        self.bn1=_norm(c_in); self.c1=nn.Conv2d(c_in,c_out,k,padding=p,bias=False)\n",
    "        self.bn2=_norm(c_out); self.c2=nn.Conv2d(c_out,c_out,k,padding=p,bias=False)\n",
    "        self.act=act(); self.se=SEBlock(c_out) if se else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        h=self.act(self.bn1(x)); h=self.c1(h)\n",
    "        h=self.act(self.bn2(h)); h=self.c2(h)\n",
    "        h=self.se(h); return h + self.proj(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); self.pool=nn.MaxPool2d(2); self.rb=ResBlock(c_in,c_out)\n",
    "    def forward(self,x): return self.rb(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self,c_in,c_skip,c_out): super().__init__(); self.up=nn.ConvTranspose2d(c_in,c_in,2,stride=2); self.rb1=ResBlock(c_in+c_skip,c_out); self.rb2=ResBlock(c_out,c_out)\n",
    "    def forward(self,x,skip):\n",
    "        x=self.up(x)\n",
    "        dh=skip.size(-2)-x.size(-2); dw=skip.size(-1)-x.size(-1)\n",
    "        if dh or dw: x=F.pad(x,(0,max(0,dw),0,max(0,dh)))\n",
    "        x=torch.cat([x,skip],1); x=self.rb1(x); x=self.rb2(x); return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self,c,r=[1,6,12,18]):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList([nn.Sequential(nn.Conv2d(c,c//4,3,padding=d,dilation=d,bias=False), nn.BatchNorm2d(c//4), nn.SiLU(True)) for d in r])\n",
    "        self.project=nn.Conv2d(c,c,1)\n",
    "    def forward(self,x): return self.project(torch.cat([b(x) for b in self.blocks],1))\n",
    "\n",
    "class UNetResSE(nn.Module):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(); w=widths\n",
    "        self.stem=nn.Sequential(nn.Conv2d(in_ch,w[0],3,padding=1,bias=False), nn.BatchNorm2d(w[0]), nn.SiLU(True), ResBlock(w[0],w[0]))\n",
    "        self.d1=Down(w[0],w[1]); self.d2=Down(w[1],w[2]); self.d3=Down(w[2],w[3]); self.d4=Down(w[3],w[4])\n",
    "        self.u1=Up(w[4],w[3],w[3]); self.u2=Up(w[3],w[2],w[2]); self.u3=Up(w[2],w[1],w[1]); self.u4=Up(w[1],w[0],w[0])\n",
    "        self.head=nn.Conv2d(w[0],out_ch,1)\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x) # logits\n",
    "\n",
    "class UNetResSEASPP(UNetResSE):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(in_ch,out_ch,widths); self.aspp=ASPP(widths[-1]); self.d4=Down(widths[3],widths[4])\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3); b=self.aspp(b)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e7b8664-f820-4187-9da5-d4bf560485b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_masks_to(logits, masks):\n",
    "    \"\"\"Make masks match logits spatially via nearest (keeps 0/1).\"\"\"\n",
    "    H, W = logits.shape[-2:]\n",
    "    if masks.dtype != torch.float32:\n",
    "        masks = masks.float()\n",
    "    if masks.dim() == 3:  # (B,H,W) -> (B,1,H,W)\n",
    "        masks = masks.unsqueeze(1)\n",
    "    if masks.shape[-2:] == (H, W):\n",
    "        return (masks > 0.5).float()\n",
    "    out = F.interpolate(masks, size=(H, W), mode=\"nearest\")\n",
    "    return (out > 0.5).float()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _pix_eval(m, loader, thr=0.2, max_batches=12):\n",
    "    m.eval(); dev = next(m.parameters()).device\n",
    "    tp=fp=fn=0.0; posm=[]; negm=[]\n",
    "    t0=time.time()\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev), yb.to(dev)\n",
    "        logits = m(xb)\n",
    "        yb_r   = resize_masks_to(logits, yb)\n",
    "        p      = torch.sigmoid(logits)\n",
    "        if (yb_r>0.5).any(): posm.append(float(p[yb_r>0.5].mean()))\n",
    "        negm.append(float(p[yb_r<=0.5].mean()))\n",
    "        pv,tv = p.view(-1), yb_r.view(-1)\n",
    "        pred = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if i>=max_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); F = 2*P*R/max(P+R,1e-8)\n",
    "    return {\"P\":P,\"R\":R,\"F\":F,\"pos_mean\":float(sum(posm)/max(len(posm),1)), \"neg_mean\":float(sum(negm)/len(negm))}\n",
    "\n",
    "# ============ Losses ============\n",
    "class SoftIoULoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6): super().__init__(); self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits); t = targets.clamp(0,1)\n",
    "        inter = (p*t).sum(dim=(1,2,3))\n",
    "        union = (p + t - p*t).sum(dim=(1,2,3)) + self.eps\n",
    "        iou = inter/union\n",
    "        return (1 - iou).mean()\n",
    "\n",
    "class AFTL(nn.Module):\n",
    "    def __init__(self, alpha=0.45, beta=0.55, gamma=1.3, eps=1e-6):\n",
    "        super().__init__(); self.alpha, self.beta, self.gamma, self.eps = alpha,beta,gamma,eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits).clamp(self.eps, 1-self.eps)\n",
    "        t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0), -1); t = t.view(t.size(0), -1)\n",
    "        TP = (p*t).sum(1); FP = ((1-t)*p).sum(1); FN = (t*(1-p)).sum(1)\n",
    "        tv = (TP+self.eps)/(TP + self.alpha*FP + self.beta*FN + self.eps)\n",
    "        return torch.pow(1.0 - tv, self.gamma).mean()\n",
    "\n",
    "class BCEIoUEdge(nn.Module):\n",
    "    \"\"\"\n",
    "    λ_bce * BCE(pos_weight) + (1-λ_bce) * SoftIoU [+ λ_edge * Sobel L1]\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_bce=0.6, pos_weight=8.0, lambda_edge=0.0):\n",
    "        super().__init__()\n",
    "        self.lambda_bce = float(lambda_bce)\n",
    "        self.lambda_edge = float(lambda_edge)\n",
    "        self.iou = SoftIoULoss()\n",
    "        self.posw = float(pos_weight)\n",
    "        # Sobel\n",
    "        kx = torch.tensor([[[-1,0,1],[-2,0,2],[-1,0,1]]], dtype=torch.float32).unsqueeze(0)\n",
    "        ky = torch.tensor([[[-1,-2,-1],[0,0,0],[1,2,1]]], dtype=torch.float32).unsqueeze(0)\n",
    "        self.register_buffer(\"kx\", kx); self.register_buffer(\"ky\", ky)\n",
    "    def _edge(self, x):\n",
    "        gx = F.conv2d(x, self.kx, padding=1)\n",
    "        gy = F.conv2d(x, self.ky, padding=1)\n",
    "        return torch.sqrt(gx*gx + gy*gy + 1e-12)\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        posw = torch.tensor(self.posw, device=logits.device)\n",
    "        bce  = F.binary_cross_entropy_with_logits(logits, t, pos_weight=posw)\n",
    "        siou = self.iou(logits, t)\n",
    "        loss = self.lambda_bce*bce + (1.0-self.lambda_bce)*siou\n",
    "        if self.lambda_edge>0:\n",
    "            p = torch.sigmoid(logits)\n",
    "            loss = loss + self.lambda_edge * F.l1_loss(self._edge(p), self._edge(t))\n",
    "        return loss\n",
    "\n",
    "def blended_loss(core, aftl, w, logits, targets):\n",
    "    loss = w[\"w_core\"] * core(logits, targets)\n",
    "    if aftl is not None and w.get(\"w_aftl\", 0) > 0:\n",
    "        loss = loss + w[\"w_aftl\"] * aftl(logits, targets)\n",
    "    return loss\n",
    "\n",
    "def _make_loss_for_epoch(ep: int):\n",
    "    # Early recall → mid mixed → late precision (edge)\n",
    "    if ep <= 10:\n",
    "        core = BCEIoUEdge(lambda_bce=0.6, pos_weight=8.0, lambda_edge=0.00).to(device); aftl=None\n",
    "        return core, aftl, {\"w_core\":1.0, \"w_aftl\":0.0}\n",
    "    elif ep <= 25:\n",
    "        core = BCEIoUEdge(lambda_bce=0.6, pos_weight=8.0, lambda_edge=0.00).to(device)\n",
    "        aftl = AFTL(alpha=0.45, beta=0.55, gamma=1.3).to(device)\n",
    "        return core, aftl, {\"w_core\":0.85, \"w_aftl\":0.15}\n",
    "    else:\n",
    "        core = BCEIoUEdge(lambda_bce=0.8, pos_weight=8.0, lambda_edge=0.03).to(device)\n",
    "        aftl = AFTL(alpha=0.45, beta=0.55, gamma=1.3).to(device)\n",
    "        return core, aftl, {\"w_core\":0.85, \"w_aftl\":0.15}\n",
    "\n",
    "def _make_opt_sched(ep: int, base_lrs, weight_decay):\n",
    "    if ep <= 12:  base_lr = base_lrs[0]\n",
    "    elif ep <= 25: base_lr = base_lrs[1]\n",
    "    else:          base_lr = base_lrs[2]\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=base_lr, weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=6, T_mult=2, eta_min=base_lr/10)\n",
    "    return opt, sched\n",
    "\n",
    "def set_requires_grad(mod, flag: bool):\n",
    "    for p in mod.parameters(): p.requires_grad = flag\n",
    "        \n",
    "def freeze_all(model): set_requires_grad(model, False)\n",
    "    \n",
    "def _unfreeze_if_exists(path: str):\n",
    "    mod = model\n",
    "    for name in path.split('.'):\n",
    "        if not hasattr(mod, name): return False\n",
    "        mod = getattr(mod, name)\n",
    "    for p in mod.parameters(): p.requires_grad = True\n",
    "    return True\n",
    "\n",
    "def _apply_phase(ep: int):\n",
    "    \"\"\"\n",
    "    Phases:\n",
    "      1–3:   head only\n",
    "      4–12:  head + tail (u4, u3, aspp)\n",
    "      13–25: head + u2,u3,u4,aspp\n",
    "      26+:   full\n",
    "    \"\"\"\n",
    "    freeze_all(model)\n",
    "    groups = []\n",
    "    if ep <= 3:\n",
    "        if hasattr(model, \"head\"):\n",
    "            for p in model.head.parameters(): p.requires_grad = True\n",
    "        groups = [\"head\"]\n",
    "    elif ep <= 12:\n",
    "        if hasattr(model, \"head\"):\n",
    "            for p in model.head.parameters(): p.requires_grad = True\n",
    "        for g in [\"u4\",\"u3\",\"aspp\"]:\n",
    "            if _unfreeze_if_exists(g): groups.append(g)\n",
    "    elif ep <= 25:\n",
    "        if hasattr(model, \"head\"):\n",
    "            for p in model.head.parameters(): p.requires_grad = True\n",
    "        for g in [\"u4\",\"u3\",\"u2\",\"aspp\"]:\n",
    "            if _unfreeze_if_exists(g): groups.append(g)\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "        groups = [\"<FULL>\"]\n",
    "    ntrain = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"[phase] ep={ep} | trainable params={ntrain:,} | groups={groups}\")\n",
    "    return groups\n",
    "\n",
    "def _maybe_init_head_bias_to_prior(model, p0=0.70):\n",
    "    \"\"\"If model.head.bias exists, set it to logit(p0) to start slightly positive-prior aware.\"\"\"\n",
    "    if p0 is None: return\n",
    "    if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "        with torch.no_grad():\n",
    "            b = math.log(p0/(1-p0))\n",
    "            model.head.bias.data.fill_(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57076487-922f-4e97-a5c8-87d5938ccc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pix_metrics(model, loader, thr=0.5, n_batches=6):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    tp = fp = fn = 0.0\n",
    "    pos_means, neg_means = [], []\n",
    "    t0 = time.time()\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb, yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yb_r   = resize_masks_to(logits, yb)\n",
    "        p      = torch.sigmoid(logits)\n",
    "        if (yb_r>0.5).any(): pos_means.append(float(p[yb_r>0.5].mean()))\n",
    "        neg_means.append(float(p[yb_r<=0.5].mean()))\n",
    "        pv, tv = p.reshape(-1), yb_r.reshape(-1)\n",
    "        pred   = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if i>=n_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "    print(f\"[quick_prob_stats] batches={min(n_batches,i)} | pos≈{np.mean(pos_means) if pos_means else float('nan'):.4f} | \"\n",
    "          f\"neg≈{np.mean(neg_means):.4f} | P {P:.3f} R {R:.3f} F1 {f1:.3f} @ thr={thr:.3f} | {time.time()-t0:.1f}s\")\n",
    "    return dict(P=P,R=R,F1=f1,pos_mean=np.mean(pos_means) if pos_means else float('nan'),neg_mean=np.mean(neg_means))\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_under_min(model, loader, max_batches=40, n_bins=256, beta=2.0):\n",
    "    \"\"\"Histogram-based pixel threshold selection (recall-lean if beta>1).\"\"\"\n",
    "    model.eval(); dev = next(model.parameters()).device\n",
    "    hist_pos = torch.zeros(n_bins, device=dev); hist_neg = torch.zeros(n_bins, device=dev)\n",
    "    edges = torch.linspace(0,1,n_bins+1, device=dev)\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev), yb.to(dev)\n",
    "        p = torch.sigmoid(model(xb))\n",
    "        yb_r = resize_masks_to(p, yb)\n",
    "        pv = p.reshape(-1); tv = (yb_r>0.5).reshape(-1)\n",
    "        hist_pos += torch.histc(pv[tv], bins=n_bins, min=0, max=1)\n",
    "        hist_neg += torch.histc(pv[~tv], bins=n_bins, min=0, max=1)\n",
    "        if i>=max_batches: break\n",
    "    cpos = torch.flip(torch.cumsum(torch.flip(hist_pos, dims=[0]), 0), dims=[0])  # >=t\n",
    "    cneg = torch.flip(torch.cumsum(torch.flip(hist_neg, dims=[0]), 0), dims=[0])\n",
    "    TP = cpos; FP = cneg; FN = (hist_pos.sum() - TP).clamp(min=0)\n",
    "    P = TP / (TP + FP + 1e-8); R = TP / (TP + FN + 1e-8)\n",
    "    fbeta = (1+beta*beta)*P*R / (beta*beta*P + R + 1e-8)\n",
    "    idx = int(torch.argmax(fbeta).item())\n",
    "    thr = float((edges[idx] + edges[idx+1])/2)\n",
    "    return thr, (float(P[idx]), float(R[idx]), float(fbeta[idx])), dict(pos_rate=float((TP[idx]+FP[idx])/(hist_pos.sum()+hist_neg.sum()+1e-8)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_with_floor(model, loader, max_batches=40, n_bins=256, beta=1.0, min_pos_rate=0.05, max_pos_rate=0.10):\n",
    "    thr, (P,R,F), aux = pick_thr_under_min(model, loader, max_batches=max_batches, n_bins=n_bins, beta=beta)\n",
    "    # simple clamp pass using percentile of preds to hit pos_rate band\n",
    "    # (if your earlier “floor” function is available, feel free to swap it in)\n",
    "    return thr, (P,R,F), aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66795a3f-5fb2-4462-904e-17c553715796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# End-to-end training pipeline (warmup → probe → train → pick thr)\n",
    "# ===========================================================\n",
    "def train_full_probe(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    resize_masks_to,                 # callable(logits, y) -> y_resized\n",
    "    pick_thr_with_floor,             # callable(model, loader, max_batches, n_bins, beta, min_pos_rate, max_pos_rate)\n",
    "    *,\n",
    "    device=None,\n",
    "    seed: int = 1337,\n",
    "    init_head_prior: float = 0.70,   # initialize head bias to prior P(Y=1)\n",
    "    # Warmup (BCE only, whole net) — quick grads + stable head\n",
    "    warmup_epochs: int = 1,\n",
    "    warmup_batches: int = 800,\n",
    "    warmup_lr: float = 2e-4,\n",
    "    warmup_pos_weight: float = 40.0,\n",
    "    # Head-only calibration (BCE)\n",
    "    head_epochs: int = 2,\n",
    "    head_batches: int = 2000,\n",
    "    head_lr: float = 3e-5,\n",
    "    head_pos_weight: float = 5.0,\n",
    "    # Tail probe (optional gentle shape)\n",
    "    tail_epochs: int = 2,\n",
    "    tail_batches: int = 2500,\n",
    "    tail_lr: float = 1.5e-4,\n",
    "    tail_pos_weight: float = 2.0,\n",
    "    # Long training (cosine restarts + curriculum)\n",
    "    max_epochs: int = 60,\n",
    "    val_every: int = 3,\n",
    "    base_lrs=(3e-4, 2e-4, 1e-4),     # (early, mid, late)\n",
    "    weight_decay: float = 1e-4,\n",
    "    # Threshold selection\n",
    "    thr_beta: float = 1.0,           # Fβ for picking thresholds during long training\n",
    "    thr_pos_rate_early=(0.03, 0.10),\n",
    "    thr_pos_rate_late=(0.08, 0.12),\n",
    "    # Checkpointing\n",
    "    save_best_to: str | None = \"ckpt_best.pt\",\n",
    "    # Print & eval settings\n",
    "    quick_eval_train_batches: int = 6,\n",
    "    quick_eval_val_batches: int = 12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        model (nn.Module): trained model (best weights loaded if save_best_to is not None)\n",
    "        metric_thr (float): final recommended threshold\n",
    "        summary (dict): basic metrics {'best_F','best_P','best_R','best_ep','final_thr'}\n",
    "    Notes:\n",
    "        - Assumes model has attributes like 'head', optionally 'u2','u3','u4','aspp'. Uses best-effort unfreezing.\n",
    "        - Requires your existing resize_masks_to and pick_thr_with_floor utilities.\n",
    "    \"\"\"\n",
    "    # --------------- Setup ---------------\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    model.to(device)\n",
    "    _maybe_init_head_bias_to_prior(model, init_head_prior)\n",
    "    # --------------- Warmup (BCE-only) ---------------\n",
    "    print(\"Warmup…\")\n",
    "    freeze_all(model)\n",
    "    for p in model.parameters(): p.requires_grad = True  # warm everything briefly\n",
    "    posw = torch.tensor(warmup_pos_weight, device=device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=warmup_lr, weight_decay=0.0)\n",
    "    for ep in range(1, warmup_epochs+1):\n",
    "        model.train(); seen=0; loss_sum=0.0; tp=fp=fn=0.0\n",
    "        for b,(xb,yb) in enumerate(train_loader, 1):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, yb_r, pos_weight=posw)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=warmup_batches: break\n",
    "        stats = _pix_eval(model, train_loader, thr=0.2, max_batches=quick_eval_train_batches)\n",
    "        print(f\"[WARMUP] ep{ep} loss {loss_sum/seen:.4f} | F1 {stats['F']:.3f} P {stats['P']:.3f} R {stats['R']:.3f}\")\n",
    "\n",
    "    # initial recall-friendly threshold\n",
    "    thr0, *_ = pick_thr_with_floor(model, val_loader, max_batches=200, n_bins=256,\n",
    "                                   beta=2.0, min_pos_rate=thr_pos_rate_early[0], max_pos_rate=thr_pos_rate_early[1])\n",
    "    thr0 = float(max(0.05, min(0.20, thr0)))\n",
    "    print(f\"[thr0] ≈ {thr0:.3f}\")\n",
    "\n",
    "    # --------------- Head-only calibration (BCE) ---------------\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        for p in model.head.parameters(): p.requires_grad = True\n",
    "    head_posw = torch.tensor(head_pos_weight, device=device)\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=head_lr, weight_decay=0.0)\n",
    "    for ep in range(1, head_epochs+1):\n",
    "        model.train(); seen=0; loss_sum=0.0\n",
    "        tp=fp=fn=0.0\n",
    "        for b,(xb,yb) in enumerate(train_loader, 1):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, yb_r, pos_weight=head_posw)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=head_batches: break\n",
    "        stats = _pix_eval(model, train_loader, thr=thr0, max_batches=quick_eval_train_batches)\n",
    "        print(f\"[HEAD] ep{ep} loss {loss_sum/seen:.4f} | F1 {stats['F']:.3f} P {stats['P']:.3f} R {stats['R']:.3f}\")\n",
    "\n",
    "    # --------------- Tail probe (gentle BCE+IoU) ---------------\n",
    "    # Unfreeze tail (best-effort): head + (u4,u3,aspp)\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        for p in model.head.parameters(): p.requires_grad = True\n",
    "    tails = []\n",
    "    for g in [\"u4\",\"u3\",\"aspp\"]:\n",
    "        if _unfreeze_if_exists(g): tails.append(g)\n",
    "    core_probe = BCEIoUEdge(lambda_bce=0.9, pos_weight=tail_pos_weight, lambda_edge=0.0).to(device)\n",
    "    opt = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=tail_lr, weight_decay=1e-4)\n",
    "    for ep in range(1, tail_epochs+1):\n",
    "        model.train(); seen=0; loss_sum=0.0\n",
    "        for b,(xb,yb) in enumerate(train_loader, 1):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = core_probe(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=tail_batches: break\n",
    "    stats = _pix_eval(model, train_loader, thr=thr0, max_batches=quick_eval_train_batches)\n",
    "    print(f\"[tail-probe] loss≈{loss_sum/seen:.4f}\")\n",
    "    print(\"[quick_prob_stats] train @ thr0:\", {k:round(v,3) for k,v in stats.items()})\n",
    "\n",
    "    # --------------- Long training (cosine restarts + curriculum) ---------------\n",
    "    best = {\"F\": -1.0, \"state\": None, \"thr\": thr0, \"ep\": 0}\n",
    "    metric_thr = thr0\n",
    "\n",
    "    for ep in range(1, max_epochs+1):\n",
    "        # freeze/unfreeze per phase\n",
    "        groups = _apply_phase(ep)\n",
    "        core, aftl, w = _make_loss_for_epoch(ep)\n",
    "        opt, sched = _make_opt_sched(ep, base_lrs, weight_decay)\n",
    "\n",
    "        model.train(); seen=0; loss_sum=0.0; t0=time.time()\n",
    "        for i,(xb,yb) in enumerate(train_loader, 1):\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = blended_loss(core, aftl, w, logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step(); sched.step(i)\n",
    "            loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "        train_loss = loss_sum/seen\n",
    "        tr_stats = _pix_eval(model, train_loader, thr=metric_thr, max_batches=quick_eval_train_batches)\n",
    "        print(f\"[EP{ep:02d}] loss {train_loss:.4f} | train P {tr_stats['P']:.3f} R {tr_stats['R']:.3f} F {tr_stats['F']:.3f} \"\n",
    "              f\"| pos≈{tr_stats['pos_mean']:.3f} neg≈{tr_stats['neg_mean']:.3f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "        if ep % val_every == 0 or ep <= 3:\n",
    "            # pick threshold; early epochs looser pos-rate, late epochs tighter\n",
    "            pr_min, pr_max = (thr_pos_rate_early if ep < 26 else thr_pos_rate_late)\n",
    "            thr, (VP,VR,VF), aux = pick_thr_with_floor(\n",
    "                model, val_loader,\n",
    "                max_batches=120, n_bins=256, beta=thr_beta,\n",
    "                min_pos_rate=pr_min, max_pos_rate=pr_max\n",
    "            )\n",
    "            metric_thr = float(thr)\n",
    "            print(f\"[thr@ep{ep}] thr={metric_thr:.3f} | val P {VP:.3f} R {VR:.3f} F {VF:.3f} | pos_rate≈{aux['pos_rate']:.3f}\")\n",
    "\n",
    "            val_stats = _pix_eval(model, val_loader, thr=metric_thr, max_batches=quick_eval_val_batches)\n",
    "            print(f\"[VAL ep{ep}] P {val_stats['P']:.3f} R {val_stats['R']:.3f} F {val_stats['F']:.3f}\")\n",
    "            if val_stats['F'] > best[\"F\"]:\n",
    "                best = {\"F\": val_stats['F'], \"state\": copy.deepcopy(model.state_dict()),\n",
    "                        \"thr\": metric_thr, \"ep\": ep, \"P\": val_stats[\"P\"], \"R\": val_stats[\"R\"]}\n",
    "                if save_best_to:\n",
    "                    torch.save({\"state\": best[\"state\"], \"thr\": best[\"thr\"], \"ep\": best[\"ep\"],\n",
    "                                \"P\":best[\"P\"], \"R\":best[\"R\"], \"F\":best[\"F\"]}, save_best_to)\n",
    "                    print(f\"  ↳ saved best → {save_best_to} (F={best['F']:.3f}, thr={best['thr']:.3f}, ep={best['ep']})\")\n",
    "\n",
    "    # Load best and return\n",
    "    if best[\"state\"] is not None:\n",
    "        model.load_state_dict(best[\"state\"], strict=True)\n",
    "    summary = {\"best_F\": float(best[\"F\"]), \"best_P\": float(best.get(\"P\", 0.0)), \"best_R\": float(best.get(\"R\", 0.0)),\n",
    "               \"best_ep\": int(best[\"ep\"]), \"final_thr\": float(best[\"thr\"])}\n",
    "    print(\"=== DONE ===\")\n",
    "    print(\"Best summary:\", summary)\n",
    "    return model, best[\"thr\"], summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4742dd09-124a-42d5-a78e-9a2b492a6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with h5py.File(DATA[\"train_h5\"], \"r\") as f:\n",
    "    N = f[\"images\"].shape[0]\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "split = int(0.9*N)\n",
    "idx_tr, idx_va = np.sort(idx[:split]), np.sort(idx[split:])\n",
    "\n",
    "ds_full = H5TiledDataset(DATA[\"train_h5\"], tile=TILE, k_sigma=5.0)\n",
    "\n",
    "pos_panels = panels_with_positives(DATA[\"train_h5\"], max_panels=2000)\n",
    "sub_tr = np.random.default_rng(SEED).choice(np.intersect1d(idx_tr, pos_panels),\n",
    "                                            size=min(200, len(pos_panels)), replace=False)\n",
    "sub_va = np.random.default_rng(SEED+1).choice(np.intersect1d(idx_va, pos_panels),\n",
    "                                              size=min(80, len(pos_panels)), replace=False)\n",
    "\n",
    "train_loader_small = DataLoader(SubsetDS(ds_full, np.sort(sub_tr)), batch_size=BATCH, shuffle=True,\n",
    "                                num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader_small   = DataLoader(SubsetDS(ds_full, np.sort(sub_va)), batch_size=BATCH, shuffle=False,\n",
    "                                num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "# Full loaders (for end-to-end epochs)\n",
    "train_loader = DataLoader(SubsetDS(ds_full, idx_tr), batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader   = DataLoader(SubsetDS(ds_full, idx_va), batch_size=BATCH, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "839cd470-89d0-40bf-8e3c-f20271b00e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup…\n",
      "[WARMUP] ep1 loss 0.3786 | F1 0.028 P 0.014 R 0.478\n",
      "[thr0] ≈ 0.200\n",
      "[HEAD] ep1 loss 0.0908 | F1 0.011 P 0.006 R 0.055\n",
      "[HEAD] ep2 loss 0.0827 | F1 0.037 P 0.025 R 0.068\n",
      "[tail-probe] loss≈0.1325\n",
      "[quick_prob_stats] train @ thr0: {'P': 0.0, 'R': 0.0, 'F': 0.0, 'pos_mean': 0.012, 'neg_mean': 0.005}\n",
      "[phase] ep=1 | trainable params=33 | groups=['head']\n",
      "[EP01] loss 0.4639 | train P 0.059 R 0.063 F 0.061 | pos≈0.081 neg≈0.023 | 609.8s\n",
      "[thr@ep1] thr=0.178 | val P 0.055 R 0.112 F 0.074 | pos_rate≈0.007\n",
      "[VAL ep1] P 0.357 R 0.163 F 0.224\n",
      "  ↳ saved best → ckpt_best.pt (F=0.224, thr=0.178, ep=1)\n",
      "[phase] ep=2 | trainable params=33 | groups=['head']\n",
      "[EP02] loss 0.4637 | train P 0.030 R 0.063 F 0.041 | pos≈0.047 neg≈0.024 | 610.6s\n",
      "[thr@ep2] thr=0.189 | val P 0.054 R 0.114 F 0.074 | pos_rate≈0.007\n",
      "[VAL ep2] P 0.353 R 0.167 F 0.226\n",
      "  ↳ saved best → ckpt_best.pt (F=0.226, thr=0.189, ep=2)\n",
      "[phase] ep=3 | trainable params=33 | groups=['head']\n",
      "[EP03] loss 0.4636 | train P 0.043 R 0.148 F 0.067 | pos≈0.053 neg≈0.024 | 610.1s\n",
      "[thr@ep3] thr=0.197 | val P 0.057 R 0.110 F 0.075 | pos_rate≈0.007\n",
      "[VAL ep3] P 0.367 R 0.162 F 0.225\n",
      "[phase] ep=4 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP04] loss 0.4612 | train P 0.007 R 0.013 F 0.009 | pos≈0.055 neg≈0.021 | 1338.8s\n",
      "[phase] ep=5 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP05] loss 0.4583 | train P 0.034 R 0.098 F 0.050 | pos≈0.048 neg≈0.021 | 1339.3s\n",
      "[phase] ep=6 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP06] loss 0.4568 | train P 0.164 R 0.237 F 0.194 | pos≈0.154 neg≈0.020 | 1339.8s\n",
      "[thr@ep6] thr=0.537 | val P 0.346 R 0.238 F 0.282 | pos_rate≈0.002\n",
      "[VAL ep6] P 0.839 R 0.189 F 0.308\n",
      "  ↳ saved best → ckpt_best.pt (F=0.308, thr=0.537, ep=6)\n",
      "[phase] ep=7 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP07] loss 0.4553 | train P 0.510 R 0.190 F 0.277 | pos≈0.193 neg≈0.021 | 1339.6s\n",
      "[phase] ep=8 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP08] loss 0.4540 | train P 0.317 R 0.434 F 0.367 | pos≈0.343 neg≈0.021 | 1339.6s\n",
      "[phase] ep=9 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP09] loss 0.4532 | train P 0.458 R 0.485 F 0.471 | pos≈0.382 neg≈0.019 | 1339.6s\n",
      "[thr@ep9] thr=0.592 | val P 0.469 R 0.241 F 0.318 | pos_rate≈0.002\n",
      "[VAL ep9] P 0.994 R 0.075 F 0.139\n",
      "[phase] ep=10 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP10] loss 0.4527 | train P 0.380 R 0.254 F 0.304 | pos≈0.137 neg≈0.020 | 1339.6s\n",
      "[phase] ep=11 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP11] loss 0.5332 | train P 0.459 R 0.304 F 0.366 | pos≈0.394 neg≈0.016 | 1357.6s\n",
      "[phase] ep=12 | trainable params=3,000,985 | groups=['u4', 'u3', 'aspp']\n",
      "[EP12] loss 0.5330 | train P 0.663 R 0.258 F 0.372 | pos≈0.219 neg≈0.015 | 1353.0s\n",
      "[thr@ep12] thr=0.545 | val P 0.490 R 0.260 F 0.339 | pos_rate≈0.002\n",
      "[VAL ep12] P 0.992 R 0.059 F 0.112\n",
      "[phase] ep=13 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n",
      "[EP13] loss 0.5323 | train P 0.372 R 0.296 F 0.330 | pos≈0.257 neg≈0.021 | 1392.6s\n",
      "[phase] ep=14 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n",
      "[EP14] loss 0.5320 | train P 0.249 R 0.176 F 0.206 | pos≈0.114 neg≈0.018 | 1393.4s\n",
      "[phase] ep=15 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n",
      "[EP15] loss 0.5318 | train P 0.309 R 0.321 F 0.315 | pos≈0.322 neg≈0.021 | 1386.4s\n",
      "[thr@ep15] thr=0.686 | val P 0.489 R 0.269 F 0.347 | pos_rate≈0.002\n",
      "[VAL ep15] P 0.946 R 0.088 F 0.161\n",
      "[phase] ep=16 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n",
      "[EP16] loss 0.5316 | train P 0.685 R 0.490 F 0.571 | pos≈0.361 neg≈0.018 | 1400.5s\n",
      "[phase] ep=17 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n",
      "[EP17] loss 0.5313 | train P 0.520 R 0.410 F 0.458 | pos≈0.339 neg≈0.022 | 1409.6s\n",
      "[phase] ep=18 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n",
      "[EP18] loss 0.5312 | train P 0.358 R 0.415 F 0.384 | pos≈0.319 neg≈0.020 | 1449.2s\n",
      "[thr@ep18] thr=0.611 | val P 0.496 R 0.284 F 0.362 | pos_rate≈0.002\n",
      "[VAL ep18] P 0.845 R 0.112 F 0.197\n",
      "[phase] ep=19 | trainable params=4,207,417 | groups=['u4', 'u3', 'u2', 'aspp']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = UNetResSEASPP(in_ch=\u001b[32m1\u001b[39m, out_ch=\u001b[32m1\u001b[39m).to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model, thr, summary = train_full_probe(\n\u001b[32m      3\u001b[39m     model,\n\u001b[32m      4\u001b[39m     train_loader=train_loader,\n\u001b[32m      5\u001b[39m     val_loader=val_loader,\n\u001b[32m      6\u001b[39m     resize_masks_to=resize_masks_to,\n\u001b[32m      7\u001b[39m     pick_thr_with_floor=pick_thr_with_floor,\n\u001b[32m      8\u001b[39m     save_best_to=\u001b[33m\"\u001b[39m\u001b[33mckpt_best.pt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal threshold:\u001b[39m\u001b[33m\"\u001b[39m, thr)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSummary:\u001b[39m\u001b[33m\"\u001b[39m, summary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mtrain_full_probe\u001b[39m\u001b[34m(model, train_loader, val_loader, resize_masks_to, pick_thr_with_floor, device, seed, init_head_prior, warmup_epochs, warmup_batches, warmup_lr, warmup_pos_weight, head_epochs, head_batches, head_lr, head_pos_weight, tail_epochs, tail_batches, tail_lr, tail_pos_weight, max_epochs, val_every, base_lrs, weight_decay, thr_beta, thr_pos_rate_early, thr_pos_rate_late, save_best_to, quick_eval_train_batches, quick_eval_val_batches)\u001b[39m\n\u001b[32m    140\u001b[39m xb,yb = xb.to(device), yb.to(device)\n\u001b[32m    141\u001b[39m logits = model(xb); yb_r = resize_masks_to(logits, yb)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m loss = blended_loss(core, aftl, w, logits, yb_r)\n\u001b[32m    143\u001b[39m opt.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m); loss.backward()\n\u001b[32m    144\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mblended_loss\u001b[39m\u001b[34m(core, aftl, w, logits, targets)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mblended_loss\u001b[39m(core, aftl, w, logits, targets):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     loss = w[\u001b[33m\"\u001b[39m\u001b[33mw_core\u001b[39m\u001b[33m\"\u001b[39m] * core(logits, targets)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m aftl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m w.get(\u001b[33m\"\u001b[39m\u001b[33mw_aftl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) > \u001b[32m0\u001b[39m:\n\u001b[32m     85\u001b[39m         loss = loss + w[\u001b[33m\"\u001b[39m\u001b[33mw_aftl\u001b[39m\u001b[33m\"\u001b[39m] * aftl(logits, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/Asteroid_detection_CNN/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mBCEIoUEdge.forward\u001b[39m\u001b[34m(self, logits, targets)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits, targets):\n\u001b[32m     72\u001b[39m     t = targets.clamp(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     posw = torch.tensor(\u001b[38;5;28mself\u001b[39m.posw, device=logits.device)\n\u001b[32m     74\u001b[39m     bce  = F.binary_cross_entropy_with_logits(logits, t, pos_weight=posw)\n\u001b[32m     75\u001b[39m     siou = \u001b[38;5;28mself\u001b[39m.iou(logits, t)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = UNetResSEASPP(in_ch=1, out_ch=1).to('cuda')\n",
    "model, thr, summary = train_full_probe(\n",
    "    model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    resize_masks_to=resize_masks_to,\n",
    "    pick_thr_with_floor=pick_thr_with_floor,\n",
    "    save_best_to=\"ckpt_best.pt\",\n",
    ")\n",
    "print(\"Final threshold:\", thr)\n",
    "print(\"Summary:\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
