{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a27db1-ce8a-4322-b87a-ac1658a81e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: Setup & Safeguards ====\n",
    "import os, math, json, time, copy, random, h5py\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Repro ---\n",
    "SEED = 123\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "DATA = {\n",
    "    \"train_h5\": \"/home/karlo/train_chunked.h5\",\n",
    "    \"test_h5\":  \"../DATA/test.h5\",\n",
    "    \"train_csv\": \"../DATA/train.csv\",  \n",
    "    \"test_csv\":  \"../DATA/test.csv\",\n",
    "}\n",
    "TILE         = 128\n",
    "BATCH        = 64\n",
    "NUM_WORKERS  = 2          # HDF5 is happier with 0–2\n",
    "\n",
    "# --- IMPORTANT: don't shadow torch.nn.functional as \"F\" anywhere below ---\n",
    "# Avoid variables named \"F\" (use f1, fbeta, etc.)\n",
    "\n",
    "# Expect these to exist from your data code:\n",
    "# - train_loader_small: small loader with guaranteed positives (for warmup/head/tail probe)\n",
    "# - train_loader: full training loader\n",
    "# - val_loader_small or val_loader: a validation loader (either is fine)\n",
    "# If their names differ, just alias them here:\n",
    "# val_loader = val_loader_small\n",
    "\n",
    "# --- Helper: ensure mask matches model output size ---\n",
    "def resize_masks_to(logits, masks):\n",
    "    if logits.shape[-2:] == masks.shape[-2:]:\n",
    "        return masks\n",
    "    return F.interpolate(masks, size=logits.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "# --- Tiny metric (pixel-level) ---\n",
    "@torch.no_grad()\n",
    "def pix_metrics(model, loader, thr=0.5, n_batches=6):\n",
    "    model.eval()\n",
    "    dev = next(model.parameters()).device\n",
    "    tp = fp = fn = 0.0\n",
    "    pos_means, neg_means = [], []\n",
    "    t0 = time.time()\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb, yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        yb_r   = resize_masks_to(logits, yb)\n",
    "        p      = torch.sigmoid(logits)\n",
    "        if (yb_r>0.5).any(): pos_means.append(float(p[yb_r>0.5].mean()))\n",
    "        neg_means.append(float(p[yb_r<=0.5].mean()))\n",
    "        pv, tv = p.reshape(-1), yb_r.reshape(-1)\n",
    "        pred   = (pv>=thr).float()\n",
    "        tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "        if i>=n_batches: break\n",
    "    P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "    print(f\"[quick_prob_stats] batches={min(n_batches,i)} | pos≈{np.mean(pos_means) if pos_means else float('nan'):.4f} | \"\n",
    "          f\"neg≈{np.mean(neg_means):.4f} | P {P:.3f} R {R:.3f} F1 {f1:.3f} @ thr={thr:.3f} | {time.time()-t0:.1f}s\")\n",
    "    return dict(P=P,R=R,F1=f1,pos_mean=np.mean(pos_means) if pos_means else float('nan'),neg_mean=np.mean(neg_means))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605a7671-8042-47ff-b4cf-57bf2fa1b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 2: Losses (SoftIoU + BCE) ====\n",
    "\n",
    "class SoftIoU(nn.Module):\n",
    "    def __init__(self, eps=1e-6): \n",
    "        super().__init__(); self.eps=eps\n",
    "    def forward(self, logits, targets):\n",
    "        p = torch.sigmoid(logits)\n",
    "        t = targets.clamp(0,1)\n",
    "        p = p.view(p.size(0), -1)\n",
    "        t = t.view(t.size(0), -1)\n",
    "        inter = (p*t).sum(1)\n",
    "        union = p.sum(1) + t.sum(1) - inter\n",
    "        iou   = (inter + self.eps) / (union + self.eps)\n",
    "        return (1 - iou).mean()\n",
    "\n",
    "class SoftIoUWithBCE(nn.Module):\n",
    "    \"\"\"\n",
    "    total = lambda_bce * BCE(pos_weight) + (1 - lambda_bce) * SoftIoU\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight=8.0, lambda_bce=0.7):\n",
    "        super().__init__()\n",
    "        self.lambda_bce = float(lambda_bce)\n",
    "        self.pos_weight = float(pos_weight)\n",
    "        self.bce  = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.siou = SoftIoU()\n",
    "    def forward(self, logits, targets):\n",
    "        t = targets.clamp(0,1)\n",
    "        loss_bce  = self.bce(logits, t) if self.pos_weight<=0 else \\\n",
    "                    F.binary_cross_entropy_with_logits(logits, t, pos_weight=torch.tensor(self.pos_weight, device=logits.device))\n",
    "        loss_siou = self.siou(logits, t)\n",
    "        return self.lambda_bce*loss_bce + (1.0-self.lambda_bce)*loss_siou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b4ca80d-601e-43dc-90b4-2309a7843b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 3: Fast threshold pickers ====\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_under_min(model, loader, max_batches=40, n_bins=256, beta=2.0):\n",
    "    \"\"\"Histogram-based pixel threshold selection (recall-lean if beta>1).\"\"\"\n",
    "    model.eval(); dev = next(model.parameters()).device\n",
    "    hist_pos = torch.zeros(n_bins, device=dev); hist_neg = torch.zeros(n_bins, device=dev)\n",
    "    edges = torch.linspace(0,1,n_bins+1, device=dev)\n",
    "    for i,(xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(dev), yb.to(dev)\n",
    "        p = torch.sigmoid(model(xb))\n",
    "        yb_r = resize_masks_to(p, yb)\n",
    "        pv = p.reshape(-1); tv = (yb_r>0.5).reshape(-1)\n",
    "        hist_pos += torch.histc(pv[tv], bins=n_bins, min=0, max=1)\n",
    "        hist_neg += torch.histc(pv[~tv], bins=n_bins, min=0, max=1)\n",
    "        if i>=max_batches: break\n",
    "    cpos = torch.flip(torch.cumsum(torch.flip(hist_pos, dims=[0]), 0), dims=[0])  # >=t\n",
    "    cneg = torch.flip(torch.cumsum(torch.flip(hist_neg, dims=[0]), 0), dims=[0])\n",
    "    TP = cpos; FP = cneg; FN = (hist_pos.sum() - TP).clamp(min=0)\n",
    "    P = TP / (TP + FP + 1e-8); R = TP / (TP + FN + 1e-8)\n",
    "    fbeta = (1+beta*beta)*P*R / (beta*beta*P + R + 1e-8)\n",
    "    idx = int(torch.argmax(fbeta).item())\n",
    "    thr = float((edges[idx] + edges[idx+1])/2)\n",
    "    return thr, (float(P[idx]), float(R[idx]), float(fbeta[idx])), dict(pos_rate=float((TP[idx]+FP[idx])/(hist_pos.sum()+hist_neg.sum()+1e-8)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def pick_thr_with_floor(model, loader, max_batches=40, n_bins=256, beta=1.0, min_pos_rate=0.05, max_pos_rate=0.10):\n",
    "    thr, (P,R,F), aux = pick_thr_under_min(model, loader, max_batches=max_batches, n_bins=n_bins, beta=beta)\n",
    "    # simple clamp pass using percentile of preds to hit pos_rate band\n",
    "    # (if your earlier “floor” function is available, feel free to swap it in)\n",
    "    return thr, (P,R,F), aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99bb8444-8313-4d17-b7aa-f8babea9cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4: Freeze/Unfreeze + Warmup/Head fits ====\n",
    "\n",
    "def set_requires_grad(mod, flag: bool):\n",
    "    for p in mod.parameters(): p.requires_grad = flag\n",
    "\n",
    "def freeze_all(model): set_requires_grad(model, False)\n",
    "\n",
    "def unfreeze_head_only(model):\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"):\n",
    "        set_requires_grad(model.head, True)\n",
    "    else:\n",
    "        raise AttributeError(\"Model has no attribute 'head'.\")\n",
    "\n",
    "def unfreeze_head_and_tail(model):\n",
    "    \"\"\"\n",
    "    Unfreeze head + late upsample blocks + ASPP (assumes UNetResSEASPP)\n",
    "    Adjust attribute names if your class differs.\n",
    "    \"\"\"\n",
    "    freeze_all(model)\n",
    "    if hasattr(model, \"head\"): set_requires_grad(model.head, True)\n",
    "    for path in [\"u3\", \"u4\", \"aspp\"]:\n",
    "        if hasattr(model, path): set_requires_grad(getattr(model, path), True)\n",
    "\n",
    "def init_head_bias_to_prior(model, p0=0.70):\n",
    "    b = math.log(p0/(1-p0))\n",
    "    with torch.no_grad():\n",
    "        if hasattr(model, \"head\") and hasattr(model.head, \"bias\"):\n",
    "            model.head.bias.data.fill_(b)\n",
    "\n",
    "def fit_quick_warmup(model, loader, epochs=2, max_batches=800, lr=2e-4, metric_thr=0.20, pos_weight=30.0):\n",
    "    dev = next(model.parameters()).device\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    for ep in range(1, epochs+1):\n",
    "        seen=tp=fp=fn=0.0; loss_sum=0.0\n",
    "        t0=time.time()\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb)\n",
    "            yb_r   = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits); pv, tv = p.view(-1), yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[WARMUP] ep{ep} loss {loss_sum/seen:.4f} | F1 {f1:.3f} P {P:.3f} R {R:.3f}\")\n",
    "\n",
    "def fit_head_only(model, loader, epochs=2, max_batches=600, lr=3e-5, metric_thr=0.15, pos_weight=5.0):\n",
    "    dev = next(model.parameters()).device\n",
    "    unfreeze_head_only(model)\n",
    "    head_params = [p for p in model.head.parameters() if p.requires_grad]\n",
    "    opt = torch.optim.Adam(head_params, lr=lr, weight_decay=0.0)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=dev))\n",
    "    for ep in range(1, epochs+1):\n",
    "        seen=tp=fp=fn=0.0; loss_sum=0.0\n",
    "        for b,(xb,yb) in enumerate(loader,1):\n",
    "            xb,yb = xb.to(dev), yb.to(dev)\n",
    "            logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "            loss = bce(logits, yb_r)\n",
    "            opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "            with torch.no_grad():\n",
    "                p = torch.sigmoid(logits); pv, tv = p.view(-1), yb_r.view(-1)\n",
    "                pred = (pv>=metric_thr).float()\n",
    "                tp += float((pred*tv).sum()); fp += float((pred*(1-tv)).sum()); fn += float(((1-pred)*tv).sum())\n",
    "                loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "            if b>=max_batches: break\n",
    "        P = tp/max(tp+fp,1); R = tp/max(tp+fn,1); f1 = 2*P*R/max(P+R,1e-8)\n",
    "        print(f\"[HEAD] ep{ep} loss {loss_sum/seen:.4f} | F1 {f1:.3f} P {P:.3f} R {R:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa381abe-4bbf-4324-b785-08869d4b9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self,c,r=8): super().__init__(); self.fc1=nn.Conv2d(c,c//r,1); self.fc2=nn.Conv2d(c//r,c,1)\n",
    "    def forward(self,x): s=F.adaptive_avg_pool2d(x,1); s=F.silu(self.fc1(s),inplace=True); s=torch.sigmoid(self.fc2(s)); return x*s\n",
    "def _norm(c, groups=8): g=min(groups,c) if c%groups==0 else 1; return nn.GroupNorm(g,c)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); p=1\n",
    "    def __init__(self,c_in,c_out,k=3,act=nn.SiLU,se=True):\n",
    "        super().__init__(); p=k//2\n",
    "        self.proj = nn.Identity() if c_in==c_out else nn.Conv2d(c_in,c_out,1)\n",
    "        self.bn1=_norm(c_in); self.c1=nn.Conv2d(c_in,c_out,k,padding=p,bias=False)\n",
    "        self.bn2=_norm(c_out); self.c2=nn.Conv2d(c_out,c_out,k,padding=p,bias=False)\n",
    "        self.act=act(); self.se=SEBlock(c_out) if se else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        h=self.act(self.bn1(x)); h=self.c1(h)\n",
    "        h=self.act(self.bn2(h)); h=self.c2(h)\n",
    "        h=self.se(h); return h + self.proj(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self,c_in,c_out): super().__init__(); self.pool=nn.MaxPool2d(2); self.rb=ResBlock(c_in,c_out)\n",
    "    def forward(self,x): return self.rb(self.pool(x))\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self,c_in,c_skip,c_out): super().__init__(); self.up=nn.ConvTranspose2d(c_in,c_in,2,stride=2); self.rb1=ResBlock(c_in+c_skip,c_out); self.rb2=ResBlock(c_out,c_out)\n",
    "    def forward(self,x,skip):\n",
    "        x=self.up(x)\n",
    "        dh=skip.size(-2)-x.size(-2); dw=skip.size(-1)-x.size(-1)\n",
    "        if dh or dw: x=F.pad(x,(0,max(0,dw),0,max(0,dh)))\n",
    "        x=torch.cat([x,skip],1); x=self.rb1(x); x=self.rb2(x); return x\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self,c,r=[1,6,12,18]):\n",
    "        super().__init__()\n",
    "        self.blocks=nn.ModuleList([nn.Sequential(nn.Conv2d(c,c//4,3,padding=d,dilation=d,bias=False), nn.BatchNorm2d(c//4), nn.SiLU(True)) for d in r])\n",
    "        self.project=nn.Conv2d(c,c,1)\n",
    "    def forward(self,x): return self.project(torch.cat([b(x) for b in self.blocks],1))\n",
    "\n",
    "class UNetResSE(nn.Module):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(); w=widths\n",
    "        self.stem=nn.Sequential(nn.Conv2d(in_ch,w[0],3,padding=1,bias=False), nn.BatchNorm2d(w[0]), nn.SiLU(True), ResBlock(w[0],w[0]))\n",
    "        self.d1=Down(w[0],w[1]); self.d2=Down(w[1],w[2]); self.d3=Down(w[2],w[3]); self.d4=Down(w[3],w[4])\n",
    "        self.u1=Up(w[4],w[3],w[3]); self.u2=Up(w[3],w[2],w[2]); self.u3=Up(w[2],w[1],w[1]); self.u4=Up(w[1],w[0],w[0])\n",
    "        self.head=nn.Conv2d(w[0],out_ch,1)\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x) # logits\n",
    "\n",
    "class UNetResSEASPP(UNetResSE):\n",
    "    def __init__(self,in_ch=1,out_ch=1,widths=(32,64,128,256,512)):\n",
    "        super().__init__(in_ch,out_ch,widths); self.aspp=ASPP(widths[-1]); self.d4=Down(widths[3],widths[4])\n",
    "    def forward(self,x):\n",
    "        s0=self.stem(x); s1=self.d1(s0); s2=self.d2(s1); s3=self.d3(s2); b=self.d4(s3); b=self.aspp(b)\n",
    "        x=self.u1(b,s3); x=self.u2(x,s2); x=self.u3(x,s1); x=self.u4(x,s0); return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a776c6-77d0-4fa7-8da3-d9eb13427372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_stats_mad(arr):\n",
    "    med = np.median(arr); mad = np.median(np.abs(arr - med))\n",
    "    sigma = 1.4826 * (mad + 1e-12)\n",
    "    return np.float32(med), np.float32(1.0 if not np.isfinite(sigma) or sigma<=0 else sigma)\n",
    "\n",
    "class H5TiledDataset(Dataset):\n",
    "    \"\"\"Stream tiles from big (H,W) images, robust-normalize per-image, k-sigma clip, pad edges.\"\"\"\n",
    "    def __init__(self, h5_path, tile=128, k_sigma=5.0, crop_for_stats=512):\n",
    "        self.h5_path, self.tile, self.k_sigma, self.crop_for_stats = h5_path, int(tile), float(k_sigma), int(crop_for_stats)\n",
    "        self._h5 = self._x = self._y = None\n",
    "        self._stats_cache = {}\n",
    "        with h5py.File(self.h5_path, \"r\") as f:\n",
    "            self.N, self.H, self.W = f[\"images\"].shape\n",
    "            assert f[\"masks\"].shape == (self.N, self.H, self.W)\n",
    "        Hb = math.ceil(self.H/self.tile); Wb = math.ceil(self.W/self.tile)\n",
    "        self.indices = [(i, r, c) for i in range(self.N) for r in range(Hb) for c in range(Wb)]\n",
    "    def _ensure(self):\n",
    "        if self._h5 is None:\n",
    "            self._h5 = h5py.File(self.h5_path, \"r\")\n",
    "            self._x, self._y = self._h5[\"images\"], self._h5[\"masks\"]\n",
    "    def _image_stats(self, i):\n",
    "        if i in self._stats_cache: return self._stats_cache[i]\n",
    "        s = min(self.crop_for_stats, self.H, self.W)\n",
    "        h0, w0 = (self.H-s)//2, (self.W-s)//2\n",
    "        crop = self._x[i, h0:h0+s, w0:w0+s].astype(\"float32\")\n",
    "        med, sig = robust_stats_mad(crop); self._stats_cache[i] = (med, sig); return med, sig\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        self._ensure()\n",
    "        i, r, c = self.indices[idx]; t = self.tile\n",
    "        r0, c0 = r*t, c*t; r1, c1 = min(r0+t, self.H), min(c0+t, self.W)\n",
    "        x = self._x[i, r0:r1, c0:c1].astype(\"float32\"); y = self._y[i, r0:r1, c0:c1].astype(\"float32\")\n",
    "        if x.shape != (t,t):\n",
    "            xp = np.zeros((t,t), np.float32); yp = np.zeros((t,t), np.float32)\n",
    "            xp[:x.shape[0], :x.shape[1]] = x; yp[:y.shape[0], :y.shape[1]] = y; x, y = xp, yp\n",
    "        med, sig = self._image_stats(i); x = np.clip((x-med)/sig, -5, 5)\n",
    "        return torch.from_numpy(x[None,...]), torch.from_numpy(y[None,...])\n",
    "\n",
    "class SubsetDS(Dataset):\n",
    "    \"\"\"Select full panels by id while reusing tiling of base dataset.\"\"\"\n",
    "    def __init__(self, base, panel_ids):\n",
    "        self.base, self.panel_ids = base, np.asarray(panel_ids)\n",
    "        t = base.tile; Hb, Wb = math.ceil(base.H/t), math.ceil(base.W/t)\n",
    "        base_map = {(i,r,c):k for k,(i,r,c) in enumerate(base.indices)}\n",
    "        self.map = [base_map[(i,r,c)] for i in self.panel_ids for r in range(Hb) for c in range(Wb)]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, k): return self.base[self.map[k]]\n",
    "\n",
    "def tile_pos_weights(h5_path, tile=128):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        Y = f[\"masks\"]; N,H,W = Y.shape\n",
    "    Hb, Wb = math.ceil(H/tile), math.ceil(W/tile)\n",
    "    w = []\n",
    "    with h5py.File(h5_path,\"r\") as f:\n",
    "        Y = f[\"masks\"]\n",
    "        for i in range(N):\n",
    "            for r in range(Hb):\n",
    "                for c in range(Wb):\n",
    "                    r0,c0=r*tile,c*tile; r1,c1=min(r0+tile,H),min(c0+tile,W)\n",
    "                    w.append(1.0 + 9.0*(Y[i,r0:r1,c0:c1].any()))\n",
    "    return np.asarray(w, np.float64)\n",
    "\n",
    "\n",
    "def panels_with_positives(h5_path, tile=128, max_panels=None):\n",
    "    ids=[]\n",
    "    with h5py.File(h5_path,'r') as f:\n",
    "        Y=f['masks']; N,H,W=Y.shape\n",
    "        rng = np.random.default_rng(0)\n",
    "        order = rng.permutation(N) if max_panels else np.arange(N)\n",
    "        for i in order:\n",
    "            yi = Y[i]\n",
    "            if yi.any(): ids.append(i)\n",
    "            if max_panels and len(ids)>=max_panels: break\n",
    "    return np.array(sorted(ids))\n",
    "\n",
    "# per-panel median/MAD normalize, clip like stream_panels_direct\n",
    "def norm_medmad_clip(x, clip=5.0, eps=1e-6):\n",
    "    # x: torch.Tensor [B,1,H,W] or [1,H,W]\n",
    "    if x.ndim == 4:\n",
    "        med = x.median(dim=-1, keepdim=True).values.median(dim=-2, keepdim=True).values\n",
    "    else:  # [1,H,W]\n",
    "        med = x.median()\n",
    "        med = med.view(1,1,1)\n",
    "    mad = (x - med).abs().median()\n",
    "    sigma = 1.4826 * mad + eps\n",
    "    z = (x - med) / sigma\n",
    "    return z.clamp_(-clip, clip)\n",
    "\n",
    "class WithTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.base[i]\n",
    "        x = norm_medmad_clip(x)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d05adaa0-9abc-4d8e-8fa7-d51269c30586",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(DATA[\"train_h5\"], \"r\") as f:\n",
    "    N = f[\"images\"].shape[0]\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "split = int(0.9*N); idx_tr, idx_va = np.sort(idx[:split]), np.sort(idx[split:])\n",
    "\n",
    "ds_full  = H5TiledDataset(DATA[\"train_h5\"], tile=TILE, k_sigma=5.0)\n",
    "\n",
    "pos_panels = panels_with_positives(DATA[\"train_h5\"], max_panels=2000)\n",
    "# Sample small train/val from the intersection with idx_tr/idx_va\n",
    "sub_tr = np.random.default_rng(SEED).choice(np.intersect1d(idx_tr, pos_panels), size=min(200, len(pos_panels)), replace=False)\n",
    "sub_va = np.random.default_rng(SEED+1).choice(np.intersect1d(idx_va, pos_panels), size=min(80,  len(pos_panels)), replace=False)\n",
    "\n",
    "train_ds_small = SubsetDS(ds_full, np.sort(sub_tr))\n",
    "val_ds_small   = SubsetDS(ds_full, np.sort(sub_va))\n",
    "\n",
    "train_ds_small = WithTransform(train_ds_small)\n",
    "val_ds_small   = WithTransform(val_ds_small)\n",
    "\n",
    "train_loader_small = DataLoader(train_ds_small, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader_small   = DataLoader(val_ds_small,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "\n",
    "train_ds = SubsetDS(ds_full, idx_tr)\n",
    "val_ds   = SubsetDS(ds_full, idx_va)\n",
    "\n",
    "train_ds = WithTransform(train_ds)\n",
    "val_ds   = WithTransform(val_ds)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=(device.type=='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9bfba5e-3b03-4612-b9b1-14a7d5b8c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup…\n",
      "[WARMUP] ep1 loss 0.3919 | F1 0.015 P 0.008 R 0.433\n",
      "[thr0] ≈ 0.200\n",
      "[HEAD] ep1 loss 0.1040 | F1 0.032 P 0.017 R 0.272\n",
      "[HEAD] ep2 loss 0.0908 | F1 0.034 P 0.021 R 0.084\n",
      "[quick_prob_stats] batches=6 | pos≈0.0896 | neg≈0.0350 | P 0.011 R 0.022 F1 0.015 @ thr=0.200 | 0.8s\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 5: Build model & warmup ====\n",
    "# If UNetResSEASPP is already defined in your session, this will work directly.\n",
    "# Otherwise import/define it before running this cell.\n",
    "\n",
    "probe = UNetResSEASPP(in_ch=1, out_ch=1).to(device)\n",
    "init_head_bias_to_prior(probe, p0=0.80)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Warmup…\")\n",
    "fit_quick_warmup(probe, train_loader_small, epochs=1, max_batches=400, lr=2e-4, metric_thr=0.20, pos_weight=40.0)\n",
    "\n",
    "thr0, *_ = pick_thr_under_min(probe, val_loader_small, max_batches=40, n_bins=256, beta=2.0)\n",
    "thr0 = float(np.clip(thr0, 0.10, 0.20))\n",
    "print(f\"[thr0] ≈ {thr0:.3f}\")\n",
    "\n",
    "# Head-only calibration\n",
    "fit_head_only(probe, train_loader_small, epochs=2, max_batches=600, lr=3e-5, metric_thr=thr0, pos_weight=5.0)\n",
    "_ = pix_metrics(probe, train_loader_small, thr=thr0, n_batches=6)\n",
    "warm_state = copy.deepcopy(probe.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8519a5f9-e977-4d13-b337-7ef984c0b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tail-probe] loss≈0.3756\n",
      "[quick_prob_stats] batches=6 | pos≈0.0436 | neg≈0.0169 | P 0.000 R 0.000 F1 0.000 @ thr=0.200 | 0.8s\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell 6: Unfreeze tail + loss + brief probe ====\n",
    "probe.load_state_dict(warm_state, strict=True)\n",
    "unfreeze_head_and_tail(probe)\n",
    "\n",
    "criterion = SoftIoUWithBCE(pos_weight=8.0, lambda_bce=0.7).to(device)\n",
    "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, probe.parameters()),\n",
    "                       lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Brief tail probe (optional)\n",
    "probe.train(); loss_sum=0.0; seen=0\n",
    "for b,(xb,yb) in enumerate(train_loader_small,1):\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    logits = probe(xb); yb_r = resize_masks_to(logits, yb)\n",
    "    loss = criterion(logits, yb_r)\n",
    "    opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "    loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "    if b>=400: break\n",
    "print(f\"[tail-probe] loss≈{loss_sum/max(seen,1):.4f}\")\n",
    "\n",
    "_ = pix_metrics(probe, train_loader_small, thr=thr0, n_batches=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69fbb9-0d7c-47cb-89d6-58a592f12872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] batch 744/11520 | loss=0.3783 | 88.7s"
     ]
    }
   ],
   "source": [
    "# ==== Cell 7: Full training (3–6 epochs) with early-stop + threshold repick ====\n",
    "\n",
    "def val_pick_thr(model, vloader, min_pr=0.05, max_pr=0.10):\n",
    "    thr, (P,R,F), aux = pick_thr_with_floor(model, vloader, max_batches=60, n_bins=256, beta=1.0,\n",
    "                                            min_pos_rate=min_pr, max_pos_rate=max_pr)\n",
    "    return float(thr), (P,R,F), aux\n",
    "\n",
    "def one_epoch(model, loader, criterion, opt):\n",
    "    t0=time.time()\n",
    "    model.train(); loss_sum=0.0; seen=0\n",
    "    for b, (xb,yb) in enumerate(loader,1):\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb); yb_r = resize_masks_to(logits, yb)\n",
    "        loss = criterion(logits, yb_r)\n",
    "        opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
    "        loss_sum += float(loss.item())*xb.size(0); seen += xb.size(0)\n",
    "        print(f\"\\r[TRAIN] batch {b}/{len(loader)} | loss={loss_sum/seen:.4f} | {time.time()-t0:.1f}s\", end='')\n",
    "    print (f\"\\r\")\n",
    "    return loss_sum/max(seen,1)\n",
    "\n",
    "def pix_eval(model, loader, thr):\n",
    "    m = pix_metrics(model, loader, thr=thr, n_batches=6)\n",
    "    return m[\"P\"], m[\"R\"], m[\"F1\"]\n",
    "\n",
    "# --- train ---\n",
    "best_F = -1\n",
    "best = {\"state\": None, \"thr\": None, \"ep\": 0}\n",
    "epochs = 4  # adjust 3–6\n",
    "metric_thr = thr0\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    # optional small LR bump if stall\n",
    "    if ep == 3: \n",
    "        for g in opt.param_groups: g[\"lr\"] = 4e-4\n",
    "\n",
    "    ep_loss = one_epoch(probe, train_loader, criterion, opt)\n",
    "    P_tr, R_tr, F_tr = pix_eval(probe, train_loader_small, thr=metric_thr)\n",
    "    print(f\"[EP{ep:02d}] loss {ep_loss:.4f} | train P {P_tr:.3f} R {R_tr:.3f} F {F_tr:.3f}\")\n",
    "\n",
    "    # repick thr on val (pos-rate band recall→precision balance)\n",
    "    metric_thr, (VP,VR,VF), aux = val_pick_thr(probe, val_loader_small, min_pr=0.05, max_pr=0.10)\n",
    "    print(f\"[thr@ep{ep}] thr={metric_thr:.3f} | val P {VP:.3f} R {VR:.3f} F {VF:.3f} | pos_rate≈{aux['pos_rate']:.3f}\")\n",
    "\n",
    "    # simple early-stop on best F\n",
    "    if VF > best_F:\n",
    "        best_F = VF\n",
    "        best[\"state\"] = copy.deepcopy(probe.state_dict())\n",
    "        best[\"thr\"] = metric_thr\n",
    "        best[\"ep\"] = ep\n",
    "        print(f\"[VAL ep{ep}] P {VP:.3f} R {VR:.3f} F {VF:.3f}  ↳ saved best (F={best_F:.3f})\")\n",
    "\n",
    "print(\"Best so far:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445047e0-7baf-4305-bfac-51785581bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 8: Load best, final eval, save ====\n",
    "if best[\"state\"] is not None:\n",
    "    probe.load_state_dict(best[\"state\"], strict=True)\n",
    "    metric_thr = best[\"thr\"]\n",
    "\n",
    "val_stats = pix_metrics(probe, val_loader_small, thr=metric_thr, n_batches=12)\n",
    "print(f\"[FINAL] thr={metric_thr:.3f} | val: P {val_stats['P']:.3f} R {val_stats['R']:.3f} F {val_stats['F1']:.3f}\")\n",
    "\n",
    "# quick train sanity\n",
    "_ = pix_metrics(probe, train_loader_small, thr=metric_thr, n_batches=6)\n",
    "\n",
    "# Save weights & threshold\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "ckpt_path = f\"checkpoints/probe_softiou_bce_best_ep{best['ep']:02d}.pt\"\n",
    "torch.save({\"model\": probe.state_dict(), \"thr\": metric_thr, \"epoch\": best[\"ep\"], \"seed\": SEED}, ckpt_path)\n",
    "\n",
    "with open(\"checkpoints/probe_threshold.json\",\"w\") as f:\n",
    "    json.dump({\"metric_thr\": float(metric_thr)}, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456258c5-626d-41b8-866a-84372e708cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
