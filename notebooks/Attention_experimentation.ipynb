{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182c9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fuser -k /dev/nvidia0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0c9052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 15 02:19:39 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:21:00.0  On |                  N/A |\r\n",
      "| 26%   33C    P8              21W / 260W |     55MiB / 11264MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce RTX 2080 Ti     Off | 00000000:48:00.0 Off |                  N/A |\r\n",
      "| 26%   27C    P8              14W / 260W |      1MiB / 11264MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A     67207      G   /usr/bin/X                                   39MiB |\r\n",
      "|    0   N/A  N/A     67268      G   /usr/bin/gnome-shell                         13MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5733f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "sys.path.append(\"../\")\n",
    "import tools.model\n",
    "import json\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48591974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expend_as(tensor, rep,name):\n",
    "    my_repeat = tf.keras.layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), \n",
    "                                       arguments={'repnum': rep},  name='psi_up'+name)(tensor)\n",
    "    return my_repeat\n",
    "\n",
    "def AttnGatingBlock(x, g, inter_shape, name):\n",
    "    ''' take g which is the spatially smaller signal, do a conv to get the same\n",
    "    number of feature channels as x (bigger spatially)\n",
    "    do a conv on x to also get same geature channels (theta_x)\n",
    "    then, upsample g to be same size as x \n",
    "    add x and g (concat_xg)\n",
    "    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n",
    "    \n",
    "    shape_x = K.int_shape(x)  # 32\n",
    "    shape_g = K.int_shape(g)  # 16\n",
    "\n",
    "    theta_x = tf.keras.layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl'+name)(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = tf.keras.layers.Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = tf.keras.layers.Conv2DTranspose(inter_shape, (3, 3),strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),padding='same', name='g_up'+name)(phi_g)  # 16\n",
    "\n",
    "    concat_xg = tf.keras.layers.add([upsample_g, theta_x])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv2D(1, (1, 1), padding='same', name='psi'+name)(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = tf.keras.layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3],  name)\n",
    "    y = tf.keras.layers.multiply([upsample_psi, x], name='q_attn'+name)\n",
    "\n",
    "    result = tf.keras.layers.Conv2D(shape_x[3], (1, 1), padding='same',name='q_attn_conv'+name)(y)\n",
    "    result_bn = tf.keras.layers.BatchNormalization(name='q_attn_bn'+name)(result)\n",
    "    return result_bn\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm, name):\n",
    "    ''' this is simply 1x1 convolution, bn, activation '''\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\",  kernel_initializer='glorot_normal', \n",
    "               name=name + 'gatingconv')(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization(name=name + 'gatingbn')(x)\n",
    "    x = Activation('relu', name = name + 'gatingact')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15c5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GridAttentionBlockND(tf.keras.layers.Layer):\n",
    "    def __init__(self, gating_channels, inter_channels=None,\n",
    "                 dimension=2, mode='concatenation', sub_sample_factor=(2, 2)):\n",
    "        super(_GridAttentionBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [2, 3]\n",
    "        assert mode in ['concatenation', 'concatenation_debug',\n",
    "                        'concatenation_residual']\n",
    "\n",
    "        # Downsampling rate for the input featuremap\n",
    "        if isinstance(sub_sample_factor, tuple):\n",
    "            self.sub_sample_factor = sub_sample_factor\n",
    "        elif isinstance(sub_sample_factor, list):\n",
    "            self.sub_sample_factor = tuple(sub_sample_factor)\n",
    "        else:\n",
    "            self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
    "\n",
    "        # Default parameter set\n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels (pixel dimensions)\n",
    "        self.gating_channels = gating_channels\n",
    "\n",
    "        assert inter_channels is not None\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = partial(tf.keras.layers.Conv3D,\n",
    "                              kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "            self.upsample = tf.keras.layers.UpSampling3D\n",
    "        elif dimension == 2:\n",
    "            conv_nd = partial(tf.keras.layers.Conv2D,\n",
    "                              kernel_initializer=tf.keras.initializers.HeNormal())\n",
    "            self.upsample = tf.keras.layers.UpSampling2D\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        self.conv_nd = conv_nd\n",
    "\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        self.theta = conv_nd(self.inter_channels,\n",
    "                             kernel_size=self.sub_sample_kernel_size,\n",
    "                             strides=self.sub_sample_factor,\n",
    "                             padding='same', use_bias=False)\n",
    "        self.phi = conv_nd(self.inter_channels, kernel_size=1, strides=1,\n",
    "                           padding='same', use_bias=True)\n",
    "        self.psi = conv_nd(1, kernel_size=1, strides=1, padding='same',\n",
    "                           use_bias=True)\n",
    "        self.W = None\n",
    "        self.phi_upsample = None\n",
    "        self.sigm_psi_upsample = None\n",
    "\n",
    "        # Define the operation\n",
    "        if mode == 'concatenation':\n",
    "            self.operation_function = self._concatenation\n",
    "        elif mode == 'concatenation_debug':\n",
    "            self.operation_function = self._concatenation_debug\n",
    "        elif mode == 'concatenation_residual':\n",
    "            self.operation_function = self._concatenation_residual\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown operation function.')\n",
    "\n",
    "    def build(self, inputs):\n",
    "        self.W = tf.keras.models.Sequential([\n",
    "            self.conv_nd(filters=inputs[0][-1], kernel_size=1, strides=1,\n",
    "                         padding='same'),\n",
    "            tf.keras.layers.BatchNormalization()])\n",
    "        upsample_times = inputs[0][1] // inputs[1][1]\n",
    "        self.phi_upsample = self.upsample(upsample_times // 2,\n",
    "                                          name='upsample_phi')\n",
    "        self.sigm_psi_upsample = self.upsample(2, name='upsample_sigm_psi')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, g = inputs\n",
    "        return self.operation_function(x, g)\n",
    "\n",
    "    def _concatenation(self, x, g):\n",
    "        input_size = x.shape\n",
    "\n",
    "        # theta => (b, c, h, w) -> (b, i_c, h, w) -> (b, i_c, hw)\n",
    "        # phi   => (b, g_d) -> (b, i_c)\n",
    "        theta_x = self.theta(x)\n",
    "\n",
    "        # g (b, c, h', w') -> phi_g (b, i_c, h', w')\n",
    "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, hw) -> (b, i_c, h/s1, w/s2)\n",
    "        phi_g = self.phi(g)\n",
    "        upsample_times = theta_x.shape[1] // phi_g.shape[1]\n",
    "        phi_g = self.phi_upsample(phi_g)\n",
    "        f = tf.keras.activations.relu(theta_x + phi_g)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, h/s1, w/s2)\n",
    "        sigm_psi_f = tf.keras.activations.sigmoid(self.psi(f))\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        sigm_psi_f = self.sigm_psi_upsample(sigm_psi_f)\n",
    "        y = sigm_psi_f * x\n",
    "        W_y = self.W(y)\n",
    "        return W_y, sigm_psi_f\n",
    "    \n",
    "class GridAttentionBlock2D(_GridAttentionBlockND):\n",
    "    def __init__(self, gating_channels, inter_channels=None, mode='concatenation',\n",
    "                 sub_sample_factor=(2, 2)):\n",
    "        super(GridAttentionBlock2D, self).__init__(\n",
    "            gating_channels=gating_channels, inter_channels=inter_channels,\n",
    "            dimension=2, mode=mode, sub_sample_factor=sub_sample_factor)\n",
    "        \n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_filters, name, kernel_size=3):\n",
    "        super(ConvBlock, self).__init__(name=name)\n",
    "        self.conv_2d = tf.keras.layers.Conv2D(\n",
    "            num_filters, kernel_size=kernel_size, padding='same',\n",
    "            activation='linear')\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.conv_2d(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = tf.keras.activations.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75298b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_gate(g, s, num_filters):\n",
    "    Wg = tf.keras.layers.Conv2D(num_filters, 1, padding=\"same\")(g)\n",
    "    Wg = tf.keras.layers.BatchNormalization()(Wg)\n",
    " \n",
    "    Ws = tf.keras.layers.Conv2D(num_filters, 1, padding=\"same\")(s)\n",
    "    Ws = tf.keras.layers.BatchNormalization()(Ws)\n",
    " \n",
    "    out = tf.keras.layers.Activation(\"relu\")(Wg + Ws)\n",
    "    out = tf.keras.layers.Conv2D(num_filters, 1, padding=\"same\")(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(out)\n",
    " \n",
    "    return out * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a6c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_mini_block(inputs, n_filters=32, activation=\"relu\", dropout_prob=0.3, max_pooling=True, name=\"\"):\n",
    "    \"\"\"\n",
    "    Encoder mini block for U-Net architecture. It consists of two convolutional layers with the same activation function\n",
    "    and number of filters. Optionally, a dropout layer can be added after the second convolutional layer. If max_pooling\n",
    "    is set to True, a max pooling layer is added at the end of the block. The skip connection is the output of the second\n",
    "    convolutional layer.\n",
    "\n",
    "    :param inputs: Input tensor to the block\n",
    "    :param n_filters: Number of filters for the convolutional layers\n",
    "    :param activation: Activation function for the convolutional layers\n",
    "    :param dropout_prob: Dropout probability for the dropout layer (0 means no dropout)\n",
    "    :param max_pooling: Boolean to add a max pooling layer at the end of the block\n",
    "    :param name: Name of the block (Optional)\n",
    "    :return: The output tensor of the block and the skip connection tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    conv = tf.keras.layers.Conv2D(n_filters,\n",
    "                                  3,  # filter size\n",
    "                                  activation=\"linear\",\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer='HeNormal',\n",
    "                                  name=\"eblock\" + name + \"conv1\")(inputs)\n",
    "    \n",
    "    conv = tf.keras.layers.BatchNormalization(name=\"eblock\" + name + \"norm1\")(conv)\n",
    "    conv = tf.keras.layers.Activation(activation=activation, name=\"eblock\" + name + activation+\"1\")(conv)\n",
    "    \n",
    "    conv = tf.keras.layers.Conv2D(n_filters,\n",
    "                                  3,  # filter size\n",
    "                                  activation=\"linear\",\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer='HeNormal',\n",
    "                                  name=\"eblock\" + name + \"conv2\")(conv)\n",
    "    conv = tf.keras.layers.BatchNormalization(name=\"eblock\" + name + \"norm2\")(conv)\n",
    "    conv = tf.keras.layers.Activation(activation=activation, name=\"eblock\" + name + activation+\"2\")(conv)\n",
    "    \n",
    "    if dropout_prob > 0:\n",
    "        conv = tf.keras.layers.Dropout(dropout_prob, name=\"eblock\" + name + \"drop\")(conv)\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"eblock\" + name + \"pool\")(conv)\n",
    "    else:\n",
    "        next_layer = conv\n",
    "    skip_connection = conv\n",
    "    return next_layer, skip_connection\n",
    "\n",
    "\n",
    "def decoder_mini_block(prev_layer_input, skip_layer_input, n_filters=32, activation=\"relu\", dropout_prob=0.3,\n",
    "                       max_pooling=True, attention=True, name=\"\"):\n",
    "    \"\"\"\n",
    "    Decoder mini block for U-Net architecture that consists of a transposed convolutional layer followed by two\n",
    "    convolutional layers. The skip connection is the concatenation of the transposed convolutional layer and the\n",
    "    corresponding encoder skip connection.\n",
    "\n",
    "    :param prev_layer_input: Input tensor to the block from the previous layer\n",
    "    :param skip_layer_input: Input tensor to the block from the corresponding encoder skip connection\n",
    "    :param n_filters: Number of filters for the convolutional layers\n",
    "    :param activation: Activation function for the convolutional layers\n",
    "    :param name: Name of the block (Optional)\n",
    "    :return: The output tensor of the block\n",
    "    \"\"\"\n",
    "    \n",
    "    if max_pooling:\n",
    "        prev_layer_input =  tf.keras.layers.UpSampling2D(interpolation=\"bilinear\")(prev_layer_input)\n",
    "    if attention and max_pooling:\n",
    "        skip_layer_input=attention_gate(prev_layer_input, skip_layer_input, n_filters)\n",
    "    merge = tf.keras.layers.concatenate([prev_layer_input, skip_layer_input], name=\"dblock\" + name + \"concat\")\n",
    "    conv = tf.keras.layers.Conv2D(n_filters,\n",
    "                                  3,  # filter size\n",
    "                                  activation=\"linear\",\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer='HeNormal',\n",
    "                                  name=\"dblock\" + name + \"conv1\")(merge)\n",
    "    conv = tf.keras.layers.BatchNormalization(name=\"dblock\" + name + \"norm1\")(conv)\n",
    "    conv = tf.keras.layers.Activation(activation=activation, name=\"dblock\" + name + activation+\"1\")(conv)\n",
    "    \n",
    "    conv = tf.keras.layers.Conv2D(n_filters,\n",
    "                                  3,  # filter size\n",
    "                                  activation=\"linear\",\n",
    "                                  padding='same',\n",
    "                                  kernel_initializer='HeNormal',\n",
    "                                  name=\"dblock\" + name + \"conv2\")(conv)\n",
    "    conv = tf.keras.layers.BatchNormalization(name=\"dblock\" + name + \"norm2\")(conv)\n",
    "    conv = tf.keras.layers.Activation(activation=activation, name=\"dblock\" + name + activation+\"2\")(conv)\n",
    "    if dropout_prob > 0:\n",
    "        conv = tf.keras.layers.Dropout(dropout_prob, name=\"dblock\" + name + \"drop\")(conv)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def unet_model(input_size, arhitecture, attention=True):\n",
    "    \"\"\"\n",
    "    U-Net model for semantic segmentation. The model consists of an encoder and a decoder. The encoder downsamples the\n",
    "    input image and extracts features. The decoder upsamples the features and generates the segmentation mask. Skip\n",
    "    connections are used to concatenate the encoder features with the decoder features. The model is created from the\n",
    "    architecture dictionary that contains the number of filters, activation functions, dropout probabilities, and max\n",
    "    pooling for each mini block.\n",
    "\n",
    "    :param input_size: Size of the input image\n",
    "    :param arhitecture: Dictionary containing the architecture of the U-Net model\n",
    "    :return: U-Net model\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tf.keras.layers.Input(input_size, name=\"input\")\n",
    "    inputs = tf.keras.layers.BatchNormalization(name=\"inputnormalisation\")(inputs)\n",
    "    skip_connections = []\n",
    "    layer = inputs\n",
    "    if not \"attention\" in arhitecture.keys():\n",
    "        arhitecture[\"attention\"] = [False for i in arhitecture[\"upFilters\"]]\n",
    "    # Encoder\n",
    "    for i in range(len(arhitecture[\"downFilters\"])):\n",
    "        layer, skip = encoder_mini_block(layer,\n",
    "                                         n_filters=arhitecture[\"downFilters\"][i],\n",
    "                                         activation=arhitecture[\"downActivation\"][i],\n",
    "                                         dropout_prob=arhitecture[\"downDropout\"][i],\n",
    "                                         max_pooling=arhitecture[\"downMaxPool\"][i],\n",
    "                                         name=str(i))\n",
    "        skip_connections.append(skip)       \n",
    "    # Decoder\n",
    "    for i in range(len(arhitecture[\"upFilters\"])):\n",
    "        \"\"\"if arhitecture[\"attention\"][i]:\n",
    "            gating = ConvBlock(arhitecture[\"upFilters\"][i], name=str(len(arhitecture[\"upFilters\"])-1-i)+'gating')(layer)\n",
    "            skip_con, _ = GridAttentionBlock2D(inter_channels=arhitecture[\"upFilters\"][i], gating_channels=128)(\n",
    "                [skip_connections[len(arhitecture[\"upFilters\"])-1-i], gating])\n",
    "        else:\"\"\"\n",
    "        skip_con = skip_connections[len(arhitecture[\"upFilters\"])-1-i]\n",
    "        layer = decoder_mini_block(layer,\n",
    "                                   skip_con,\n",
    "                                   n_filters=arhitecture[\"upFilters\"][i],\n",
    "                                   activation=arhitecture[\"upActivation\"][i],\n",
    "                                   attention=arhitecture[\"attention\"][i],\n",
    "                                   dropout_prob=arhitecture[\"upDropout\"][i],\n",
    "                                   max_pooling=arhitecture[\"downMaxPool\"][len(arhitecture[\"upFilters\"])-1-i],\n",
    "                                   name=str(len(arhitecture[\"upFilters\"])-1-i))\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', name=\"output\")(layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs], name=\"AsteroidNET\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5ea941",
   "metadata": {},
   "outputs": [],
   "source": [
    "arhitecture = {\"downFilters\": [16, 32, 64, 128, 256, 512], \n",
    "               \"downActivation\": [\"relu\", \"sigmoid\", \"relu\", \"sigmoid\", \"relu\", \"sigmoid\"], \n",
    "               \"downDropout\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1], \n",
    "               \"downMaxPool\": [True, True, True, True, True, True], \n",
    "               \"upFilters\": [512, 256, 128, 64, 32, 16], \n",
    "               \"upActivation\": [\"sigmoid\", \"relu\", \"sigmoid\", \"relu\", \"sigmoid\", \"relu\"], \n",
    "               \"upDropout\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "               \"attention\": [True, True, True, True, True, False],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2779d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "dataset_train = tf.data.TFRecordDataset([\"../DATA/train1.tfrecord\"])\n",
    "tfrecord_shape = tools.model.get_shape_of_quadratic_image_tfrecord(dataset_train)\n",
    "dataset_train = dataset_train.map(tools.model.parse_function(img_shape=tfrecord_shape, test=False))\n",
    "dataset_train = dataset_train.shuffle(5*batch_size).batch(batch_size).prefetch(2)\n",
    "dataset_val = tf.data.TFRecordDataset([\"../DATA/test.tfrecord\"])\n",
    "dataset_val = dataset_val.map(tools.model.parse_function(img_shape=tfrecord_shape, test=False))\n",
    "dataset_val = dataset_val.batch(batch_size).prefetch(2)\n",
    "\n",
    "terminateonnan_kb = tf.keras.callbacks.TerminateOnNaN()\n",
    "reducelronplateau_kb = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.75,\n",
    "                                                            patience=2, cooldown=2, verbose=1)\n",
    "kb = [terminateonnan_kb, reducelronplateau_kb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1246eed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "INFO:tensorflow:Collective all_reduce tensors: 158 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 158 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "726/726 [==============================] - 208s 246ms/step - loss: 0.6233 - precision: 0.0066 - recall: 0.5274 - f1_score: 0.1613 - val_loss: 0.5335 - val_precision: 0.1302 - val_recall: 0.3078 - val_f1_score: 0.1701 - lr: 0.0010\n",
      "Epoch 2/128\n",
      "726/726 [==============================] - 194s 267ms/step - loss: 0.2959 - precision: 0.2428 - recall: 0.4635 - f1_score: 0.3151 - val_loss: 0.5251 - val_precision: 0.1814 - val_recall: 0.2724 - val_f1_score: 0.2110 - lr: 0.0010\n",
      "Epoch 3/128\n",
      "726/726 [==============================] - 198s 272ms/step - loss: 0.2762 - precision: 0.2702 - recall: 0.4886 - f1_score: 0.3397 - val_loss: 0.6324 - val_precision: 0.5577 - val_recall: 0.1060 - val_f1_score: 0.1542 - lr: 0.0010\n",
      "Epoch 4/128\n",
      "726/726 [==============================] - 197s 272ms/step - loss: 0.2765 - precision: 0.2691 - recall: 0.4905 - f1_score: 0.3427 - val_loss: 0.5128 - val_precision: 0.4604 - val_recall: 0.2134 - val_f1_score: 0.2586 - lr: 0.0010\n",
      "Epoch 5/128\n",
      "726/726 [==============================] - 198s 273ms/step - loss: 0.2475 - precision: 0.3220 - recall: 0.5003 - f1_score: 0.3774 - val_loss: 0.4930 - val_precision: 0.4560 - val_recall: 0.2333 - val_f1_score: 0.2730 - lr: 0.0010\n",
      "Epoch 6/128\n",
      "726/726 [==============================] - 199s 273ms/step - loss: 0.2486 - precision: 0.2937 - recall: 0.5027 - f1_score: 0.3631 - val_loss: 0.5678 - val_precision: 0.5011 - val_recall: 0.1715 - val_f1_score: 0.2186 - lr: 0.0010\n",
      "Epoch 7/128\n",
      "726/726 [==============================] - 199s 273ms/step - loss: 0.2406 - precision: 0.3155 - recall: 0.5131 - f1_score: 0.3804 - val_loss: 0.4624 - val_precision: 0.3314 - val_recall: 0.2777 - val_f1_score: 0.2742 - lr: 0.0010\n",
      "Epoch 8/128\n",
      "726/726 [==============================] - 198s 273ms/step - loss: 0.2319 - precision: 0.3330 - recall: 0.5107 - f1_score: 0.3921 - val_loss: 0.5512 - val_precision: 0.1447 - val_recall: 0.2783 - val_f1_score: 0.1872 - lr: 0.0010\n",
      "Epoch 9/128\n",
      "726/726 [==============================] - 198s 273ms/step - loss: 0.2296 - precision: 0.3347 - recall: 0.5262 - f1_score: 0.3908 - val_loss: 0.4466 - val_precision: 0.4302 - val_recall: 0.2781 - val_f1_score: 0.3082 - lr: 0.0010\n",
      "Epoch 10/128\n",
      "726/726 [==============================] - 202s 279ms/step - loss: 0.2290 - precision: 0.3397 - recall: 0.5258 - f1_score: 0.4009 - val_loss: 0.5150 - val_precision: 0.5093 - val_recall: 0.2045 - val_f1_score: 0.2590 - lr: 0.0010\n",
      "Epoch 11/128\n",
      "726/726 [==============================] - 198s 273ms/step - loss: 0.2257 - precision: 0.3346 - recall: 0.5244 - f1_score: 0.3948 - val_loss: 0.5254 - val_precision: 0.1023 - val_recall: 0.3340 - val_f1_score: 0.1599 - lr: 0.0010\n",
      "Epoch 12/128\n",
      "482/726 [==================>...........] - ETA: 1:02 - loss: 0.2370 - precision: 0.3571 - recall: 0.5276 - f1_score: 0.4107"
     ]
    }
   ],
   "source": [
    "FE = tf.keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True, alpha=0.95)\n",
    "FT = tools.metrics.FocalTversky(alpha=0.7, gamma=1/5)\n",
    "with mirrored_strategy.scope():\n",
    "    model = unet_model((128, 128, 1), arhitecture)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss=FT,\n",
    "                  metrics=[\"Precision\", \"Recall\", tools.metrics.F1_Score()])\n",
    "results = model.fit(dataset_train, epochs=128, validation_data=dataset_val,\n",
    "                    callbacks=kb, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21883c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../DATA/Model_test_5.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b70db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Karlo's LSST Stack",
   "language": "python",
   "name": "kmrakovcic_lsststack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
